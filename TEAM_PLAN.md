# CS 5542 Lab 6: AI Agent Integration ‚Äî Team Plan

## üéØ **Objective**
Our goal is to evolve the Lab 5 metrics dashboard into a fully conversational **AI Assistant**. The Agent will natively reason over user queries and route requests to 1) structured data in Snowflake, or 2) qualitative evidence in the RAG pipeline.

### Project Context: "Academic RAG Pipeline Agent"
Following the loading of our chunks and metrics into Snowflake, we need to make it accessible to non-technical users. We will use Google GenAI to build an agent that binds directly to our custom analytical tools.

---

## üèóÔ∏è **The Agent Tool Design**
To empower the Agent, we've designed these modular capabilities:

1.  **`run_snowflake_query`**: Executes arbitrary SQL generated by the LLM to pull precise metrics (like token counts or aggregated analytics).
2.  **`get_database_schema`**: An introspective tool allowing the agent to view available column names *before* guessing SQL, reducing hallucinations.
3.  **`search_knowledge_base`**: Uses our localized Sentence-Transformer embeddings and BM25 matrix to retrieve original paper chunks containing qualitative evidence.

---

## üë• **Role-Based Task Breakdown**

### **Person 1: The AI Engineer üß† (Rohan Hashmi)**
**Focus:** The Agent Execution Pipeline and GenAI SDK.

*   **Core Tasks:**
    *   Initialize the `google-genai` client and bind functions to the `client.chats.create` session.
    *   Develop the **System Prompt** so the agent knows when to use Database Tools vs Knowledge Base tools.
    *   Error handle tool outputs (if SQL fails, return the error to the LLM to try again).

---

### **Person 2: The Full-Stack Developer üíª (Kenneth Kakie)**
**Focus:** Streamlit Chat Interface.

*   **Core Tasks:**
    *   Refactor `app/streamlit_app.py` to use `st.tabs` so the old dashboard can coexist with the Agent.
    *   Implement `st.chat_message` and `st.session_state` to keep conversation history.
    *   Add loading visual indicators during agent tool reasoning.

---

### **Person 3: The Analyst & Documentation üìù (Blake Simpson)**
**Focus:** Converting pipelines to Tools and Testing.

*   **Core Tasks:**
    *   Write `backend/tools.py` pulling logic from `scripts/` and `rag_pipeline.py`.
    *   Write the **Evaluation Scenarios** testing the agent's ability to reason cleanly.
    *   Provide the Demo recording executing complex, multi-tool queries.

---

## üìÖ **Execution Plan**

1.  **Data Prep (Immediately)**
    *   **All**: Help create the 4 CSVs. Manually open the PDFs and copy/paste some title/text/metrics into Excel, then Save As CSV.
    *   **Files**: `data/papers.csv`, `data/chunks.csv`, `data/figures.csv`, `data/metrics.csv`.

2.  **Migration (Day 1-2)**
    *   **P1**: Writes the SQL Schema and runs Python Loader. Checks Snowflake to see data.
    *   **P3**: Writes SQL queries against the new tables.
    *   **P2**: Updates `streamlit_app.py` to remove "Events" and add "Papers".

3.  **Extensions (Day 3)**
    *   Implement the 3 specific extensions above.
    *   Record the Demo Video showing the "RAG Knowledge Base" dashboard.

---

## ‚úÖ **Submission Checklist** (Due Feb 23, Noon)

*   [ ] **GitHub Repo** with code + CSVs.
*   [ ] **`pipeline_logs.csv`** (Generated by using the app).
*   [ ] **`CONTRIBUTIONS.md`** (One entry per person).
*   [ ] **Pipeline Diagram** (PDF/PNG).
*   [ ] **Demo Video Link**.
