"evidence_id","doc_id","source_file","page","chunk_index","chunk_text"
"bm25_prf_p01_c001","bm25_prf","bm25_prf.pdf","1","1","Foundations and TrendsR⃝ in Information Retrieval Vol. 3, No. 4 (2009) 333–389 c⃝ 2009 S. Robertson and H. Zaragoza DOI: 10.1561/1500000019 The Probabilistic Relevance Framework: BM25 and Beyond By Stephen Robertson and Hugo Zaragoza Contents 1 Introduction 334 2 Development of the Basic Model 336 2.1 Information Needs and Queries 336 2.2 Binary Relevance 337 2.3 The Probability Ranking Principle 337 2.4 Some Notation 338 2.5 A Note on Probabilities and Rank Equivalence 345 3 Derived Models 347 3.1 The Binary Independence Model 347 3.2 Relevance Feedback and Query Expansion 349 3.3 Blind Feedback 351 3.4 The Eliteness Model and BM25 352 3.5 Uses of BM25 360 3.6 Multiple Streams and BM25F 361 3.7 Non-Textual Relevance Features 365 3.8 Positional Information 367 3.9 Open Source Implementations of BM25 and BM25F 369"
"bm25_prf_p02_c001","bm25_prf","bm25_prf.pdf","2","1","4 Comparison with Other Models 371 4.1 Maron and Kuhns 371 4.2 The Uniﬁed Model 372 4.3 The Simple Language Model 374 4.4 The Relevance (Language) Model 375 4.5 Topic Models 375 4.6 Divergence from Randomness 376 5 Parameter Optimisation 377 5.1 Greedy Optimisation 378 5.2 Multidimensional Optimisation 380 5.3 Gradient Optimisation 382 6 Conclusions 384 References 385"
"bm25_prf_p03_c001","bm25_prf","bm25_prf.pdf","3","1","Foundations and TrendsR⃝ in Information Retrieval Vol. 3, No. 4 (2009) 333–389 c⃝ 2009 S. Robertson and H. Zaragoza DOI: 10.1561/1500000019 The Probabilistic Relevance Framework: BM25 and Beyond Stephen Robertson 1 and Hugo Zaragoza 2 1 Microsoft Research,7JJ Thomson Avenue, Cambridge CB3 0FB, UK ser@microsoft.com 2 Yahoo! Research, Av. Diagonal 177, Barcelona 08028, Spain hugoz@yahoo-inc.com Abstract The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970–1980s, which led to the development of one of the most successful text-retrieval algo- rithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algo- rithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the diﬀerent ranking algorithms that result from its application: the binary independence model, relevance feedback mod- els, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."
"bm25_prf_p04_c001","bm25_prf","bm25_prf.pdf","4","1","1 Introduction This monograph addresses theclassical probabilistic model of informa- tion retrieval. The model is characterised by including a speciﬁc notion of relevance, an explicit variable associated with a query–document pair, normally hidden in the sense ofnot observable. The model revolves around the notion of estimating a probability of relevance for each pair, and ranking documents in relation to a given query in descending order of probability of relevance. The best-known instantiation of the model is the BM25 term-weighting and document-scoring function. The model has been developed in stages over a period of about 30 years, with a precursor in 1960. A few of the main references are as follows: [30, 44, 46, 50, 52, 53, 58]; other surveys of a range of proba- bilistic approaches include [14, 17]. Some more detailed references are given below. There are a number of later developments of IR models which are also probabilistic but which diﬀer considerably from the models developed here — speciﬁcally and notably the language model (LM) approach [24, 26, 33] and thedivergence from randomness(DFR) mod- els [2]. For this reason we refer to the family of models developed here as the Probabilistic Relevance Framework(PRF), emphasising the 334"
"bm25_prf_p05_c001","bm25_prf","bm25_prf.pdf","5","1","335 importance of the relevance variable in the development of the models. We do not cover the development of other probabilistic models in the present survey, but some points of comparison are made. This is not primarily an experimental survey; throughout, asser- tions will be made about techniques which are said to work well. In general such statements derive from experimental results, many exper- iments by many people over a long period, which will not in general be fully referenced. The emphasis is on the theoretical development of the methods, the logic and assumptions behind the models. The survey is organised as follows. In Section 2 we develop the most generic retrieval model, which subsumes a number of speciﬁc instanti- ations developed in Section 3. In Section 4 we discuss the similarities and diﬀerences with other retrieval frameworks. Finally in Section 5 we give an overview of optimisation techniques we have used to tune the diﬀerent parameters in the models and Section 6 concludes the survey."
"bm25_prf_p06_c001","bm25_prf","bm25_prf.pdf","6","1","2 Development of the Basic Model 2.1 Information Needs and Queries We start from a notion of information need. We take a query to be a representation (not necessarily very good or very complete) of an individual user’s information need or perhaps search intent. We take relevance to mean the relevance of a document to the information need, as judged by the user. We make no speciﬁc assumption about the conceptual nature of relevance; in particular, we donot assume rel- evance to be topical in nature. 1 We do, however, makes some assump- tions about the formal status of the relevance variable, given below, which might be taken to imply some assumptions about its conceptual nature. 1 The notion of topic, as used in TREC, is somewhat akin to information need, or at least to a more detailed and complete representation and speciﬁcation of an information need. The use of the term ‘topic’ for this purpose is a little unfortunate, as it seems to imply some kind of topical nature of relevance. However, we also note that the widespread use of the term following TREC also includes some examples of non-topical ‘topics’, for example the home-page topics used in the TREC Web track [59]. 336"
"bm25_prf_p07_c001","bm25_prf","bm25_prf.pdf","7","1","2.2 Binary Relevance 337 2.2 Binary Relevance The assumptions about relevance are as follows: 1. Relevance is assumed to be a property of the document given information need only, assessable without reference to other documents; and 2. The relevance property is assumed to be binary. Either of these assumptions is at the least arguable. We might easily imagine situations in which one document’s relevance can only be per- ceived by the user in the context of another document, for example. Regarding the binary property, many recent experimental studies have preferred a graded notion of relevance. One might go further and sug- gest that diﬀerent documents may be relevant to a need in diﬀerent ways, not just to diﬀerent degrees. However, we emphasise that we consider the situation of a single information need (rather than the multiple needs or intents that might be represented by a single text query). If we take relevant to mean ‘the user would like to be pointed to this document’, the binary notion is at least moderately plausible. 2.3 The Probability Ranking Principle Given that an information retrieval system cannot know the values of the relevance property of each document, we assume that the infor- mation available to the system is at best probabilistic. That is, the known (to the system) properties of the document and the query may provide probabilistic or statistical evidence as to the relevance of that document to the underlying need. Potentially these properties may be rich and include a variety of"
"bm25_prf_p07_c002","bm25_prf","bm25_prf.pdf","7","2","the known (to the system) properties of the document and the query may provide probabilistic or statistical evidence as to the relevance of that document to the underlying need. Potentially these properties may be rich and include a variety of diﬀerent kinds of evidence; the only infor- mation assumed to be absent is the actual relevance property itself. Given whatever information is available, the system may make some statistical statement about the possible value of the relevance property. Given a binary document-by-document relevance property, then this statistical information may be completely encapsulated in aprobability of relevance. The probability of relevance of a given document to a"
"bm25_prf_p08_c001","bm25_prf","bm25_prf.pdf","8","1","338 Development of the Basic Model given query plays a central role in the present theory. We can in fact make a general statement about this: If retrieved documents are ordered by decreasing prob- ability of relevance on the data available, then the sys- tem’s eﬀectiveness is the best that can be obtained for the data. This is a statement of the Probability Ranking Principle (PRP), taken from [52], an abbreviated version of the one in [40]. The PRP can be justiﬁed under some further assumptions, for a range of speciﬁc measures of eﬀectiveness. It is not, however, completely general; one can also construct counter-examples. But these counter-examples depend on probabilities deﬁned over a population of users. The PRP is safe for the case considered: an individual user. It will be assumed for the remainder of this work. 2.4 Some Notation In this section, we introduce some of the notation that will be used throughout the survey. The notation assumes a single query q repre- senting a single information need. The ﬁrst symbol is used to indicate that two functions are equivalent as ranking functions. rank equivalence: ∝ q e.g. g() ∝ q h() In developing the model, from the probability of relevance of a doc- ument to a term-weighting and document-scoring function, we make frequent use of transformations which thus preserve rank order. Such a transformation (in a document-scoring function, say) may be linear or non-linear, but must be strictly monotonic, so that if documents are ranked by the"
"bm25_prf_p08_c002","bm25_prf","bm25_prf.pdf","8","2","and document-scoring function, we make frequent use of transformations which thus preserve rank order. Such a transformation (in a document-scoring function, say) may be linear or non-linear, but must be strictly monotonic, so that if documents are ranked by the transformed function, they will be in the same rank order as if they had been ranked by the original function. The property of relevance is represented by a random variable Rel with two possible values: relevance Rel: rel , rel (relevant or not)"
"bm25_prf_p09_c001","bm25_prf","bm25_prf.pdf","9","1","2.4 Some Notation 339 As discussed above, we assume that relevance is a binary property of a document (given an information need). We will use the short-hand notation P(rel|d,q) to denote P(Rel = rel|d,q). For documents and queries, we generally assume a bag or set of words model. We have a vocabulary of terms indexed into the set V, each of which may be present or absent (in the set of words model) or may be present with some frequency (bag of words model). In either case the objects (documents or queries) may be represented as vectors over the space deﬁned by the vocabulary. Thus a document is: document: d := (tf 1,..., tf |V|), where tf i normally represents the frequency of termti in the document. We will also need to distinguish between the random variable TF i and its observed value in a document, tf i. The random variable will usually refer to the term frequencies of a given term in the vocabulary. However, the formulation of the basic model is somewhat more general than this, and can accommodate any discrete variable as a feature (e.g., any discrete property or attribute of the document). Thus we can re-interpret tf as simply representing presence or absence of a term; this is the basis of Section 3.1 below. Continuous variables can also be accommodated by replacing probability mass functionsTF i (of discrete variables) by probability density functions (of continuous variables); although we do not present a formal development of the"
"bm25_prf_p09_c002","bm25_prf","bm25_prf.pdf","9","2","this is the basis of Section 3.1 below. Continuous variables can also be accommodated by replacing probability mass functionsTF i (of discrete variables) by probability density functions (of continuous variables); although we do not present a formal development of the model with continuous variables, we will use this fact in Section 3.7 to introduce some non-textual continuous features into the model. A query is represented in two diﬀerent ways. In the ﬁrst, it is treated similarly as a vector: query: q := (qtf 1,..., qtf |V|) Here again the components qtf i may represent term frequencies in the query, or may represent a binary presence or absence feature. Throughout this survey we will need to sum or multiply variables of terms present in the query (i.e., with qtf i > 0). For this purpose we deﬁne the set of indices: query terms: q := {i|qtf i > 0} (indices of terms in the query)"
"bm25_prf_p10_c001","bm25_prf","bm25_prf.pdf","10","1","340 Development of the Basic Model 2.4.1 Preview of the Development The following provides an overview of the development of the basic model; the individual steps are discussed below. The main point to note is that we start with the very general PRP, and end up with an explicit document scoring function, composed of a simple sum of weights for the individual query terms present in the document. P(rel|d,q) ∝ q P(rel|d,q) P(rel|d,q) (2.1) = P(d|rel,q) P(d|rel,q) P(rel|q) P(rel|q) (2.2) ∝ q P(d|rel,q) P(d|rel,q) (2.3) ≈ ∏ i∈V P(TF i = tf i |rel,q) P(TF i = tf i |rel,q) (2.4) ≈ ∏ i∈q P(TF i = tf i |rel) P(TF i = tf i |rel) (2.5) ∝ q ∑ q log P(TF i = tf i |rel) P(TF i = tf i |rel) (2.6) = ∑ q Ui(tf i) (2.7) = ∑ q,tf i>0 Ui(tf i)+ ∑ q,tf i=0 Ui(0) − ∑ q,tf i>0 Ui(0) + ∑ q,tf i>0 Ui(0) (2.8) = ∑ q,tf i>0 (Ui(tf i) − Ui(0)) + ∑ q Ui(0) (2.9) ∝ q ∑ q,tf i>0 (Ui(tf i) − Ui(0)) (2.10) = ∑ q,tf i>0 wi (2.11)"
"bm25_prf_p11_c001","bm25_prf","bm25_prf.pdf","11","1","2.4 Some Notation 341 2.4.2 Details We start with the probability of relevance of a given document and query. The ﬁrst three steps are simple algebraic transformations. In Step (2.1), we simply replace the probability by an odds-ratio. 2 In Step (2.2), we perform Bayesian inversions on both numerator and denominator. In Step (2.3), we drop the second component which is independent of the document, and therefore does not aﬀect the rank- ing of the documents. In Step (2.4) (term independence assumption), we expand each probability as a product over the terms of the vocabulary. This step depends on an assumption of statistical independence between terms — actually a pair of such assumptions, for the relevant and non-relevant cases, respectively. Note that we are not assuming unconditional term independence, but a weaker form, namely conditional independence. 3 This is clearly a much more arguable step: in general terms will not be statistically independent in this fashion. The obvious reason for taking this step is to lead us to a simple and tractable (even if approximate) scoring function. However, we may make some further arguments to support this step: 1. Models of this type in other domains, known as Na¨ıve Bayes models, are well known to be remarkably good, simple and robust, despite signiﬁcant deviations from independence. Experimental evidence in IR provides some support for this general statement. 2. The pair of assumptions is not in general equivalent to a blanket assumption of independence between terms over the whole collection. On"
"bm25_prf_p11_c002","bm25_prf","bm25_prf.pdf","11","2","robust, despite signiﬁcant deviations from independence. Experimental evidence in IR provides some support for this general statement. 2. The pair of assumptions is not in general equivalent to a blanket assumption of independence between terms over the whole collection. On the contrary, for a pair of query terms, both statistically correlated with relevance, the pair of assumptions predict a positive association between the two terms over the whole collection. In fact we often observe such a positive association. In eﬀect the model says that this 2 This transformation is order preserving; the odds-ratio of p is p 1− p , and this function is a monotonous increasing function of p ∈ [0,1). 3 P(ab| c)= P(a| c)P(b| c)."
"bm25_prf_p12_c001","bm25_prf","bm25_prf.pdf","12","1","342 Development of the Basic Model positive association is induced by relevance to the query. 4 This is clearly an over-simpliﬁcation, but perhaps not such a bad one. 3. Cooper [11] has demonstrated that we can arrive at the same transformation on the basis of a weaker assumption, called linked dependence. 5 This is essentially that the degree of sta- tistical association between the terms is the same in the rel- evant as in the non-relevant subsets. Again, this theoretical result may help to explain the robustness of such a model. We may represent the independence assumptions by means of a graph- ical model, as in Figure 2.1. This diagram shows how the term frequen- cies (tf i)i∈V are assumed to be generated as stochastic observations dependant only on the state of the relevance variable Rel (a full expla- nation of graphical model diagrams is outside the scope of this paper, we refer the reader to [6]). In step (2.5) (query-terms assumption), we restrict ourselves to the query terms only: in eﬀect, we assume that for any non-query term, the ratio of conditional probabilities is constant and equal to one. 6 This might seem a drastic assumption (that no non-query terms have any association with relevance); however, this is not quite the explanation of the step. We have to consider the question of what information we Fig. 2.1 Graphical model indicating basic independence assumptions. 4 If P(a| c) >P (a| c) and similarly for b, then it follows that P(ab)"
"bm25_prf_p12_c002","bm25_prf","bm25_prf.pdf","12","2","quite the explanation of the step. We have to consider the question of what information we Fig. 2.1 Graphical model indicating basic independence assumptions. 4 If P(a| c) >P (a| c) and similarly for b, then it follows that P(ab) >P (a)P(b) even under the conditional independence assumptions made. 5 P (a,b| c) P (a,b| c) = P (a| c) P (a| c) P (b| c) P (b| c) . 6 Note that if the ratio is constant, then it must be equal to one; otherwise we would have one of the probability distribution with probabilities always higher than another one, which is impossible since they both need to sum to 1."
"bm25_prf_p13_c001","bm25_prf","bm25_prf.pdf","13","1","2.4 Some Notation 343 have on which to base an estimate of the probability of relevance. It is a reasonable prior assumption, and turns out to be a good one, that in general query terms are (positively) correlated with relevance. However, we can make no such assumption about non-query terms (a term relating to a completely diﬀerent topic could well be negatively correlated with relevance). In the absence of any link to the query, the obvious vague prior assumption about a random vocabulary term must be that it is not correlated with relevance to this query. Later we will consider the issue of query expansion, adding new terms to the query, when we have evidence to link a non-query term with relevance to the query. Again, we can illustrate the assumptions by means of a graphical model, as in Figure 2.2. This diagram shows that in this model the relevance variable only aﬀects terms in the query. Starting in Step (2.6) we use the following short-hand notation: under the summation operator we will writeq (the starting set of values for i) followed by conditions that i should satisfy. For example: ∑ q should be read as ∑ i∈q, and ∑ q,tf i>0 should be read as ∑ {i| i∈q,tf i>0}. In Step (2.6), we make a common, order-preserving transformation, namely we take a log. This allows us to express the product of proba- bilities as a sum of log probabilities — actually log-odds because of the ratio in the product."
"bm25_prf_p13_c002","bm25_prf","bm25_prf.pdf","13","2","In Step (2.6), we make a common, order-preserving transformation, namely we take a log. This allows us to express the product of proba- bilities as a sum of log probabilities — actually log-odds because of the ratio in the product. In Step (2.7), we rewrite the previous equation using a function U i(x): Ui(x): =l o gP(TF i = x|rel) P(TF i = x|rel) (2.12) Fig. 2.2 Graphical model for restriction to query terms."
"bm25_prf_p14_c001","bm25_prf","bm25_prf.pdf","14","1","344 Development of the Basic Model Note that this is not the log-odds function. Note also that in Step (2.7) each term frequencytf i will be applied to a diﬀerent function Ui(tf i). This is necessary since the weight of a term does not depend only on its frequency, but also in other factors such as the collection frequency, etc. The following two steps use an arithmetic trick (sometimes referred to as removing the zeros) which eliminates from the equation terms that are not present in the document. This is crucial for the eﬃcient implementation of PRF ranking functions using inverted indices. In Step (2.8), we do two things. In the ﬁrst line, the sum over query terms in Step (2.7) has been split into two groups: those terms that are present in the document (ﬁrst sum) and those that are absent (second sum). In the second line, we subtract and add the same quantity (the sum of zero frequency weights for terms in the document) leaving the result unchanged. The reason for doing this will become clear in the next step. In Step (2.9), we regroup the sums of Step (2.8). First we note that the ﬁrst and third sums in Step (2.8) are over the same terms, and can be grouped. This forms the ﬁrst sum in Step (2.9). Then we note that the second and fourth sums in Step (2.8) span all terms in the query. This forms the second sum in Step (2.9). In Step (2.10)"
"bm25_prf_p14_c002","bm25_prf","bm25_prf.pdf","14","2","can be grouped. This forms the ﬁrst sum in Step (2.9). Then we note that the second and fourth sums in Step (2.8) span all terms in the query. This forms the second sum in Step (2.9). In Step (2.10) we drop the last sum since its value is document- independent. We note that by doing so we are left with a single sum over terms present both in the document and in the query. We have removed terms in the query with zero frequency in the document. In Step (2.11), we again rewrite the equation using the short-hand notation: W i(x): = Ui(x) − Ui(0) (2.13) = log P(TF i = x|rel)P(TF i =0 |rel) P(TF i = x|rel)P(TF i =0 |rel) (2.14) wi := Wi(tf i) (2.15) Equation (2.14) is the formula for the basic weight of a query term in a document. BothUi and Wi are term-weighting functions which can be precomputed and stored at indexing time. The diﬀerence is that in"
"bm25_prf_p15_c001","bm25_prf","bm25_prf.pdf","15","1","2.5 A Note on Probabilities and Rank Equivalence 345 order to use theUi in a document, we would have to sum over all query terms, whether or not they are present in the document. With Wi,w e need only to consider the weights wi of terms present in the document (in eﬀect the rewriting has deﬁned the weight of absent terms as zero). This ﬁts very well with the sparseness of the document-term matrix — we have no need to calculate scores of any document that contains no query terms. Indeed, this is a highly desirable property for a scoring function to be used in an inverted-ﬁle-based document retrieval system, since it means that we can easily calculate the score of all documents with non-zero scores by merging the inverted lists of the query terms. As indicated above, the model is not restricted to terms and term frequencies — any property, attribute or feature of the document, or of the document–query pair, which we reasonably expect to provide some evidence as to relevance, may be included. Below we will consider static features of documents — properties that are not dependent on the query — for inclusion. Any discrete property with a natural zero can be dealt with using the W i form of the weight — if we want to include a property without such a natural zero, we need to revert to the U i form. We note also that both forms are simple linear models — the combi-"
"bm25_prf_p15_c002","bm25_prf","bm25_prf.pdf","15","2","i form of the weight — if we want to include a property without such a natural zero, we need to revert to the U i form. We note also that both forms are simple linear models — the combi- nation of evidence from the diﬀerent query terms is just by summation. This is not in itself an assumption — it arises naturally from the more basic assumptions of the model. In the sections which follow, we deﬁne various instantiations of this basic sum-of-weights scoring model. 2.5 A Note on Probabilities and Rank Equivalence One consequence of our reliance on the probability ranking principle is that we are enabled to make the very cavalier transformations dis- cussed above, on the basis that the only property we wish to preserve is the rank order of documents. This might be a reasonable assump- tion for traditional ad hoc retrieval, but does not work for all retrieval situations. In some, for example in adaptive ﬁltering [42], we ﬁnd it desirable or necessary to arrive at an explicit estimate of the proba- bility of relevance of each considered document. Unfortunately, while"
"bm25_prf_p16_c001","bm25_prf","bm25_prf.pdf","16","1","346 Development of the Basic Model the above development allows us to serve the ranking purpose well, it is not easily reversible to give us such an explicit estimate. In particu- lar, some of the transformations involved dropping components which would not aﬀect the ranking, but would be required for a good proba- bility estimate. Often, as in the case of the component that we drop at Step (2.3), it would be very diﬃcult to estimate. Thus the above model has to be considerably modiﬁed if it is to be used in a situation which requires an explicit probability. This issue is not discussed further in the present survey."
"bm25_prf_p17_c001","bm25_prf","bm25_prf.pdf","17","1","3 Derived Models The models discussed in this section are all derived from the basic model presented in the previous section. We note again that the symbol TF in the basic weighting formula (2.14) can in general be any discrete property or attribute of the document. We start by interpreting it as a binary variable, indicating the presence or absence only of a query term in a document; in Section 3.4 we return to the more familiar term frequency. 3.1 The Binary Independence Model Suppose that TF i is a binary variable, having only the values zero and one. We can think of this, without loss of generality, as presence (one) or absence (zero) of a term: t i will refer to the event that the term is present in the document. The absence event is simply the complement of the presence event; probability of absence is one minus probability of presence. Now Equation (2.14) reduces to: w BIM i = logP(ti |rel)(1 − P(ti |rel)) (1 − P(ti |rel))P(ti |rel) (3.1) We now suppose that we do actually have some evidence on which to base estimates of these probabilities. Since they are conditional on 347"
"bm25_prf_p18_c001","bm25_prf","bm25_prf.pdf","18","1","348 Derived Models the relevance property, we are assuming that we have some judgements of relevance. We ﬁrst imagine, unrealistically, that we have a random sample of the whole collection, which has been completely judged for relevance. We derive an estimator that will also be useful for more realistic cases. We use the following notation: N — Size of the judged sample n i — Number of documents in the judged sample containing ti R — Relevant set size (i.e., number of documents judged relevant) ri — Number of judged relevant docs containing ti Given this information, we would like to estimate in an appropri- ate fashion the four probabilities needed for the weights deﬁned in Equation (3.1). The standard maximum likelihood estimate of a prob- ability from trials is a simple ratio, e.g., P(t i |rel) ≈ ri R . However, this is not a good estimator to plug into the weighting formula. A very obvious practical reason is that the combination of a ratio of proba- bilities and a log function may yield (positive or negative) inﬁnities as estimates. In fact there are also good theoretical reasons to modify the simple ratio estimates somewhat, as discussed in [44], to obtain a more robust estimator which introduces a small pseudo-count of frequency 0.5. The resulting formula is the well-known Robertson/Sprck Jones weight: w RSJ i = log(ri +0 .5)(N − R − ni + ri +0 .5) (ni − ri +0 .5)(R − ri +0 .5) (3.2) We next"
"bm25_prf_p18_c002","bm25_prf","bm25_prf.pdf","18","2","pseudo-count of frequency 0.5. The resulting formula is the well-known Robertson/Sprck Jones weight: w RSJ i = log(ri +0 .5)(N − R − ni + ri +0 .5) (ni − ri +0 .5)(R − ri +0 .5) (3.2) We next suppose that some documents, probably a small number, have been retrieved and judged – this is the usual relevance feedback scenario. In this case we might reasonably estimate the probability conditioned on (positive) relevance in the same way, from the known relevant documents. Estimation of the probabilities conditioned on non- relevance is more tricky. The obvious way, which is what Equation (3.2) ﬁrst suggests, would be to use the documents judged to be not relevant. However, we also know that (normally, for any reasonable query and any reasonable collection) the vast majority of documents are not rele- vant; those we have retrieved and judged are not only probably few in"
"bm25_prf_p19_c001","bm25_prf","bm25_prf.pdf","19","1","3.2 Relevance Feedback and Query Expansion 349 number, they are also likely to be a very non-typical set. We can make use of this knowledge to get a better estimate (at the risk of intro- ducing a little bias) by assuming that any document not yet known to be relevant is non-relevant. This is known as thecomplement method. Under this assumption, the RSJ weighting formula deﬁned above still applies, with the following redeﬁnitions of N and n i. N — Size of the whole collection ni — Number of documents in the collection containing ti Experiments suggest that using this complement method gives better estimates than using judged documents only. Finally, we suppose that we have no relevance information at all (the more usual scenario). In this case, we can only assume that the relevance probability P(t i |rel) is ﬁxed, but we can continue to use the complement method for the non-relevance probability — now we assume for this estimate that the entire collection is non-relevant. All this can be achieved by setting R = r i = 0 in the RSJ formula (3.2) — this is equivalent to settingP(ti |rel) = 0.5 (other values can be used [15]). The resulting formula is a close approximation to classical idf (it can be made closer still by a slight modiﬁcation of the model [47]): wIDF i = logN − ni +0 .5 ni +0 .5 (3.3) 3.2 Relevance Feedback and Query Expansion The model thus far clearly contains a natural"
"bm25_prf_p19_c002","bm25_prf","bm25_prf.pdf","19","2","(it can be made closer still by a slight modiﬁcation of the model [47]): wIDF i = logN − ni +0 .5 ni +0 .5 (3.3) 3.2 Relevance Feedback and Query Expansion The model thus far clearly contains a natural mechanism for rele- vance feedback — that is, for modifying the query based on relevance information. If we start with no relevance information, then we would weight the terms using the inverse document frequency (IDF) formula. Once the user makes some judgements of relevance, we should clearly reweight the terms according to the RSJ formula. But term reweighting is not in general an eﬀective method for improving search. Additionally, we have to consider expanding the query by adding new terms. At an early stage in the development of the basic model, rather than considering the entire vocabulary of terms in the estimation of"
"bm25_prf_p20_c001","bm25_prf","bm25_prf.pdf","20","1","350 Derived Models probabilities, we restricted ourselves to query terms only. This was not because we assumed that non-query terms were incapable of giving us any useful information, but rather that in the absence of any evidence about either which terms might be useful, or how useful they might be, a reasonable neutral prior assumption was that all non-query terms had zero correlation with relevance. However, in the relevance feedback scenario discussed above, we do indeed have some evidence for the inclusion of non-query terms. Potentially we can treat any term that occurs in any of the relevant documents as possibly useful. However, it is clear that many such terms will be noise, so we will probably need a conservative rule for adding new terms to the query. For each candidate term (i.e., non-query term present in a document judged relevant) we can consider how useful it might be if added to the query. One measure of this is simply the RSJ weight as above. However, this will emphasise very rare terms (this is consistent with the IDF idea) — such terms may indeed be good evidence of relevance when they occur, but because they occur in so few documents, they will not have much overall eﬀect on retrieval. As an alternative, we look for a measure of the possible overall eﬀect of adding a term; we refer to this (following [53]) as an oﬀer weight. We could base an oﬀer weight formula on a number of diﬀerent models. One"
"bm25_prf_p20_c002","bm25_prf","bm25_prf.pdf","20","2","an alternative, we look for a measure of the possible overall eﬀect of adding a term; we refer to this (following [53]) as an oﬀer weight. We could base an oﬀer weight formula on a number of diﬀerent models. One proposed in [41], to go with the binary independence model, is as follows. We look for terms that will maximally increase the diﬀerence in average score between relevant and non-relevant items. If we were to add term t i to the query with weight wi, then under the binary independence model (or indeed any additive term-weighting system based on term presence/absence only), it would increase the score of any document containing it by w i. The scores of other doc- uments would not be aﬀected, so the increase in average score could be calculated from the probability of the presence of t i. Thus the dif- ference in average score between relevant and non-relevant documents would be OW i =( P(ti |rel) − P(ti |rel))wi (3.4)"
"bm25_prf_p21_c001","bm25_prf","bm25_prf.pdf","21","1","3.3 Blind Feedback 351 (note that this is not the same as wi itself). Further, an appropriate estimate of this quantity can be derived as follows: OWRSJ i ≈ P(ti |rel)wi ≈ ri Rwi ∝ q ri wRSJ i (3.5) The ﬁrst step approximates by ignoring the second probability (usually much smaller than the ﬁrst); the second replaces the probability by the obvious estimate; and the third multiplies by R, which is constant for a given query. The usual approach is to extract all terms from the relevant doc- uments, rank them in order of oﬀer weight by formula (3.5), and add only the top x terms from this ranked list ( x m a yb eo ft h e order of 10). This approach to query expansion was intended for the binary inde- pendence model and RSJ weighting, but has also been used with some success for BM25 (see Section 3.4 below). But it clearly has some limi- tations. As we add more and more terms to the query, we are likely to introduce synonyms or closely related words (indeed, this is why we do query expansion in the ﬁrst place). However, in [36, 37] authors argue that this query expansion may worsen the term independence assump- tion; they propose an extension of the PRF model which attempts to correct this by taking into account some of the semantic structure of the query. 3.3 Blind Feedback The same principles may be used in what is now known as pseudo-"
"bm25_prf_p21_c002","bm25_prf","bm25_prf.pdf","21","2","propose an extension of the PRF model which attempts to correct this by taking into account some of the semantic structure of the query. 3.3 Blind Feedback The same principles may be used in what is now known as pseudo- relevance or blind feedback. Here we assume that we have no actual relevance judgements, but we run an initial search on an initial version of the query (using only original query terms), take the top-ranked y documents (say 5 or 10), assume them to be relevant, and then follow the above relevance feedback procedures."
"bm25_prf_p22_c001","bm25_prf","bm25_prf.pdf","22","1","352 Derived Models We note, however, the following: 1. Blind feedback is generally known to be capable of improving search results on average, but tends to fail on some queries, particularly on queries that are diﬃcult to start with, where the top-ranked documents from the initial search may be poor. 2. The (true) relevance feedback procedure described above is in some sense correct in terms of the probabilistic model, on the assumption that the relevance judgements are good. In the blind feedback case, we have a set of documents whose relevance is (at best) likely rather than sure. It would be better to take account of the probability of relevance of each of these top-ranked documents, rather than simply assum- ing relevance. Such a method could be devised if we had a calibrated probability of relevance for each of these docu- ments. However, the fact that we do not normally have such a calibrated probability in the present model, as discussed in Section 2.5, makes it more diﬃcult to see how to accomplish this. 3.4 The Eliteness Model and BM25 We now re-introduce term frequencies into our model; this requires a model of how diﬀerent term frequencies might arise in a document (this model is originally due to Harter [19]). We suppose that for any document-term pair, there is a hidden property which we refer to as eliteness. This can be interpreted as a form of aboutness: if the term is elite in the document, in some sense the document"
"bm25_prf_p22_c002","bm25_prf","bm25_prf.pdf","22","2","We suppose that for any document-term pair, there is a hidden property which we refer to as eliteness. This can be interpreted as a form of aboutness: if the term is elite in the document, in some sense the document isabout the concept denoted by the term. Now we assume that actual occurrences of the term in the document depend on eliteness, and that there may be an association between eliteness (to the term) and relevance (to the query). But we further assume that these relations are enough to explain the association between term frequency tf and relevance to the query — that is, given the two assumed dependencies, tf is independent of Rel. As before, we can illustrate the assumptions by means of a graphical model, as in Figure 3.1."
"bm25_prf_p23_c001","bm25_prf","bm25_prf.pdf","23","1","3.4 The Eliteness Model and BM25 353 Fig. 3.1 Graphical model of eliteness (E). We further assume that eliteness itself is binary. The following development, using Harter’s ideas in the context of the PRF, was originally proposed in part in [45] and in full (up to and including BM25) in [46]. 3.4.1 The 2-Poisson Model We introduce some new notation. The eliteness random variableE can take two values: Eliteness E: elite , elite (elite, not elite) We now decompose all the probabilities we want using these two disjoint events, following this pattern: P(α|β)= P(α|elite)P(elite|β)+ P(α|elite)P(elite|β) The relationship between eliteness and relevance is denoted thus: pi1 := P(Ei = elite|rel); pi0 := P(Ei = elite|rel) The relationship between term frequencies and eliteness is denoted thus: Ei1(tf ): =P(TF i = tf |Ei = elite); Ei0(tf ): =P(TF i = tf |Ei = elite) Now, following the above pattern, we arrive at expressions for all the probabilities we are interested in relating the observedtf s to relevance"
"bm25_prf_p24_c001","bm25_prf","bm25_prf.pdf","24","1","354 Derived Models like the following: P(TF i = tf |rel) =pi1Ei1(tf )+( 1 − pi1)Ei0(tf ), etc. This gives us an equation for our term-weights: welite i = log(p1E1(tf )+( 1 − p1)E0(tf ))(p0E1( 0 )+( 1− p0)E0(0)) (p1E1( 0 )+( 1− p1)E0(0))(p0E1(tf )+( 1 − p0)E0(tf )) (3.6) (for readability, we dropped thei subscript from all the elements in the fraction). More speciﬁcally, we make distributional assumptions about these events. Again following Harter, we assume that the distributions of term frequencies across documents, conditioned on eliteness, are Poisson: E ie(tf ) ∼P (λie) (Poisson with mean λie), (3.7) where e ∈{ 0,1}. In general, we expect λi1 >λ i0. Thus in the entire collection of documents, which is a mixture of elite and non-elite doc- uments, tf is distributed as a mixture of two Poisson distributions — so that this model is known as the 2-Poisson model. The consequences of these distributional assumptions are analysed below. The nature of the 2-Poisson model deserves further discussion. In Harter’s original formulation, it was applied to abstracts rather than full-text documents, and indeed it can be said to assume that docu- ments are of ﬁxed length. We can interpret the model as follows. We assume that each document is generated by ﬁlling a certain number of word-positions (ﬁxed length) from a vocabulary of words. Further- more, we assume a simple multinomial distribution over words, so that for each position each word has a ﬁxed (small) probability of being chosen, independent of"
"bm25_prf_p24_c002","bm25_prf","bm25_prf.pdf","24","2","ﬁlling a certain number of word-positions (ﬁxed length) from a vocabulary of words. Further- more, we assume a simple multinomial distribution over words, so that for each position each word has a ﬁxed (small) probability of being chosen, independent of what other words have been chosen for other positions. Then it follows that the distribution of tf s for a given word is binomial, which approximates to a Poisson under these conditions [16, VI.5]. Now the eliteness model can be seen as a simple topical model which causes variation in the unigram distributions. The author is assumed ﬁrst to choose which topics to cover, i.e., which terms to treat as elite and which not. This deﬁnes speciﬁc probabilities for the unigram model,"
"bm25_prf_p25_c001","bm25_prf","bm25_prf.pdf","25","1","3.4 The Eliteness Model and BM25 355 and the author then ﬁlls the word-positions according to this chosen model. This generative version of the 2-Poisson model (that is, a model for how documents are generated) ties it very closely with the language models and topical models discussed further in Sections 4.3 and 4.5. We note the following characteristics of this model: 1. The model of topicality is a very simple one — one word one topic. 2. There is no attempt to normalise the probabilities across the full unigram model for the document. 3. The model depends fairly crucially on the notion that all documents are of the same (ﬁxed) length. We do not in the present survey attempt to do anything about the ﬁrst two points — however, they do point forward to the more recent work on language models, discussed brieﬂy below. Concerning the third point, the issue of document-length normalisation is critical to the present model, and is discussed in Section 3.4.5. 3.4.2 Saturation If we plug the Poisson distributional assumptions into Equation (3.6), we can express the term weight as a function of the two meansλ e and the mixing proportion of elite and non-elite documents in the collection (as well as the observed tf ). This is a somewhat messy formula, and furthermore we do not in general know the values of these three param- eters, or have any easy way of estimating them. The next step in the development of the model was therefore"
"bm25_prf_p25_c002","bm25_prf","bm25_prf.pdf","25","2","This is a somewhat messy formula, and furthermore we do not in general know the values of these three param- eters, or have any easy way of estimating them. The next step in the development of the model was therefore to investigate the qualitative behaviour of the term-weighting function, under diﬀerent conditions, in the hope of arriving at a much simpler expression which would capture its dominant behaviour [46]. Clearly its exact behaviour depends on the parameters, but some generalisations are possible. We note in particular that: 1. w elite i (0) = 0 (this is by design); 2. welite i (tf ) increases monotonically with tf ;"
"bm25_prf_p26_c001","bm25_prf","bm25_prf.pdf","26","1","356 Derived Models 3. ... but asymptotically approaches a maximum value as tf →∞ ; and 4. the asymptotic limit being lim tf →∞ welite i (tf ) = log p1(1 − p0) (1 − p1)p0 (3.8) = wBIM i . (3.9) This last formulation is the weight that the eliteness feature on its own would have. That is, if eliteness were observable, instead of being hidden, we could treat it like a simple binary attribute and weight it in exactly the same way as we weighted term presence in the binary independence model. This asymptotic property makes perfect sense. Given (as we have assumed) that the only association betweentf and relevance is via elite- ness, the best information we can hope to get from a term is that the document is indeed elite for that term. In reality our information on this score is probabilistic, and thus the term weight is correspondingly reduced. Although this behaviour of the weighting function has been established only for the 2-Poisson case, it seems likely thatany reason- able distributional assumptions would exhibit similar characteristics. We refer to this behaviour as saturation. That is, any one term’s contribution to the document score cannot exceed a saturation point (the asymptotic limit), however, frequently it occurs in the document. This turns out to be a very valuable property of the BM25 weighting function deﬁned below. 3.4.3 A Special Case There is one case in which the saturation limit does not apply. If we assume that the"
"bm25_prf_p26_c002","bm25_prf","bm25_prf.pdf","26","2","in the document. This turns out to be a very valuable property of the BM25 weighting function deﬁned below. 3.4.3 A Special Case There is one case in which the saturation limit does not apply. If we assume that the eliteness property for each query term coincides with relevance for the query/need, so thatp i1 = 1 andpi0 = 0, then the limit is inﬁnite, and the weight becomes linear in tf . Thus the commonly used term-weighting functions such as the traditional tf *idf , linear in tf , seem to ﬁt with such a model. However, the non-linear, saturating function of tf developed below (also combined with anidf component) has frequently been shown to work better than traditional tf *idf ."
"bm25_prf_p27_c001","bm25_prf","bm25_prf.pdf","27","1","3.4 The Eliteness Model and BM25 357 3.4.4 BM25 Precursor We investigate the shape of the saturation function a little more closely. It is clear that the properties listed above severely limit the possible functions; nevertheless, there remain many possibilities, as illustrated for example in the left-hand graph in Figure 3.2. However, the 2-Poisson model generates much smoother functions, as shown in the right-hand graph. For most realistic combinations of the parameters the curve is convex, as the top two lines; for some combinations it has an initial concavity, as the bottom line. The next step in the development of BM25 is to approximate this shape. Lacking an appropriate generative corpus model from which to derive a convenient formula, the authors of BM25 decided to ﬁt a simple parametric curve to this shape. The following one-parameter function was chosen: tf k + tf for some k> 0 (3.10) This function satisﬁes the properties listed above, and ﬁts well the possible convex curves. We show values of this function for three diﬀer- ent values ofk in Figure 3.3; the middle line is fork = 1, the upper line for lower k and the lower line for higherk. Note that because we apply this to all terms, the absolute height does not matter; what matters is the relative increments for diﬀerent increments in tf . Thus for high k, increments in tf continue to contribute signiﬁcantly to the score, Fig. 3.2 Left: some possible saturation functions. Right: saturation functions generated by the"
"bm25_prf_p27_c002","bm25_prf","bm25_prf.pdf","27","2","matter; what matters is the relative increments for diﬀerent increments in tf . Thus for high k, increments in tf continue to contribute signiﬁcantly to the score, Fig. 3.2 Left: some possible saturation functions. Right: saturation functions generated by the 2-Poisson model."
"bm25_prf_p28_c001","bm25_prf","bm25_prf.pdf","28","1","358 Derived Models 123456789 1 0 0.0 0.2 0.4 0.6 0.8 1.0 x x / (x + k ) k = 0.2 k = 1 k = 3 k = 10 123456789 1 0 0.0 0.2 0.4 0.6 0.8 1.0 x x / (x + k ( (1 − b) + b*dl/avdl) ) dl = avdl dl = avdl * 0.1 dl = avdl * 10 Fig. 3.3 Saturation functions generated by Equation (3.10) with raw frequencies (left) and with frequencies normalised using Equation (3.13) ( right). Stronger saturation is obtained with lower values of k (left) and with shorter documents ( right). For the plot on the right we used k = 1 and b =0 .5. whereas for low k, the additional contribution of a newly observed occurrence tails oﬀ very rapidly. We obtain an early version of BM25 by combining the saturation function of Equation (3.10) with an approximation to the asymptotic maximum of Equation (3.9). The latter is obtained by using the old RSJ presence–absence term weight of Equation (3.2): w i(tf )= tf k + tf wRSJ i (3.11) (We will modify this below for the ﬁnal version of BM25.) The main thing missing so far from the analysis is the question of document length. 3.4.5 Document Length The ﬁxed-document-length assumption was made to allow a connection between a simple language model and BM25; we imagined an author ﬁlling a ﬁxed number of slots with a ﬁxed unigram model. Now we imagine instead that the"
"bm25_prf_p28_c002","bm25_prf","bm25_prf.pdf","28","2","3.4.5 Document Length The ﬁxed-document-length assumption was made to allow a connection between a simple language model and BM25; we imagined an author ﬁlling a ﬁxed number of slots with a ﬁxed unigram model. Now we imagine instead that the author also chooses a document length. We suppose that there is something like a standard length for a document, but that an author may decide to make a document longer"
"bm25_prf_p29_c001","bm25_prf","bm25_prf.pdf","29","1","3.4 The Eliteness Model and BM25 359 or shorter; we consider only the longer case. Why might an author so decide? We can postulate two extreme cases: V erbosity: Some authors are simply more verbose than others, using more words to say the same thing. Scope: Some authors have more to say: they may write a single document containing or covering more ground. An extreme version would have the author writing two or more documents and concatenating them. The verbosity hypothesis suggests that we should simply normalise any observed tf s by dividing by document length. The scope hypothesis, on the other hand, at least in its extreme version, suggests the opposite. In a real collection of documents we will observe variations in length, which might be due to either eﬀect, or to a combination. We suppose in general a combination: that each hypothesis represents some partial explanation for the observed variation. This in turn suggests that we should apply some kind of soft normalisation. We deﬁne document length in an obvious way: document length: dl := ∑ i∈V tf i and also an average document length for the collection: average doclength: avdl (average over collection) The length normalisation component will be deﬁned in relation to the average; this ensures that the deﬁnition of document length used is not critical. In practice, we could take (for example) the number of characters in the document, or the number of words before parsing, or even the number of unique terms, and still"
"bm25_prf_p29_c002","bm25_prf","bm25_prf.pdf","29","2","that the deﬁnition of document length used is not critical. In practice, we could take (for example) the number of characters in the document, or the number of words before parsing, or even the number of unique terms, and still get very similar results. The soft length normalisation component is: B := ( (1 − b)+ b dl avdl ) , 0 ≤ b ≤ 1 (3.12) Thus setting b = 1 will perform full document-length normalisation, while b = 0 will switch normalisation oﬀ. Now we use B to normalise"
"bm25_prf_p30_c001","bm25_prf","bm25_prf.pdf","30","1","360 Derived Models tf , before applying the saturation function, as follows: tf ′ = tf B (3.13) wBM25 i (tf )= tf ′ k1 + tf ′ wRSJ i (3.14) = tf k1 ( (1 − b)+ b dl avdl ) + tf wRSJ i (3.15) This is the classic BM25 term-weighting and document-scoring func- tion. As with all term-document weights deﬁned in this survey, the full document score is obtained by summing these term-weights over the (original or expanded) set of query terms. 3.5 Uses of BM25 In order to use BM25 as a ranking function for retrieval, we need to choose values for the internal parametersb and k 1, and also instantiate RSJ. Concerning the RSJ weight (Equation (3.2)), all the previous com- ments apply. In particular, it can be used with or without relevance information. In the absence of relevance information, it reduces as before to a form of idf . In this case, the BM25 weight looks very much like a traditional tf ∗idf weight — a product of two components, one based on tf and one onidf . However, there is one signiﬁcant diﬀerence. The tf component involves the saturation function discussed, and is therefore somewhat unlike most other tf functions seen in the litera- ture, where common choices are tf itself and (1 + logtf ). The latter has a somewhat similar shape curve, but does not have an asymptotic maximum — it goes to inﬁnity, even if somewhat slower than tf itself."
"bm25_prf_p30_c002","bm25_prf","bm25_prf.pdf","30","2","litera- ture, where common choices are tf itself and (1 + logtf ). The latter has a somewhat similar shape curve, but does not have an asymptotic maximum — it goes to inﬁnity, even if somewhat slower than tf itself. Concerning the internal parameters, the model provides no guid- ance on how these should be set. This may be regarded as a limi- tation of the model. However, it provides an opportunity for optimi- sation, given some evaluated set of queries and relevance judgements in the traditional retrieval experiment style. A signiﬁcant number of such experiments have been done, and suggest that in general values s u c ha s0.5 <b< 0.8 and 1 .2 <k 1 < 2 are reasonably good in many"
"bm25_prf_p31_c001","bm25_prf","bm25_prf.pdf","31","1","3.6 Multiple Streams and BM25F 361 circumstances. However, there is also evidence that optimal values do depend on other factors (such as the type of documents or queries). 3.5.1 Some V ariations on BM25 Published versions of BM25 can vary somewhat (the original BM25 [46] was a little more complicated than that of Equation (3.15), for example). Here we indicate some diﬀerences that might be encountered in diﬀerent versions of the function in published sources. • The original had a component for within-query term fre- quency qtf , for longer queries where a term might occur mul- tiple times. In its full generality, this had a similar saturation function to that used for tf , but with its own k 3 constant. However, experiments suggested that the saturation eﬀect for qtf was unimportant, leading to a formula which was linear in qtf . In other words, one could simply treat multiple occur- rences of a term in the query as diﬀerent terms. • The original also had a further correction for document length, to the total document score. This correction was again found to be unimportant. • A common variant is to add a ( k1 + 1) component to the numerator of the saturation function. This is the same for all terms, and therefore does not aﬀect the ranking produced. The reason for including it was to make the ﬁnal formula more compatible with the RSJ weight used on its own. If it is included, then a single occurrence"
"bm25_prf_p31_c002","bm25_prf","bm25_prf.pdf","31","2","for all terms, and therefore does not aﬀect the ranking produced. The reason for including it was to make the ﬁnal formula more compatible with the RSJ weight used on its own. If it is included, then a single occurrence of a term would have the same weight in both schemes. • Some published versions are based on speciﬁc values assigned to b and k 1. A common combination would be b =0 .5 and k1 = 2. (However, many experiments suggest a somewhat lower value of k1 and a somewhat higher value of b.) 3.6 Multiple Streams and BM25F So far, all the arguments in this survey have been based on the idea that the document is a single body of text, unstructured and undiﬀerentiated"
"bm25_prf_p32_c001","bm25_prf","bm25_prf.pdf","32","1","362 Derived Models in any way. However, it is commonplace in search systems to assume at least some minimal structure to documents. In this section we consider documents which are structured into a set of ﬁelds or streams. The assumption is that there is a single ﬂat stream structure, common to all documents. That is, there is a global set of labelled streams, and the text of each document is split between these streams. An obvious example is a title/abstract/body structure such as one might see in sci- entiﬁc papers. In the Web context, with extensive hyperlinks, it is usual to enhance the original texts with the anchor text of incoming links. This is clearly a very minimal kind of structure; one can imagine many document structures that do not ﬁt into this framework. Never- theless, such minimal structures have proved useful in search. The gen- eral idea, not at all conﬁned to the present framework but implemented in many diﬀerent ways in diﬀerent systems, is that some streams may be more predictive of relevance than others. In the above examples, a query match on the title might be expected to provide stronger evi- dence of possible relevance than an equivalent match on the body text. It is now well known in the Web context that matching on anchor text is a very strong signal. 3.6.1 Basic Idea Given a ranking algorithm or function that can be applied to a piece of undiﬀerentiated text, an obvious practical approach to such"
"bm25_prf_p32_c002","bm25_prf","bm25_prf.pdf","32","2","known in the Web context that matching on anchor text is a very strong signal. 3.6.1 Basic Idea Given a ranking algorithm or function that can be applied to a piece of undiﬀerentiated text, an obvious practical approach to such a stream structure would be to apply the function separately to each stream, and then combine these in some linear combination (with stream weights) for the ﬁnal document score. In terms of the eliteness model, this approach may be regarded as assuming a separate eliteness property for each term/stream pair. Thus for a given term, eliteness in title would be assumed to be a diﬀerent property from eliteness in body. Actu- ally, the assumption would be even stronger: we would have to apply the usual assumptions of independence (given relevance) between these distinct eliteness properties for the same term. This seems a little unreasonabl e—ab etter assumption might be that eliteness is a term/document property, shared across the streams of the document. We would then postulate that the relationship of"
"bm25_prf_p33_c001","bm25_prf","bm25_prf.pdf","33","1","3.6 Multiple Streams and BM25F 363 eliteness to term frequency is stream-speciﬁc. In terms of the underlying language model discussed above, we might imagine that the author chooses a length for (say) the title and another for the body. Then, given eliteness (from the author’s choice of topics to cover), the unigram probabilities for the language model for ﬁlling the term-positions would also be stream-speciﬁc. In particular, there would be a much stronger bias to the elite terms when choosing words for the title than for the body (we expect a title to be much denser in topic-speciﬁc terms than an average body sentence). The consequence of this term-document eliteness property is that we should combine evidence across terms and streams in the opposite order to that suggested above: ﬁrst streams, then terms. That is, for each term, we should accumulate evidence for eliteness across all the streams. The saturation function should be applied at this stage, to the total evidence for each term. Then the ﬁnal document score should be derived by combination across the terms. 3.6.2 Notation We have a set of S streams, and we wish to assign relative weights v s to them. For a given document, each stream has its associated length (the total length of the document would normally be the sum of the stream lengths). Each term in the document may occur in any of the streams, with any frequency; the total across streams of these term– stream frequencies would be the usual"
"bm25_prf_p33_c002","bm25_prf","bm25_prf.pdf","33","2","of the document would normally be the sum of the stream lengths). Each term in the document may occur in any of the streams, with any frequency; the total across streams of these term– stream frequencies would be the usual term-document frequency. The entire document becomes a vector of vectors. streams s =1 ,...,S stream lengths sl s stream weights vs document ( tf1,..., tf|V|) vector of vectors tf i vector ( tf 1i,..., tf Si) where tf si is the frequency of term i in stream s. 3.6.3 BM25F The simplest extension of BM25 to weighted streams is to calculate a weighted variant of the total term frequency. This also implies having"
"bm25_prf_p34_c001","bm25_prf","bm25_prf.pdf","34","1","364 Derived Models a similarly weighted variant of the total document length: ˜tf i = S∑ s=1 vs tf si (3.16) ˜dl = S∑ s=1 vs sl s (3.17) ˜avdl = average of ˜dl across documents wsimpleBM25F i = ˜tf i k1 ( (1 − b)+ b ˜dl ˜avdl ) + ˜tf i wRSJ i (3.18) However, we may also want to allow the parameters of BM25 to vary between streams — it may be that the diﬀerent streams have diﬀerent characteristics, e.g., in relation to verbosityv. scope (as deﬁned in Section 3.4.5). It is in fact possible to re-arrange formula (3.15) so as to include any of the following in the stream-speciﬁc part: k 1, b, wRSJ i or its non-feedback version wIDF i . We have in particular found it useful to allowb to be stream-speciﬁc; we present here the appropriate version of BM25F: ˜tf i = S∑ s=1 vs tf si Bs (3.19) Bs = ( (1 − bs)+ bs sl s avsl s ) , 0 ≤ bs ≤ 1 (3.20) wBM25F i = ˜tf i k1 + ˜tf i wRSJ i (3.21) This version was used in [62]; the simple version in [50]. As usual, in the absence of relevance feedback information,wRSJ i should be replaced by wIDF i . In [50, 62] we computed IDFs on the entire collection disregard- ing streams. This worked well in practice, but it can lead to degener- ate cases (e.g., when a stream is extremely verbose and contains"
"bm25_prf_p34_c002","bm25_prf","bm25_prf.pdf","34","2","replaced by wIDF i . In [50, 62] we computed IDFs on the entire collection disregard- ing streams. This worked well in practice, but it can lead to degener- ate cases (e.g., when a stream is extremely verbose and contains most terms for most documents). The proper deﬁnition of IDF in this con- text requires further research (this is also discussed in [37] where the notion of an expected idf is introduced)."
"bm25_prf_p35_c001","bm25_prf","bm25_prf.pdf","35","1","3.7 Non-Textual Relevance Features 365 Table 3.1. BM25F parameters reported in [62] for topic distillation (TD) and name page (NP) search tasks Parameter TD’03 NP’03 k1 27.5 4.9 btitle 0.95 0.6 bbody 0.7 0.5 banchor 0.6 0.6 vtitle 38.4 13.5 vbody 1.0 1.0 vanchor 35 11.5 As an illustration, we report in Table 3.1 the BM25Fweights reported in [62] for the 2003 TREC Web Search tasks. 3.6.4 Interpretation of the Simple V ersion It is worth mentioning a very transparent interpretation of the simple version — although it does not apply directly to the version with vari- able b, it may give some insight. If the stream weights v s are inte- gers, then we can see the simple BM25F formula as an ordinary BM25 function applied to a document in which some of the streams have been replicated. For example, if the streams and weights are{v title =5 , vabstract =2 , vbody =1}, then formula 3.18 is equivalent to 3.15 applied to a document in which the title has been replicated ﬁve times and the abstract twice. 3.7 Non-Textual Relevance Features In many collections there are other sources of relevance information besides the text. Things like the age of a ﬁle, its type or its link in-degree may provide useful information about the relevance of a document. The development of the PRF is quite general and does not make explicit reference to terms or text; it is therefore possible, at least in principle, to take non-textual features into"
"bm25_prf_p35_c002","bm25_prf","bm25_prf.pdf","35","2","provide useful information about the relevance of a document. The development of the PRF is quite general and does not make explicit reference to terms or text; it is therefore possible, at least in principle, to take non-textual features into account. We will make two assumptions here about non-textual features. First we make the usual assumption of feature independence with respect to relevance odds (as discussed in Section 2). Second, we will assume that all non-textual features provide relevance information"
"bm25_prf_p36_c001","bm25_prf","bm25_prf.pdf","36","1","366 Derived Models which is independent of the query . With these two assumptions we can integrate non-textual features very easily into the PRF and BM25 scoring frameworks. It is possible in principle to relax these assumptions and derive more sophisticated models of relevance for non-textual features. Let us call c =( tf 1,..., tf |V|) the vector of term frequency counts of document d, and let us call f an extra vector of non-textual features f =( f1,...,f F ). We have that d =( c, f). Note that the initial PRF development (Equations (2.1–2.4) in Sec- tion 2) applies also to this extended version ofd. Equation (2.4) makes the assumption of feature independence, which carries over the non- textual features. Therefore the product in Equation 2.4 would include all non-textual features f. In Equation (2.5), where we drop all the non-query terms inc, none of the terms inf would be dropped — non-textual features are assumed to be correlated with relevance for all queries. After taking the log in Equation (2.6) we see that the addition of non-textual features simply adds a new term to the sum. Removing the zeros in Equations (2.7–2.9) does not aﬀect this term, so after (2.11) we may write: P(rel|d,q) ∝ q ∑ q,tf i>0 Wi(tf i)+ F∑ i=1 λiVi(fi), (3.22) where Vi(fi): =l o gP(Fi = fi |rel) P(Fi = fi |rel) (3.23) We have artiﬁcially added a free parameter λi to account for re- scalings in the approximation of Wi and"
"bm25_prf_p36_c002","bm25_prf","bm25_prf.pdf","36","2","q,tf i>0 Wi(tf i)+ F∑ i=1 λiVi(fi), (3.22) where Vi(fi): =l o gP(Fi = fi |rel) P(Fi = fi |rel) (3.23) We have artiﬁcially added a free parameter λi to account for re- scalings in the approximation of Wi and Vi. We note that features fi may well be multi-valued or continuous, and this implies the need for care in the choice of function V (fi) (just as we paid attention to ﬁnding a good function oftf ). This will depend on the nature of each non-textual feature fi and our prior knowledge about it. Here are some Vi functions that we have used with success in the past for diﬀerent features: • Logarithmic: log(λ′ i + fi)"
"bm25_prf_p37_c001","bm25_prf","bm25_prf.pdf","37","1","3.8 Positional Information 367 • Rational: fi/(λ′ i + fi) • Sigmoid: [λ′ i + exp(−fi λ′′ i )]−1 This development can explain for example why simple scoring functions such as BM25F(q,d)+log(PageRank (d)) may work well in practice for Web Search retrieval. This can much improved by adding the scaling parameter λ, and it can be further improved (only slightly) by changing the log into a sigmoid and tuning the two extra parameters λ ′ and λ′′ [12]. In our work we developed more sophisticated ranking functions integrating several forms of non-textual information and using over a dozen parameters [12, 13]. The optimisation of these parameters is discussed in Section 5. 3.8 Positional Information For the most part the PRF ignores positional information: it cares only about the number of occurrences of a term, but not about their position. There are two important reasons that have held back the PRF from considering positional information: (i) it is extremely hard to develop a sound formal model of relevance which takes into account positional information without exploding the number of parameters, and (ii) position information has been shown to have surprisingly little eﬀect on retrieval accuracy on average. In this section we only give an overview of the existing approaches and discuss the main diﬃculties. We also propose some intuitions of why position may not be as crucial as it seems at ﬁrst sight. Why is it so hard to model relevance with respect to the position of terms in the query"
"bm25_prf_p37_c002","bm25_prf","bm25_prf.pdf","37","2","the main diﬃculties. We also propose some intuitions of why position may not be as crucial as it seems at ﬁrst sight. Why is it so hard to model relevance with respect to the position of terms in the query and document? Several problems appear. First, we need to deﬁne an appropriate universe of events. In the traditional PRF this universe is simplyN |q|, all possible term frequency vectors of terms in the query. The most natural way to consider positions would be to characterise all sequences of terms in the query separated by some num- ber of non-query terms. This leads to an excessively high-dimensional space, and one that is very hard to factor it in an appropriate way. In the traditional model the natural unit over which we build (factorise) the required quantities is the occurrence of a term a given number of"
"bm25_prf_p38_c001","bm25_prf","bm25_prf.pdf","38","1","368 Derived Models times. What would be the natural unit over which to build (factorise) the probabilities required to model positions adequately? There have been a number of attempts to deal with these issues and introduce positional information into BM25 (see for example [3, 54, 51] and references therein). The three main approaches used are the following: 1. Indexing phrases as individual terms. This approach is ideal for multi-term tokens (such as White House ) for which a partial match would in fact be incorrect. Its implementation is extremely simple since one does not need to modify the ranking function in any way (each phrase would have its own tf and idf ). There are three problems however: (i) it does not take into account partial matches; (ii) it does not take into account terms which are close but not exactly adjacent; and (iii) one needs to deﬁne the valid phrases [48]. 2. Scoring spans of text instead of entire documents. This can be done explicitly, with a passage retrieval ranking func- tion [3], or implicitly by constructing a ranking function that integrates scores computed over many document spans [51]. 3. Deriving position features (such as the minimum and maxi- mum length of the document spans containing all the terms in the query) which can then be integrated into the scoring function as non-textual features (as those in Section 3.7) [54]. In our opinion none of these attempts would qualify as anatural exten- sion of the PRF, since it is"
"bm25_prf_p38_c002","bm25_prf","bm25_prf.pdf","38","2","in the query) which can then be integrated into the scoring function as non-textual features (as those in Section 3.7) [54]. In our opinion none of these attempts would qualify as anatural exten- sion of the PRF, since it is not clear what the assumptions about rele- vance are. Metzler [32, 33] proposes a novel probabilistic retrieval model which makes clear assumptions about positions and relevance, and could perhaps be integrated within the PRF. The model estimates the probability of relevance of document and query jointly: P(Q,D |Rel). This is done by a Markov Random Field (MRF) which can take into account term-positions in a natural way. The MRF can use any appro- priately deﬁned potentials: while the original work used LM-derived potentials, BM25-like potentials were used in [31]. However, even when using BM25-like potentials, we cannot call this model an extension of"
"bm25_prf_p39_c001","bm25_prf","bm25_prf.pdf","39","1","3.9 Open Source Implementations of BM25 and BM25F 369 the PRF, since it models a diﬀerent distribution:P(D,Q |Rel) instead of the posterior P(Rel|D,Q). We end this section with a brief discussion of why position infor- mation may not be as important as it may seem at ﬁrst view. It is sobering to see how hard it has been in the past to eﬀectively use prox- imity in IR experiments. All the works referenced in this section claim statistically signiﬁcant improvements over non-positional baselines, but the improvements reported are small. We believe this is specially the case for small collections and high recall situations (typical of academic IR evaluations), since position information is a precision enhancement technique. But even in large collections with high-precision require- ments (such as realistic Web Search evaluations) the gains observed are small. Why is this? We do not know of theoretical or empirical studies about this, but we propose here two hypotheses. First, we believe that natural language, and queries in particular, are quite robust. We would argue that for many queries, a human could determine the relevance of a document to a query even after words in the document and query were scrambled. And for cases that this is not true, it is likely that the user would unconsciouslycorrect the query by adding terms to it that disambiguate it. This does not mean that all queries and documents are position-proof, but the fraction that require positions is small. Second, it should be noted that taking"
"bm25_prf_p39_c002","bm25_prf","bm25_prf.pdf","39","2","the user would unconsciouslycorrect the query by adding terms to it that disambiguate it. This does not mean that all queries and documents are position-proof, but the fraction that require positions is small. Second, it should be noted that taking into account the structure of a document (e.g., in BM25F) implicitly rewards prox- imity within important short streams, such as the title. 3.9 Open Source Implementations of BM25 and BM25F We review here several implementations of BM25 and BM25F available as open source. This list is not exhaustive, there may be other search engines or extensions of them that implement BM25 and BM25F. As far as we know only the MG4J [9, 34] fully implements BM25F (version 2.2 or later). BM25 is implemented by Lemur, MG4J, Okapi, PF/Tija, Terrier, Wumpus, Xapian and Zettair [22, 27, 34, 35, 39, 56, 57, 60, 61, 63]. All these search engines are quite modular and could in principle be modiﬁed to extend BM25 in a number of ways, in particular to implement BM25F. Lucene [29] does not implement BM25"
"bm25_prf_p40_c001","bm25_prf","bm25_prf.pdf","40","1","370 Derived Models but there exist a third party Lucene extensions that implement BM25 and BM25F [38]. We inspected the code of all these implementations and we believe they are correct. 1 (inspection was visual: we did not run tests ourselves; in two cases implementation was incorrect and we asked the authors to correct it, which they did). None of the systems above provide support for parameter optimisation, although it should not be diﬃcult to extend them for this. 1 There are some minor diﬀerences in BM25 implementations in these packages at the time of writing this survey. For example, PF/Tijah uses idf= logN +0.5 n+0.5 , Terrier does not allow modifying k programmatically and Xapian does not allow modifying any parameters pro- grammatically."
"bm25_prf_p41_c001","bm25_prf","bm25_prf.pdf","41","1","4 Comparison with Other Models The ﬁrst probabilistic model for retrieval was proposed by Maron and Kuhns in 1960 [30]. It was similarly based on a notion of probability of relevance; however, there was an interesting discrepancy between the Maron and Kuhns approach and that of Robertson and Sp¨arck Jones ﬁfteen years later. The discrepancy was the subject of research in the early 1980s on a uniﬁcation of the two. In order to explain both the discrepancy and the attempted uniﬁcation, we ﬁrst describe the Maron and Kuhns model. 4.1 Maron and Kuhns The situation envisaged by Maron and Kuhns was that of a librarian indexing a book (document). The idea was that indexing should antici- pate how people make requests in the ﬁrst place. Ideal indexing should match the requests of just those people who would want to be pointed to this monograph — those people who would ﬁnd it relevant to their needs. The system was assumed to be a system of subject headings, any of which might be assigned by the librarian to the book in ques- tion; a user request would take the form of a chosen subject heading to 371"
"bm25_prf_p42_c001","bm25_prf","bm25_prf.pdf","42","1","372 Comparison with Other Models look under. Thus the librarian would be invited to estimate, in respect of each candidate subject heading, the probability that a user looking there would ﬁnd this particular book relevant. Thus far, the basis for the model looks very like the PRF deﬁned in Section 2. However, to better understand the nature of the probability of relevance as interpreted by Maron and Kuhns, we need to consider the event space in which it is deﬁned. This will give us the basis for a comparison in the following section. We have (at least in the mind of the librarian) a group of users (with information needs) who look under a particular subject heading. On the other hand, we have a single individual book in front of the librarian. Therefore the event space involved, in which the probability should be deﬁned, is a space of users. If we were to attempt to use feedback to estimate such a probability, we would ideally count users — that is, of all the users who express their queries by means of a particular subject heading, what proportion would like this monograph. This kind of approach has recently acquired new force, because the big Web Search engines typically see some queries (the so-called ‘head’ queries) repeated very many times by diﬀerent users. Click log data looks a little like relevance feedback on individual documents from a population of users with similar queries. However, the relation between this and the PRF discussed"
"bm25_prf_p42_c002","bm25_prf","bm25_prf.pdf","42","2","(the so-called ‘head’ queries) repeated very many times by diﬀerent users. Click log data looks a little like relevance feedback on individual documents from a population of users with similar queries. However, the relation between this and the PRF discussed in this monograph needs further analysis. 4.2 The Uniﬁed Model We re-visit the original RSJ model, the foundation of the model pre- sented in this survey, in order to deﬁne it in similar terms. In this case, we start with a single individual user, who puts a request using cer- tain words. Now we ask the question, what is the probability that any arbitrary document matching one (or a combination) of these words is relevant to the user. Thus the event space here consists of documents, and if we want to use feedback to estimate the probability, we would count documents, as in Section 3.1. It immediately becomes clear that although both models refer to probability of relevance, they deﬁne their respective versions of this"
"bm25_prf_p43_c001","bm25_prf","bm25_prf.pdf","43","1","4.2 The Uniﬁed Model 373 probability in diﬀerent event spaces. In fact, the two probabilities of relevance are actually quite distinct. The uniﬁcation attempt [43] was based on the following argument. We imagine a matrix of users-with-information-needs against docu- ments (individual users u, individual documents d). Ideally we would like to assign a meaningful probability of relevance to the speciﬁc com- bination of an individual user with an individual document:P(rel|u,d). However, doing this directly looks diﬃcult — at least if we are looking for direct evidence (feedback). If we show d to u and get a judgement, we no longer need to assign a probability! Indeed, it only makes sense to do so when we have classes of similar events. Maron and Kuhns start by classifying users together, according to the subject headings they consult. Thus we are dealing with a class U of users, and ask about P(rel|U,d). On the other hand, Robertson and Sprck Jones classify documents together in classes such as D, and ask about P(rel|u,D). The uniﬁed model proceeded to deﬁne four diﬀerent probabilities of relevance. We might consider starting with P(rel|U,D), which is a general model needing only feedback from similar users about similar documents (this is referred to as Model 0). If we have feedback about the particular document, we can improve this estimate by considering P(rel|U,d) (Model 1). On the other hand, if we have feedback from the particular user, we should go for P(rel|u,D) (Model 2). Finally, if we have both kinds"
"bm25_prf_p43_c002","bm25_prf","bm25_prf.pdf","43","2","feedback about the particular document, we can improve this estimate by considering P(rel|U,d) (Model 1). On the other hand, if we have feedback from the particular user, we should go for P(rel|u,D) (Model 2). Finally, if we have both kinds of more specialist evidence simultaneously (Model 3), we might aim for an even better probability. However, its exact rela- tionship to P(rel|u,d) is not quite obvious, because while it is based on feedback of the above three kinds, it would not actually have feedback on the exact individual pair. The uniﬁed model has been the subject of some more recent work [8], and we are now entering a domain in which many diﬀerent kinds of feedback may be possible, given the kind of logging of Web search- ing behaviour that is now the norm. Other authors (for example [14, 17], and later [25] in the context of the language model discussed below) have approached the problem by in eﬀect limiting themselves to Model 0, by considering onlyrepresentations of documents and queries, rather than individual instances."
"bm25_prf_p44_c001","bm25_prf","bm25_prf.pdf","44","1","374 Comparison with Other Models However, in the present survey we have restricted ourselves to the RSJ view of the probability of relevance. 4.3 The Simple Language Model The language model (LM) of IR is a more recent innovation [24, 26, 33], also with a strong probabilistic motivation. We ﬁrst describe the simple LM, and then some more sophisticated developments. In the LM, we regard any piece of text (document, query, etc.) as having been generated by a statistical process. Outside of IR, such models have been very successful in various areas, particularly in speech recognition, and the IR application has borrowed from that domain. In this particular view of text, it is regarded as being generated word- by-word in sequence, each word being chosen from a vocabulary. The simplest statistical model, so-calledunigram, has a ﬁxed probability dis- tribution over the vocabulary, applied to all word-positions (so actually the sequence is not important). n-gram models assume that the choice of the next word depends on the n − 1 previous words chosen. The simple LM approach to IR assumes that each document has its own model, and asks this question about each document: what is the probability that the query came from (was generated by) the same language model as the document (there is no separate query model in this approach). This probability is used to rank documents for a query. Thus there is no explicit notion of relevance; the implicit notion is that a document is relevant to a"
"bm25_prf_p44_c002","bm25_prf","bm25_prf.pdf","44","2","the document (there is no separate query model in this approach). This probability is used to rank documents for a query. Thus there is no explicit notion of relevance; the implicit notion is that a document is relevant to a query if the query came from the same lan- guage model as the document. This approach also does not distinguish between individual users — a query is understood to be just a text, and each query–document pair is considered as an equivalent individual event. From an individual user point of view, the model implicitly assumes that there is just one relevant document (if this instance of a query was generated by the language model for document 1, it could not also have been generated by the diﬀerent language model for docu- ment 2). However, since the approach does not distinguish individuals, in eﬀect it represents a version of Model 0 (in the terms of the uniﬁed model, as discussed above). Diﬀerent instances of the same query can be generated from diﬀerent documents; in the (U,D) class, more than"
"bm25_prf_p45_c001","bm25_prf","bm25_prf.pdf","45","1","4.4 The Relevance (Language) Model 375 one document can be relevant. But it follows that it makes no sense to consider individual feedback in the context of the simple LM. In more detail, the document language model is usually built by mixing (smoothing) probabilities derived from the document itself with those taken from a general background language model. One purpose of this smoothing is to avoid a zero probability being assigned to any term that does not occur in the document. In general, a rich fund of models and estimation methods has been mined within the general framework of the LM. We further explore only two of these developments in the following sections. 4.4 The Relevance (Language) Model Some of the limitations of the simple model are addressed in work on a relevance model for the LM framework [26, 33]. Here, by contrast, we assume that the query has (that is, is generated by) its own model, distinct from any particular document model. The initial source for this model is clearly the query itself; however, relevance feedback (from the individual user, for example) can provide additional evidence about it. With this approach, the document–query matching process becomes much less obvious. Note that both in the simple LM, and in the tradi- tional probabilistic relevance framework (PRF) described in this survey, the process of matching the query to the document is inherent in the model, entirely determined by the model itself. In this new context, no such matching process is deﬁned;"
"bm25_prf_p45_c002","bm25_prf","bm25_prf.pdf","45","2","tradi- tional probabilistic relevance framework (PRF) described in this survey, the process of matching the query to the document is inherent in the model, entirely determined by the model itself. In this new context, no such matching process is deﬁned; it is necessary to choose one from outside the model. Given that both the document LM and the query (relevance) LM take the form of statistical distributions over a vocabulary, matching becomes a question of matching two statistical distributions. The most common way to do this is to use the Kullback–Leibler (KL) divergence measure. This has good empirical support. 4.5 Topic Models There is perhaps potential for some kind of bridge between the LM approach and the PRF in work on implicit topic models. Most such"
"bm25_prf_p46_c001","bm25_prf","bm25_prf.pdf","46","1","376 Comparison with Other Models work has sought to discover the topics implicit in the statistical structure of the language of documents; examples are Latent Seman- tic Indexing (LSI) [18], Probabilistic LSI [21] and Latent Dirichlet Allocation [7]. The hidden eliteness variables postulated in the proba- bilistic relevance framework have some similarities (although much sim- pliﬁed by the assumption of one eliteness variable per term). A related approach is the parsimonious LM [20], which attempts to model in what ways a particular document or query is distinguished from the general background language. However, we appear to be some distance from a serious uniﬁcation of the LM and the PRF which is the main subject of this survey. 4.6 Divergence from Randomness The DFR models [2], like the language models, do not contain “relevance” as a primitive notion. Also like the language models, they concentrate on the statistical distribution of terms in documents. In general, they seek to identify the ways in which term distributions dif- fer from what one might expect on a random basis — evidence of such divergence is taken implicitly as evidence about relevance. It is possible to derive a variety of term-weighting and document ranking functions within this framework, including a formulation that is approximately the same as BM25."
"bm25_prf_p47_c001","bm25_prf","bm25_prf.pdf","47","1","5 Parameter Optimisation Like most IR models, the models in the PRF have free parameters that need to be set to appropriate values. The BM25 and BM25F models are known to be quite robust with respect to their parameters, mean- ing that small changes in the parameter values (or in the collection) do not produce large changes in accuracy or relevance. Nevertheless signiﬁcant gains in relevance can be obtained by properly optimising the parameters, specially when we deal with a new collection. Parameter optimisation comes with considerable costs: it will require the human evaluation of many query results, which is expensive, and the optimised parameters will be speciﬁc to the collection evalu- ated and may not work well for other collections. Furthermore, the optimisation procedure can be computationally costly, requiring more computing power that the search engine itself. For these reasons this approach is only appropriate for speciﬁc collections which merit the cost needed to optimise the ranking function. Examples of such collec- tions are the WWW, large corporate collections or high-value News or Help sites. Let us call θ the vector of all free parameters of the ranking model being tuned. In the case of BM25 this vector would have two 377"
"bm25_prf_p48_c001","bm25_prf","bm25_prf.pdf","48","1","378 Parameter Optimisation components: θ=( k1,b). In the case of BM25F it would have more: θ=( k1,b1,...,b S,v1,..., vS). If we include non-textual features then we have even more parameters, the exact number depending on the trans- formation used for each non-textual feature. ‘Tuning’ the parameters of the model can be seen as an optimisa- tion problem, where we seek to maximise the relevance of the ranking model with respect to the model parameters θ, for a given set of rele- vance judgements. More speciﬁcally, if we ﬁx the document collection and a set of queries with their associated relevance judgements, then for any parameter setting θwe can compute the performance measure of our choice M(θ). What is left is to ﬁnd the parameter setting that maximises M(θ). This is exactly an n-dimensional optimisation prob- lem with M as the target function being optimised over the space of valid θvalues. Optimising standard IR measures, 1 however, is not easy: they are very expensive to evaluate, they have local maxima andplateaus, they are not smooth and they don’t have gradients [49]. For these reasons, it is not easy to apply standard optimisation techniques. Even applying simple 0-th order optimisation techniques such as trusted region opti- misation is diﬃcult and expensive. In practice we use a number of ad hoc techniques to speed up exhaustive search. We will discuss these in the next subsection. Another alternative is to change the function being optimised. This approach is specially useful when one is optimising"
"bm25_prf_p48_c002","bm25_prf","bm25_prf.pdf","48","2","practice we use a number of ad hoc techniques to speed up exhaustive search. We will discuss these in the next subsection. Another alternative is to change the function being optimised. This approach is specially useful when one is optimising very many features, and is discussed in the last subsection. 5.1 Greedy Optimisation We discuss here a number of techniques (some heuristics, some imple- mentation tricks) we used in the past to speedup the exhaustive search and ﬁnd good parameter values. Caching: Because function evaluations are so expensive, we cache the evaluations. Indeed we may often re-visit a parameter value in our 1 Such as Average Precision, Precision@k, Mean Reciprocal Rank and Discounted Cumulative Gain."
"bm25_prf_p49_c001","bm25_prf","bm25_prf.pdf","49","1","5.1 Greedy Optimisation 379 search for the optimum. It would be wasteful to re-evaluate all the queries; instead, we store the resulting relevance in a hash table using the parameter values as the key: H[θ] ← M(θ). Grid: It is also useful to set a minimum resolution for every parame- ter, deﬁning agrid of allowed parameters values. For example, in most of our experiments we did not allow parameters to change beyond the second decimal. This has negligible aﬀect on the relevance performance and greatly increases the eﬀect of caching and the speed of conver- gence. Furthermore it makes it easier to report and share parameter values. 5.1.1 Robust Line Search We ﬁrst consider methods for optimising over a single parameter. Most parameters being optimised are positive but unbounded above. There- fore we do not know the region of interest of the parameter being opti- mised, nor do we know the required resolution (the size of the intervals to be tested). For this reason we developed the following search algo- rithm, which we call Robust Line Search (RLS). Call l and r the current left and right brackets of the 1 −D search space. Split the region inm equal parts of size (r − l)/m. Compute the performance on the m + 1 region boundaries, and call c the boundary with the highest performance. The idea is to move towards c by re-centering the search region around c and scaling it appropriately. If l<c<r we need to zoom in into"
"bm25_prf_p49_c002","bm25_prf","bm25_prf.pdf","49","2","m + 1 region boundaries, and call c the boundary with the highest performance. The idea is to move towards c by re-centering the search region around c and scaling it appropriately. If l<c<r we need to zoom in into c, by scaling down the search region. Since the function being optimised has local maxima we cannot zoom too much, or we may completely lose the global maximum; for this reason we tend to zoom very conservatively. On the other hand, if c equals r or l, it is most likely that the maximum is outside the current search region, and possibly far away. Therefore we increase the size of the search region. We iterate this until l and r are within the some minimum distance, typically below the minimum resolution set. An example optimisation is illustrated in Figure 5.1."
"bm25_prf_p50_c001","bm25_prf","bm25_prf.pdf","50","1","380 Parameter Optimisation Fig. 5.1 Greedy Optimisation example: robust line search. 5.2 Multidimensional Optimisation Any 1-D optimisation method can be generalised to n-D in several ways. We have used two methods to do this, both of which worked well in practice. Greedy Robust Line Search: Imagine that we want to run the RLS method on a 2-D problem, with parametersθ=( θ 1,θ2). Let’s leaveθ2 as a subordinate variable and run RLS onθ1. Every time that we need to compute the performance M(θ) at a given pointθ1 = z, we would need to ﬁx the subordinate θ2 at its optimal value. To ﬁnd out this value, we can run a local RLS to optimise θ2 locally while keeping θ1 = z. Generalising this, every line optimisation withi parameters ﬁres oﬀ an optimisation with i − 1 subordinate parameters and so on, recursively. Of course this is a greedy approximation to the exhaustive (exponen- tial) exploration of the parameter space, because we are running RLS. However, this approach remains exponentially expensive with respect to n because of its recursive nature, and therefore it is not practicable for large n (e.g., n> 3). Promising Directions: Another way we have used to carry out searches in n dimensions is the following. 2 Choose an initial point for 2 This method can be seen as a linear version of trusted region optimisation methods [4, 5]; it has the advantage of requiring much fewer function evaluations, which are extremely expensive in our case."
"bm25_prf_p51_c001","bm25_prf","bm25_prf.pdf","51","1","5.2 Multidimensional Optimisation 381 each parameter (call the resulting the vector θ). Run n 1−D indepen- dent searches, to ﬁnd the optimum valueθ′ i of each parameter when all others are kept ﬁxed atθ. Each of these optimal values found deﬁnes a promising directionsin parameter space. Now consider the vector going from θto θ′ =( θ′ 1,...,θ ′ n). We expect this vector to move through inter- esting regions of space if there is correlation between features. There- fore we do one more 1−D optimisation along this line. We do this by re-parameterising the system with a new variable that moves all param- eters linearly fromθto θ ′. Finally we choose the best parameter setting found so far and we re-start the process. An example optimisation is illustrated in Figure 5.2, where we show the ﬁrst two iterations (noted 1 and 2), each consisting of three line searches (noted a, b and c). The total cost of an iteration is ( n +1 )1 −D optimisations. Therefore, it grows only linearly with n, but it may require very many iterations to complete. K1 Scaling: Note that in general k 1 will depend on the weights assigned to streams in BM25F, even if it is kept as a stream-independent parameter. This is because the stream weights in eﬀect rescale the tf values, and k 1 has to be adjusted accordingly. If we have a goodkBM25 1 value for the regular BM25 function (no streams), we can propose a good initial value"
"bm25_prf_p51_c002","bm25_prf","bm25_prf.pdf","51","2","is because the stream weights in eﬀect rescale the tf values, and k 1 has to be adjusted accordingly. If we have a goodkBM25 1 value for the regular BM25 function (no streams), we can propose a good initial value ofk1 for BM25F by scaling it according to the change Fig. 5.2 Greedy optimisation example: promising directions."
"bm25_prf_p52_c001","bm25_prf","bm25_prf.pdf","52","1","382 Parameter Optimisation in average document length from the unweighted to the weighted form: kBM25F 1 ≈ kBM25 1 ∑ s vsavsl s∑ s avsl s (5.1) 5.2.1 F actoring the Search A very useful technique for speeding up the search in practice is to factor the search into batches, optimising together only the parame- ters that are known to be strongly dependent. When doing this it is important to choose the initial parameters and the order of the batches judiciously. A typical schedule for BM25F may be: 1. Compute the optimal k 1 and b (ignoring streams). This is equivalent to setting all vs = 1 and all bs = b. (Cost: 1×2D); 2. Optimise stream weights {w}s=1..S jointly. We use here thek1 re-scaling trick in Step (5.1). (Cost: 1×SD); and 3. Optimise k1 and {b}s=1..S independently (Cost: (S +1 )×1D). This schedule may be repeated until no further increase in performance is obtained. When dealing with non-textual features, the optimisation above is usually interleaved with the optimisation of the non-textual features, which can also be done independently or jointly by batches. 5.3 Gradient Optimisation A diﬀerent possibility is to choose a performance function that can be optimised directly by machine learning techniques. This approach was pursued with some success a few years ago [1, 10, 55], and has now become an active area of research (see for example the recent NIPS and SIGIR workshops on this topic [23, 28]). The main idea is to approximate rank-dependant relevance functions"
"bm25_prf_p52_c002","bm25_prf","bm25_prf.pdf","52","2","success a few years ago [1, 10, 55], and has now become an active area of research (see for example the recent NIPS and SIGIR workshops on this topic [23, 28]). The main idea is to approximate rank-dependant relevance functions such as NDCG by a function with known gradients. Then we can apply the battery of gradient-based optimisation methods. The speed of these methods does not grow exponentially with the number of dimensions optimised, so one can optimise very many parameters simultaneously. Furthermore, if BM25F is being combined with a larger number of other (non-textual)"
"bm25_prf_p53_c001","bm25_prf","bm25_prf.pdf","53","1","5.3 Gradient Optimisation 383 features, methods can be used to optimise jointly all parameters. For example, this is the case in [55] where the overall model is an artiﬁcial neural network which takes as input many features, one of them being BM25F. It is out of the scope of this survey to detail gradient-based optimisation methods."
"bm25_prf_p54_c001","bm25_prf","bm25_prf.pdf","54","1","6 Conclusions The classical probabilistic relevance framework has provided a series of well-founded scoring formula, as well as some signiﬁcant insights into diﬀerent aspects of search. One of the reasons of the success of the PRF, we believe, is the powerful combination of sound theoretical modelling and a pragmatic parameterisation that exploits our prior knowledge in IR. We do not believe that the PRF has reached the end of its useful life. When it is well understood, the PRF model can provide a solid ground on which to analyse new IR problems and derive new solutions. 384"
"bm25_prf_p55_c001","bm25_prf","bm25_prf.pdf","55","1","References [1] S. Agarwal, C. Cortes, and R. Herbrich, eds., Proceedings of the NIPS 2005 Workshop on Learning to Rank , 2005. [2] G. Amati, C. J. van Rijsbergen, and C. Joost, “Probabilistic models of infor- mation retrieval based on measuring the divergence from randomness,” ACM Transactions on Information Systems , vol. 20, no. 4, pp. 357–389, 2002. [3] M. M. Beaulieu, M. Gatford, X. Huang, S. E. Robertson, S. Walker, and P. Williams, “Okapi at TREC-5,”The Fifth Text Retrieval Conference (TREC- 5). NIST Special Publication 500-238, pp. 143–165, 1997. [4] F. V. Berghen, “Trust Region Algorithms,” Webpage, http://www. lemurproject.org. [5] F. V. Berghen, “CONDOR: A constrained, non-linear, derivative-free parallel optimizer for continuous, high computing load, noisy objective functions,” PhD thesis, Universit´e Libre de Bruxelles, 2004. [6] C. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 2006. [7] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” Journal of Machine Learning Research , vol. 3, pp. 993–1022, 2003. [8] D. Bodoﬀ and S. E. Robertson, “A new uniﬁed probabilistic model,” Jour- nal of the American Society for Information Science and Technology , vol. 55, pp. 471–487, 2004. [9] P. Boldi and S. Vigna, “MG4J at TREC 2005,” inThe Fourteenth Text Retrieval Conference (TREC 2005) P roceedings, NIST Special Publication 500-266, 2005. http://mg4j.dsi.unimi.it/. 385"
"bm25_prf_p56_c001","bm25_prf","bm25_prf.pdf","56","1","386 References [10] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, “Learning to rank using gradient descent,” in Proceedings of the International Conference on Machine Learning (ICML) , vol. 22, p. 89, 2005. [11] W. Cooper, “Some inconsistencies and misidentiﬁed modelling assumptions in probabilistic information retrieval,” ACM Transactions on Information Sys- tems, vol. 13, pp. 110–111, 1995. [12] N. Craswell, S. E. Robertson, H. Zaragoza, and M. Taylor, “Relevance weighting for query independent evidence,” in Proceedings of the 28th Annual Interna- tional ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 472–479, ACM, 2005. [13] N. Craswell, H. Zaragoza, and S. E. Robertson, “Microsoft Cambridge at TREC-14: Enterprise track,” in The Fourteenth Text Retrieval Conference (TREC 2005), 2005. [14] F. Crestani, M. Lalmas, C. J. van Rijsbergen, and I. Campbell, ““Is this doc- ument relevant? ... probably”: A survey of probabilistic models in information retrieval,” ACM Computing Surveys , vol. 30, no. 4, 1998. [15] W. B. Croft and D. J. Harper, “Using probabilistic models of document retrieval without relevance information,” Journal of Documentation , vol. 35, pp. 285–295, 1979. [16] W. Feller, An Introduction to Probability Theory and Its Applications, vol. 1. Wiley, 1968. [17] N. Fuhr, “Probabilistic Models in Information Retrieval,” The Computer Jour- nal, vol. 35, no. 3, 1992. [18] G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Landauer, R. A. Harshman, L. A. Streeter, and K. E. Lochbaum, “Information retrieval using a singular value decomposition"
"bm25_prf_p56_c002","bm25_prf","bm25_prf.pdf","56","2","Information Retrieval,” The Computer Jour- nal, vol. 35, no. 3, 1992. [18] G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Landauer, R. A. Harshman, L. A. Streeter, and K. E. Lochbaum, “Information retrieval using a singular value decomposition model of latent semantic structure,” inProceedings of the 11th Annual International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval , pp. 465–480, ACM, 1988. [19] S. P. Harter, “A probabilistic approach to automatic keyword indexing (parts 1 and 2),” Journal of the American Society for Information Science , vol. 26, pp. 197–206 and 280–289, 1975. [20] D. Hiemstra, S. E. Robertson, and H. Zaragoza, “Parsimonious language models for information retrieval,” inProceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pp. 178–185, ACM, 2004. [21] T. Hofmann, “Probabilistic latent semantic indexing,” in Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval , pp. 50–57, ACM, 1999. [22] Indri. Homepage. http://www. lemurproject.org/indri. [23] T. Joachims, H. Li, T. Y. Liu, and C. Zhai, “Learning to rank for information retrieval (LR4IR 2007),” SIGIR Forum, vol. 41, no. 2, pp. 58–62, 2007. [24] J. Laﬀerty and C. Zhai, “Document language models, query models, and risk minimization for information retrieval,” in Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval, ACM, 2001."
"bm25_prf_p57_c001","bm25_prf","bm25_prf.pdf","57","1","References 387 [25] J. Laﬀerty and C. Zhai, “Probabilistic relevance models based on document and query generation,” in Language Modelling for Information Retrieval , (W. B. Croft and J. Laﬀerty, eds.), pp. 1–10, Kluwer, 2003. [26] V. Lavrenko and W. B. Croft, “Relevance based language models,” in Proceed- ings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pp. 120–127, ACM, 2001. [27] Lemur Toolkit. Homepage. http://www.lemurproject.org. [28] H. Li, T. Y. Liu, and C. Zhai, “Learning to rank for information retrieval (LR4IR 2008),” SIGIR Forum, vol. 42, no. 2, pp. 76–79, 2008. [29] Lucene. Homepage. http://lucene.apache.org/. [30] M. E. Maron and J. L. Kuhns, “On relevance, probabilistic indexing and infor- mation retrieval,” Journal of the ACM , vol. 7, no. 3, pp. 216–244, 1960. [31] D. Metzler, “Automatic feature selection in the Markov random ﬁeld model for information retrieval,” in Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management , pp. 253–262, ACM New York, NY, USA, 2007. [32] D. Metzler and W. B. Croft, “A Markov random ﬁeld model for term dependen- cies,” in Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pp. 472–479, ACM, 2005. [33] D. Metzler, T. Strohman, and B. Croft, Information Retrieval in Practice . Pearson Education (US), 2009. [34] MG4J: Managing gigabytes for java. Homepage. http://mg4j.dsi.unimi.it/. [35] Okapi-Pack. Homepage. http://www.soi.city.ac.uk/ andym/OKAPI-PACK. [36] J. R. P´erez-Ag¨uera and H. Zaragoza, “UCM-Y!R at CLEF 2008 Robust and WSD"
"bm25_prf_p57_c002","bm25_prf","bm25_prf.pdf","57","2","Strohman, and B. Croft, Information Retrieval in Practice . Pearson Education (US), 2009. [34] MG4J: Managing gigabytes for java. Homepage. http://mg4j.dsi.unimi.it/. [35] Okapi-Pack. Homepage. http://www.soi.city.ac.uk/ andym/OKAPI-PACK. [36] J. R. P´erez-Ag¨uera and H. Zaragoza, “UCM-Y!R at CLEF 2008 Robust and WSD tasks,” CLEF 2008 Workshop , 2008. [37] J. R. P´erez-Ag¨uera, H. Zaragoza, and L. Araujo, “Exploiting morphological query structure using genetic optimization,” in NLDB 2008 13th Interna- tional Conference on Applications of Natural Language to Information Systems , Lecture Notes in Computer Science (LNCS), Springer Verlag, 2008. [38] J. P´erez-Iglesias, “BM25 and BM25F Implementation for Lucene,” Webpage, http://nlp.uned.es/∼jperezi/Lucene-BM25. [39] PF-Tijah. Homepage. http://dbappl.cs.utwente.nl/pftijah. [40] S. E. Robertson, “The probability ranking principle in information retrieval,” Journal of Documentation , vol. 33, pp. 294–304, 1977. [41] S. E. Robertson, “On term selection for query expansion,” Journal of Docu- mentation, vol. 46, pp. 359–364, 1990. [42] S. E. Robertson, “Threshold setting and performance optimization in adaptive ﬁltering,” Information Retrieval, vol. 5, pp. 239–256, 2002. [43] S. E. Robertson, M. E. Maron, and W. S. Cooper, “The uniﬁed probabilistic model for IR,” in Proceedings of Research and Development in Information Retrieval, (G. Salton and H.-J. Schneider, eds.), pp. 108–117, Berlin: Springer- Verlag, 1983. [44] S. E. Robertson and K. Sparck Jones, “Relevance weighting of search terms,” Journal of the American Society for Information Science , 1977."
"bm25_prf_p58_c001","bm25_prf","bm25_prf.pdf","58","1","388 References [45] S. E. Robertson, C. J. van Rijsbergen, and M. F. Porter, “Probabilistic models of indexing and searching,” in Information Retrieval Research (P roceedings of Research and Development in Information Retrieval, Cambridge, 1980) , (R. N. Oddy, S. E. Robertson, C. J. van Rijsbergen, and P. W. Williams, eds.), pp. 35– 56, London: Butterworths, 1981. [46] S. E. Robertson and S. Walker, “Some Simple Eﬀective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval,” in Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval , pp. 232–241, ACM/Springer, 1994. [47] S. E. Robertson and S. Walker, “On relevance weights with little relevance information,” in Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pp. 16–24, ACM, 2007. [48] S. E. Robertson, S. Walker, M. Hancock-Beaulieu, A. Gull, and M. Lau, “Okapi at TREC,” in The First Text Retrieval Conference (TREC-1), NIST Special Publication 500-207, pp. 21–30, 1992. [49] S. E. Robertson and H. Zaragoza, “On rank-based eﬀectiveness measures and optimization,” Information Retrieval, vol. 10, no. 3, pp. 321–339, 2007. [50] S. E. Robertson, H. Zaragoza, and M. Taylor, “Simple BM25 extension to multi- ple weighted ﬁelds,” inProceedings of the 2004 ACM CIKM International Con- ference on Information and Knowledge Management , pp. 42–49, ACM, 2004. [51] R. Song, M. J. Taylor, J. R. Wen, H. W. Hon, and Y. Yu, “Viewing term prox- imity from a diﬀerent perspective,” Advances in Information Retrieval"
"bm25_prf_p58_c002","bm25_prf","bm25_prf.pdf","58","2","International Con- ference on Information and Knowledge Management , pp. 42–49, ACM, 2004. [51] R. Song, M. J. Taylor, J. R. Wen, H. W. Hon, and Y. Yu, “Viewing term prox- imity from a diﬀerent perspective,” Advances in Information Retrieval (ECIR 2008), Springer LNCS 4956, pp. 346–357, 2008. [52] K. Sparck Jones, S. Walker, and S. E. Robertson, “A probabilistic model of information retrieval: Development and comparative experiments. Part 1,” in Information P rocessing and Management , pp. 779–808, 2000. [53] K. Sparck Jones, S. Walker, and S. E. Robertson, “A probabilistic model of information retrieval: Development and comparative experiments. Part 2,” in Information P rocessing and Management , pp. 809–840, 2000. [54] T. Tao and C. Zhai, “An exploration of proximity measures in information retrieval,” in Proceedings of the 20th Annual International ACM SIGIR Con- ference on Research and Development in Information Retrieval , pp. 295–302, ACM, 2007. [55] M. Taylor, H. Zaragoza, N. Craswell, S. E. Robertson, and C. Burges, “Optimi- sation methods for ranking functions with multiple parameters,” in Fifteenth Conference on Information and Knowledge Management (ACM CIKM) , 2006. [56] Terrier. Homepage. http://ir.dcs.gla.ac.uk/terrier. [57] R. van Os D. Hiemstra, H. Rode, and J. Flokstra, “PF/Tijah: Text search in an XML database system,” Proceedings of the 2nd Interna- tional Workshop on Open Source Information Retrieval (OSIR) , pp. 12–17, http://dbappl.cs.utwente.nl/pftijah, 2006. [58] C. J. van Rijsbergen, Information Retrieval. Butterworth, 1979. [59] E. M. Voorhees and D. K. Harman, “Overview of the eighth text retrieval conference (TREC-8),”The Eighth"
"bm25_prf_p58_c003","bm25_prf","bm25_prf.pdf","58","3","Interna- tional Workshop on Open Source Information Retrieval (OSIR) , pp. 12–17, http://dbappl.cs.utwente.nl/pftijah, 2006. [58] C. J. van Rijsbergen, Information Retrieval. Butterworth, 1979. [59] E. M. Voorhees and D. K. Harman, “Overview of the eighth text retrieval conference (TREC-8),”The Eighth Text Retrieval Conference (TREC-8), NIST Special Publication 500-246, pp. 1–24, 2000."
"bm25_prf_p59_c001","bm25_prf","bm25_prf.pdf","59","1","References 389 [60] Wumpus. Homepage. http://www.wumpus-search.org/. [61] Xapian. http://xapian.org. [62] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. E. Robertson, “Microsoft Cambridge at TREC 2004: Web and HARD track,” in The Thirteenth Text Retrieval Conference (TREC 2004), NIST Special Publication, 500-261, 2005. [63] Zettair. Homepage. http://www.seg.rmit.edu.au/zettair."
"chain_of_retrieval_p01_c001","chain_of_retrieval","chain_of_retrieval.pdf","1","1","Chain-of-Retrieval Augmented Generation Liang Wang†∗ Haonan Chen‡ Nan Yang† Xiaolong Huang† Zhicheng Dou‡ Furu Wei† †Microsoft Research ‡Renmin University of China https://aka.ms/GeneralAI Abstract This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the ﬁnal answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct ﬁnal answer. At test time, we propose various decoding strategies to scale the model’s test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efﬁcacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models. 4k 8k 16k 32k 64k 128k # Avg. T okens 18 20 22 24 26 28 30EM CoRAG on MuSiQue Ours Previous SoTA Pareto frontier (a) Where did the star"
"chain_of_retrieval_p01_c002","chain_of_retrieval","chain_of_retrieval.pdf","1","2","future research aimed at developing factual and grounded foundation models. 4k 8k 16k 32k 64k 128k # Avg. T okens 18 20 22 24 26 28 30EM CoRAG on MuSiQue Ours Previous SoTA Pareto frontier (a) Where did the star of Dark Hazard study? What was the name of the star of Dark Hazard? Edward G. Robinson Where did Edward G. Robinson go to college? No relevant information found. What college did Edward G. Robinson attend? City College of New York. City College of New York decomposition retrieval failure reformulation (b) Figure 1: (a) Test-time scaling behavior of CoRAG. Increased token budget leads to consistent performance improvements. (b) An example of CoRAG on the MuSiQue dataset. It learns to decompose the complex query and conduct query reformulation when encountering a retrieval failure. ∗Correspondence to wangliang@microsoft.com 39th Conference on Neural Information Processing Systems (NeurIPS 2025)."
"chain_of_retrieval_p02_c001","chain_of_retrieval","chain_of_retrieval.pdf","2","1","1 Introduction Retrieval-augmented generation (RAG) [20] is one of the core techniques in enterprise applications, necessitating the integration of large foundation models with proprietary data sources to produce responses that are both grounded and factual. Conventionally, foundation models are trained on large-scale datasets comprising trillions of tokens and remain frozen post-deployment. Nonetheless, these models frequently struggle to memorize long-tail factual knowledge or may hallucinate false claims, resulting in unreliable responses in real-world scenarios. RAG mitigates this challenge by augmenting the generation process with retrieved information, thereby improving the trustworthiness of model-generated content and facilitating the incorporation of up-to-date information. Contemporary RAG systems typically employ a sequential pipeline of retrieval and generation, wherein the retrieved information serves as additional input to the generative model. The effectiveness of RAG systems predominantly relies on the quality of the retrieved information. Retrieval models are engineered for efﬁciency to ensure scalability to large corpora. For instance, dense retrievers [18, 35] commonly utilize a bi-encoder architecture to compress documents and queries into ﬁxed-size vector representations. This architectural choice permits the use of fast approximate nearest neighbor search algorithms but simultaneously constrains the expressive capacity of retrieval models to handle complex queries. Furthermore, in multi-hop reasoning tasks, it is often unclear what information should be retrieved initially; decisions must be made based on the progressively evolving state of the reasoning process. To break the bottleneck of retrieval quality, we propose a framework that dynamically retrieves relevant information and plans subsequent retrieval steps based on the current state."
"chain_of_retrieval_p02_c002","chain_of_retrieval","chain_of_retrieval.pdf","2","2","decisions must be made based on the progressively evolving state of the reasoning process. To break the bottleneck of retrieval quality, we propose a framework that dynamically retrieves relevant information and plans subsequent retrieval steps based on the current state. By adjusting the number of retrieval steps at test time, our model can explore various aspects of the query and experiment with different query rewriting strategies when the retriever does not yield useful information. This paradigm mirrors the human problem solving process, where we iteratively seek information to address complex questions. An example is illustrated in Figure 1. Rather than solely relying on the model’s in-context learning capability [ 42] or distillation from proprietary models [1], we advocate for explicitly training language models to retrieve step by step. To this end, we utilize rejection sampling [43, 5] to augment existing RAG datasets with intermediate retrieval chains. Open-source language models are then ﬁne-tuned on these augmented datasets using standard next-token prediction objectives. To examine the scaling behavior of our model, we propose various test-time decoding strategies, including greedy decoding, best-of-N sampling, and tree search. Diverse decoding strategies and hyperparameter conﬁgurations can be employed to control test-time token consumption and the frequency of retriever calls. Our empirical evaluation demonstrates that CoRAG substantially surpasses strong baselines in QA tasks that require multi-hop reasoning, where retrievers frequently struggle to recall all necessary information in a single retrieval step. Across diverse decoding strategies, the Pareto frontier approxi- mately adheres to a log-linear relationship between total"
"chain_of_retrieval_p02_c003","chain_of_retrieval","chain_of_retrieval.pdf","2","3","surpasses strong baselines in QA tasks that require multi-hop reasoning, where retrievers frequently struggle to recall all necessary information in a single retrieval step. Across diverse decoding strategies, the Pareto frontier approxi- mately adheres to a log-linear relationship between total token consumption and model performance, although the coefﬁcients differ across datasets. On the KILT benchmark [27], which encompasses a more diverse array of tasks, new state-of-the-art scores are achieves on the hidden test set for nearly all tasks. Additionally, we uncover that CoRAG exhibits varied scaling behaviors across different task types. For datasets such as NQ [ 19], where state-of-the-art retrievers already achieve high recall, the beneﬁts of test-time scaling are often marginal. This suggests the potential for dynamically allocating test-time compute based on the complexity of the query and the quality of the retriever. Upon further analysis, we ﬁnd that CoRAG can effectively decompose complex queries and perform ﬂexible query reformulation to improve the quality of the generated responses. It also shows robustness against retrievers of varying quality. We posit that CoRAG represents a promising avenue for future research in the RAG domain, with the potential to mitigate hallucination in model-generated content. Our code, data and trained models are available at https://github.com/microsoft/LMOps/tree/main/corag. 2 Related Work Retrieval-Augmented Generation (RAG)integrates information retrieval techniques with genera- tive models to enhance the quality and factual accuracy of generated content [20, 21]. By equipping LLMs with the ability to browse the web [ 26], RAG systems can access real-time data, thereby 2"
"chain_of_retrieval_p03_c001","chain_of_retrieval","chain_of_retrieval.pdf","3","1","providing responses that are both up-to-date and grounded. The relevance and quality of the retrieved information are pivotal for the efﬁcacy of RAG systems. A substantial body of recent research has concentrated on developing better general-purpose text embeddings [ 18, 35]. Nevertheless, text embeddings frequently face limitations in addressing complex queries due to their reliance on ﬁxed-size vector representations for efﬁciency purposes. To mitigate this constraint, contemporary research has extended the conventional paradigm of a single retrieval step followed by generation, advancing to multi-step iterative retrieval and generation [6]. FLARE [ 13] prompts an LLM to actively determine when and what to retrieve during the generation process. ITER-RETGEN [ 30] proposes to interleave retrieval-augmented generation with generation-augmented retrieval, demonstrating enhancements in multi-hop QA tasks. Similarly, IRCoT [ 33] employs a chain-of-thought methodology, which recursively reﬁnes the reasoning thought for subsequent retrieval steps. Self-RAG [1] empowers LLMs to adaptively retrieve, generate, and critique through self-reﬂection, thus improving factual accuracy and citation precision in open-domain QA and long-form generation tasks. Auto-RAG [41] utilizes heuristic rules and exact answer matching to construct intermediate retrieval steps, yet its performance remains signiﬁcantly below that of state-of-the-art models. AQA [3] learns to reformulate questions using reinforcement learning but only focuses on single-hop QA tasks. In this study, rather than exclusively on few-shot prompting or distillation from proprietary models, we propose a novel approach to explicitly train LLMs to iteratively retrieve and reason over relevant information. Scaling Test-time ComputeInstead of prompting LLMs to directly generate the ﬁnal"
"chain_of_retrieval_p03_c002","chain_of_retrieval","chain_of_retrieval.pdf","3","2","study, rather than exclusively on few-shot prompting or distillation from proprietary models, we propose a novel approach to explicitly train LLMs to iteratively retrieve and reason over relevant information. Scaling Test-time ComputeInstead of prompting LLMs to directly generate the ﬁnal answer, Chain- of-Thought (CoT) [ 36] demonstrates that letting the model to think step by step can drastically improve the performance on mathematical reasoning tasks. Tree-of-Thought (ToT) [ 40] extends the idea of CoT by adopting a tree structure, allowing the model to explore the search space more comprehensively. To further enhance the reasoning capabilities of LLMs, STaR [ 43] proposes to leverage bootstrapping techniques to generate intermediate states for training. OpenAI o1 [ 12] conducts large-scale reinforcement learning and exhibits promising test-time scaling behaviors on advanced reasoning datasets, but the technical details are not publicly available. A drawback of these methods is the increased token consumption, which consequently increases the response latency. In the realm of RAG, test-time compute can be increased by retrieving more documents or performing additional retrieval steps. LongRAG [14] posits that RAG performance can be enhanced by integrating long-context LLMs with more retrieved documents. In contrast, IterDRAG [42] empirically examines the test-time scaling law through few-shot prompting and iterative retrieval for up to 5M tokens. Search-o1 [22] combines the open-source QwQ model [37] with active search from Bing, achieving competitive results on knowledge-intensive tasks. Concurrent works such as Search-R1 [15] train LLMs to use retrieval as a tool via reinforcement learning. Our work extends the"
"chain_of_retrieval_p03_c003","chain_of_retrieval","chain_of_retrieval.pdf","3","3","Search-o1 [22] combines the open-source QwQ model [37] with active search from Bing, achieving competitive results on knowledge-intensive tasks. Concurrent works such as Search-R1 [15] train LLMs to use retrieval as a tool via reinforcement learning. Our work extends the study of test-time scaling in RAG to a targeted ﬁne-tuning paradigm under diverse decoding strategies. 3 Methodology The CoRAG framework is illustrated in Figure 2. The “Current State” denotes the input context and instructions provided to the LLM, while the “Next Action” refers to the LLM output responding to the given instruction. In this section, we describe the key components of CoRAG, including retrieval chain generation through rejection sampling, model training with augmented datasets, and strategies for scaling test-time compute. 3.1 Retrieval Chain Generation Most RAG datasets only come with a query Q and the corresponding ﬁnal answer A, without providing intermediate retrieval steps. We propose an automated method for generating retrieval chains through rejection sampling. Each sampled chain consists of a sequence of sub-queries Q1:L ={Q1, Q2, . . . , QL} and the corresponding sub-answers A1:L, where L is a predetermined maximum chain length. The sub-query Qi = LLM(Q<i, A<i, Q) is generated by sampling an LLM based on the query Q and the preceding sub-queries and sub-answers. To generate the sub-answer Ai, we ﬁrst retrieve the top-k most relevant documents D(i) 1:k using a text retriever with Qi as the search 3"
"chain_of_retrieval_p04_c001","chain_of_retrieval","chain_of_retrieval.pdf","4","1","output query … … output query … …… … … … output query … … … … … … rolloutsrollouts Greedy Best-of-N Tree Search Inference Training Rejection Sampling LLM Current State Next Action Query sub-query 1 sub-answer 1 sub-query n sub-answer n … … P(Ans | Chain 1) sub-query 1* sub-answer 1* sub-query n* sub-answer n* … … P(Ans | Chain 2) … … … … … … … … … … P(Ans | Chain 3) LLM retriever chosen state discarded state answer Figure 2: Overview of CoRAG. Rejection sampling is utilized to augment QA-only datasets with retrieval chains. Each chain starts with the original query, followed by a sequence of sub-queries and sub-answers. An open-source LLM is then ﬁne-tuned to predict the next action based on the current state. During inference, multiple decoding strategies are available to control the test-time compute. query, and subsequently prompt an LLM to yield the answer Ai = LLM(Qi, D(i) 1:k). This procedure is iterated until the chain reaches the maximum length L or Ai matches the correct answer A. To assess the quality of a retrieval chain, we calculate the log-likelihood of the correct answer log P(A|Q, Q1:L, A1:L) conditioned on the chain information. The retrieval chain with the highest log-likelihood score is selected to augment the original QA-only dataset. 3.2 Training Each training instance in the augmented dataset is represented as a tuple (Q, A, Q1:L, A1:L), accom- panied by the corresponding top- k retrieved documents for the query Q and each"
"chain_of_retrieval_p04_c002","chain_of_retrieval","chain_of_retrieval.pdf","4","2","selected to augment the original QA-only dataset. 3.2 Training Each training instance in the augmented dataset is represented as a tuple (Q, A, Q1:L, A1:L), accom- panied by the corresponding top- k retrieved documents for the query Q and each sub-query. We ﬁne-tune an LLM on the augmented dataset using the standard next-token prediction objective within a uniﬁed multi-task learning framework. The model is simultaneously trained on three tasks: next sub-query prediction, sub-answer prediction, and ﬁnal answer prediction. We employ the same prompt templates as utilized in the retrieval chain generation process, with the exception that we also incorporate the top retrieved documents D1:k for the original query Q as input for the ﬁnal answer prediction task. Lsub_query =− log P(Qi|Q, Q<i, A<i), i∈ [1, L] Lsub_answer =− log P(Ai|Qi, D(i) 1:k), i∈ [1, L] Lﬁnal_answer =− log P(A|Q, Q1:L, A1:L, D1:k) The cross-entropy loss is computed only for the target output tokens. As we reuse the prompt templates for both data generation and model training, a ﬁne-tuned model can be utilized for the next round of rejection sampling in an iterative manner. 3.3 Test-time Scaling Given a trained CoRAG model, we propose several decoding strategies to control the trade-off between model performance and test-time compute. The test-time compute is measured by the 4"
"chain_of_retrieval_p05_c001","chain_of_retrieval","chain_of_retrieval.pdf","5","1","total number of token consumptions, excluding the retrieval costs. Unlike previous approaches that consider only prompt tokens [42] or generated tokens [12], we account for both. To simplify further discussion, the prompt tokens are treated equally as the generated tokens, despite prompt tokens typically being less expensive due to preﬁx caching and computation parallelism of the preﬁlling stage. Greedy Decoding This strategy utilizes greedy decoding to generate L sub-queries and their corre- sponding sub-answers sequentially. The ﬁnal answer is generated using the same prompt template as employed during the training phase. Best-of-N Sampling This method involves sampling N retrieval chains with a temperature 0.7, subsequently selecting the best chain to generate the ﬁnal answer. As the ground truth answer is not available at test time, we instead calculate the conditional log-likelihood of “No relevant information found” as a penalty score for each chain. The retrieval chain with the lowest penalty score is chosen. Tree Search We implement a breadth-ﬁrst search (BFS) variant with retrieval chain rollouts. At each step, the current state is expanded by sampling several sub-queries. For each expanded state, we perform multiple rollouts, and then compute the average penalty score of these rollouts. The state with the lowest average penalty score is retained for further expansion. To control the test-time compute, the maximum length of the retrieval chain L can be adjusted across all decoding strategies. For best-of-N sampling, the number of sampled chainsN offers an alternative option to scale the test-time compute. In tree search, the"
"chain_of_retrieval_p05_c002","chain_of_retrieval","chain_of_retrieval.pdf","5","2","control the test-time compute, the maximum length of the retrieval chain L can be adjusted across all decoding strategies. For best-of-N sampling, the number of sampled chainsN offers an alternative option to scale the test-time compute. In tree search, the number of rollouts and expansion size are two additional hyperparameters. 4 Experiments 4.1 Setup Data and Evaluation We evaluate CoRAG utilizing two sets of benchmarks: (1) a collection of multi-hop QA datasets, including 2WikiMultihopQA [ 8], HotpotQA [ 39], Bamboogle [ 28], and MuSiQue [32]; (2) the KILT benchmark [27], which encompasses a broad spectrum of knowledge- intensive tasks. The multi-hop QA datasets serve to evaluate the model’s capacity to perform multi-hop reasoning, whereas the KILT benchmark assesses the framework’s ability to generalize across more diverse tasks. For each training dataset, we prompt the open-source Llama-3.1-8B- Instruct model to perform rejection sampling, unless speciﬁed otherwise. We utilize E5-large [34] as the text retriever for intermediate retrieval steps. The retrieval corpus is the English Wikipedia provided by KILT, comprising approximately36 million passages [25]. The selected retrieval chains are employed to augment the original QA-only datasets for subsequent model training. Regarding evaluation metrics, we report the exact match (EM) and F1 scores [29] for the multi-hop QA datasets. For the KILT benchmark, we submit the model’s predictions to the ofﬁcial evaluation server and report the downstream metrics on the hidden test set . To adhere to the leaderboard submission policy, we report public validation set results when conducting ablation studies on the"
"chain_of_retrieval_p05_c003","chain_of_retrieval","chain_of_retrieval.pdf","5","3","we submit the model’s predictions to the ofﬁcial evaluation server and report the downstream metrics on the hidden test set . To adhere to the leaderboard submission policy, we report public validation set results when conducting ablation studies on the KILT benchmark. Note that while HotpotQA and MuSiQue maintain public leaderboards, these adopt either a simpliﬁed reading comprehension setting or an abstract-only retrieval conﬁguration. Conse- quently, the leaderboard results are not directly comparable to our open-domain QA evaluation setting. Model Training We conduct full-parameter ﬁne-tuning on the augmented datasets, initializing from the Llama-3.1-8B-Instruct checkpoint. Two separate models are trained: one for the multi-hop QA datasets and another for the KILT benchmark. The compiled multi-hop QA dataset comprises 125k training instances, whereas the KILT benchmark includes 660k instances after sub-sampling. The model is ﬁne-tuned for1 epoch with a maximum sequence length of 3k tokens. For the KILT benchmark, we ﬁne-tune an E5-Mistral retriever [ 35] and a RankLLaMA re-ranker [ 24] on the respective training set to boost the ranking quality. Further implementation details are provided in Appendix A. 5"
"chain_of_retrieval_p06_c001","chain_of_retrieval","chain_of_retrieval.pdf","6","1","Table 1: Results on multi-hop QA datasets. We report the performance of CoRAG-8B using various decoding strategies and retrieval chain lengths L. The “Few-shot w/o Retrieval” conﬁguration utilizes only QA pairs without retrieval augmentation. Both DRAG and IterDRAG are based on Gemini 1.5 Flash [31], while Search-o1-32B is based on QwQ [37] and the Bing Search API. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 F ew-shot w/o Retrieval 3-shot Llama-3.1-8B-Inst. 27.6 32.1 20.8 28.8 17.6 21.3 3.4 9.7 3-shot GPT-4o 39.5 47.3 38.2 51.2 49.6 61.5 15.8 27.2 w/ Retrieval 3-shot Llama-3.1-8B-Inst. 30.7 39.9 34.1 46.6 28.0 37.3 7.7 15.4 3-shot GPT-4o 49.0 56.2 45.8 59.4 53.6 63.8 15.7 25.8 Self-RAG-7B 12.2 24.1 16.6 29.4 5.6 16.8 4.6 13.2 ITER-RETGEN 35.5 47.4 45.1 60.4 40.0 50.7 26.1 42.0 DRAG (32k) 45.9 53.7 46.9 60.3 48.8 59.2 15.4 26.0 IterDRAG (32k) 44.3 54.6 38.3 49.8 46.4 56.2 12.5 23.1 Search-o1-32B 58.0 71.4 45.2 57.3 56.0 67.8 16.6 28.2 Fine-tuned Llama-8B w/ E5large 55.1 60.7 50.3 63.5 40.8 53.7 17.4 28.1 CoRAG-8B (Ours) ⊿ L=1, greedy 56.5 62.3 50.1 63.2 37.6 51.4 18.6 29.3 ⊿ L=6, greedy 70.6 75.5 54.4 67.5 48.0 63.5 27.7 38.5 ⊿ L=6, best-of-4 71.7 76.5 55.3 68.5 51.2 63.1 28.1 39.7 ⊿ L=6, tree search 71.7 76.4 55.8 69.0 48.8 64.4 29.0 40.3 ⊿ L=10, best-of-8 72.5 77.3 56.3 69.8 54.4 68.3 30.9 42.4 4.2 Main Results Multi-hop QA In Table 1, we present a comparative analysis of CoRAG-8B against several models,"
"chain_of_retrieval_p06_c002","chain_of_retrieval","chain_of_retrieval.pdf","6","2","L=6, tree search 71.7 76.4 55.8 69.0 48.8 64.4 29.0 40.3 ⊿ L=10, best-of-8 72.5 77.3 56.3 69.8 54.4 68.3 30.9 42.4 4.2 Main Results Multi-hop QA In Table 1, we present a comparative analysis of CoRAG-8B against several models, including few-shot Llama-3.1-8B-Instruct [5], GPT-4o [10], Self-RAG-7B [1], ITER-RETGEN [30], DRAG, IterDRAG [42], and Search-o1-32B [ 22]. For a fair comparison, we also include a ﬁne- tuned Llama-8B baseline utilizing the E5-large retriever, which is ﬁne-tuned on the same datasets as CoRAG-8B but without retrieval chain augmentation. CoRAG-8B substantially surpasses all baselines, with the exception of the Bamboogle dataset, despite being based on a weaker LLM compared to Search-o1-32B and IterDRAG. Conversely, we recognize that ﬁne-tuning on multi-hop QA datasets creates an advantage for CoRAG-8B, compared to the few-shot setting for DRAG and IterDRAG. The Bamboogle dataset comprises only 125 instances, resulting in considerable variance in perfor- mance across different runs. Certain questions within Bamboogle necessitate access to knowledge more recent than the Wikipedia dump used for retrieval. Systems like Search-o1-32B, which rely on commercial search engines, possess an advantage in this regard. KILT Benchmark We present several strong systems on the KILT benchmark in Table 2, including KILT-RAG [27], SEAL [2], Atlas-11B [11], RA-DIT 65B [23], and FiD with RS [9]. For submission to the KILT leaderboard, we choose the best decoding conﬁguration for each task based on the public validation set. The results of different decoding strategies are detailed in Appendix Table 7. Our CoRAG-8B model achieves a"
"chain_of_retrieval_p06_c003","chain_of_retrieval","chain_of_retrieval.pdf","6","3","RS [9]. For submission to the KILT leaderboard, we choose the best decoding conﬁguration for each task based on the public validation set. The results of different decoding strategies are detailed in Appendix Table 7. Our CoRAG-8B model achieves a new state-of-the-art performance across all tasks, with the exception of FEVER, where it marginally trails behind a larger model with 11B parameters. 4.3 Scaling Test-Time Compute In alignment with OpenAI o1 [12], our model allows for scaling test-time compute to potentially achieve better performance without updating model weights. There are multiple ways to control the test-time compute. In Figure 3, we concentrate on two factors: the retrieval chain length L and the number of sampled chains N for best-of-N sampling. Greedy decoding is a special instance of best-of-N sampling with N = 1 and the temperature set to 0. 6"
"chain_of_retrieval_p07_c001","chain_of_retrieval","chain_of_retrieval.pdf","7","1","Table 2: The downstream results on thehidden test set of the KILT benchmark. All scores are sourced directly from the ofﬁcial leaderboard, with the exception that “RA-DIT 65B” is from the original paper [23].∗: “Previous Best” refers to the highest score for each task on the public KILT leaderboard as of January 10, 2025. System Entity Linking Slot Filling Open QA Fact AIDA WnWi WnCw T-REx zsRE NQ HoPo TQA FEVER KILT-RAG 72.6 48.1 47.6 59.2 44.7 44.4 27.0 71.3 86.3 SEAL - - - 83.6 74.6 53.7 40.5 70.9 89.5 Atlas-11B 90.6 - - 85.1 80.8 61.3 50.6 84.0 93.5 RA-DIT 65B 80.5 - - 72.8 78.1 43.5 36.6 72.8 86.9 FiD with RS - - - 85.2 83.7 61.2 39.1 84.6 92.2 Previous Best∗ 90.6 87.4 71.2 87.7 85.3 62.3 50.6 84.6 93.5 CoRAG-8B (Ours) 93.9 88.2 76.7 88.0 87.2 63.1 60.6 88.3 93.1 2 4 6 8 10 Chain Length 56 58 60 62 64 66 68 70 72EM 2WikiMultihopQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 50 51 52 53 54 55 56 HotpotQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 40 42 44 46 48 50 52 54 Bamboogle greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 18 20 22 24 26 28 30 MuSiQue greedy best-of-4 best-of-8 4k 8k 16k 32k 64k 128k # Avg. T okens 57.5 60.0 62.5 65.0 67.5 70.0 72.5EM greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k"
"chain_of_retrieval_p07_c002","chain_of_retrieval","chain_of_retrieval.pdf","7","2","18 20 22 24 26 28 30 MuSiQue greedy best-of-4 best-of-8 4k 8k 16k 32k 64k 128k # Avg. T okens 57.5 60.0 62.5 65.0 67.5 70.0 72.5EM greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 50 51 52 53 54 55 56 greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 40 42 44 46 48 50 52 54 greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 18 20 22 24 26 28 30 greedy best-of-4 best-of-8 tree search pareto frontier Figure 3: Scaling test-time compute on multi-hop QA datasets. The Pareto frontier is in the form of y = a× log(x + b) + c ﬁtted on the Pareto optimal points. A point is consideredPareto optimal if no other point achieves a higher EM score with less token consumption. The metric “# Avg. Tokens” represents the average number of tokens consumed per test instance, summing up both the prompt and generated tokens. We observe that increasing the retrieval chain lengthL results in substantial performance improve- ments when L is small, but the gains diminish as L increases. This observation aligns with the intuition that longer chains can encapsulate more reasoning steps and allows for trial-and-error explo- ration of various query rewriting strategies. Several examples are provided in Appendix Table 11. Conversely, increasing N for best-of-N sampling yields mixed effects depending on the"
"chain_of_retrieval_p07_c003","chain_of_retrieval","chain_of_retrieval.pdf","7","3","intuition that longer chains can encapsulate more reasoning steps and allows for trial-and-error explo- ration of various query rewriting strategies. Several examples are provided in Appendix Table 11. Conversely, increasing N for best-of-N sampling yields mixed effects depending on the dataset. For the most challenging dataset, MuSiQue, in terms of EM score, a larger N enhances performance, whereas for the less challenging dataset, 2WikiMultihopQA, a smaller N sufﬁces. We defer the further exploration of tree search to future work, as it is considerably more computationally expensive than greedy decoding and best-of-N sampling. The Pareto frontier between the EM score and token consumption approximately follows a log-linear trajectory for up to 128k tokens, although the scaling behavior varies across different datasets. This observation assists practitioners in making informed decisions regarding the allocation of test-time compute based on the quality requirements. It is important to note that we make several simpliﬁcations 7"
"chain_of_retrieval_p08_c001","chain_of_retrieval","chain_of_retrieval.pdf","8","1","in this scaling study, such as treating the prompt tokens equivalently to the generated tokens and ignoring the retrieval costs. A more rigorous analysis could take these factors into account. 5 Analysis Table 3: Ablation study results. “Iterative training” employs a trained CoRAG model for another round of rejection sampling. “Distill from GPT-4o” leverages the GPT-4o model to generate retrieval chains. “Weak-to-strong Generalization” utilizes weaker LLMs for retrieval chain generation while using stronger LLMs ( Llama-3.1-8B-Inst.) for training. “Different Retrievers” replaces the text retriever at test time. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 CoRAG-8B (L=6, greedy) 70.6 75.5 54.4 67.5 48.0 63.5 27.7 38.5 ⊿ iterative training 72.2 76.9 53.4 66.5 45.6 60.9 26.6 37.6 ⊿ distill from GPT-4o 75.1 79.5 56.6 70.2 51.2 67.0 28.2 38.5 Weak-to-strong Generalization w/ Llama-3.2-1B-Inst. 59.3 64.2 50.3 63.6 40.8 51.6 22.3 32.7 w/ Llama-3.2-3B-Inst. 69.9 74.0 53.9 67.3 45.6 59.8 25.2 36.0 Different Retrievers E5-base w/o chain-of-retrieval 53.1 58.9 47.9 61.1 38.4 52.7 15.8 26.4 ⊿ L=6, best-of-4 70.8 75.4 53.0 66.2 47.2 59.8 26.3 37.6 BM25 w/o chain-of-retrieval 49.1 55.3 46.9 60.3 36.8 48.6 14.3 24.8 ⊿ L=6, best-of-4 62.6 67.7 51.6 64.7 37.6 52.5 23.5 33.0 5.1 Iterative Rejection Sampling Our framework facilitates self-improvement through iterative training, akin to the iterative rejection sampling employed in LLM post-training [ 5]. By utilizing the same prompt templates for both data generation and model training, a trained CoRAG model can generate new sets of retrieval chains. However, the"
"chain_of_retrieval_p08_c002","chain_of_retrieval","chain_of_retrieval.pdf","8","2","iterative training, akin to the iterative rejection sampling employed in LLM post-training [ 5]. By utilizing the same prompt templates for both data generation and model training, a trained CoRAG model can generate new sets of retrieval chains. However, the results in Table 3 are mixed, showing performance improvements on the 2WikiMultihopQA dataset but slight declines on other datasets. This indicates that instruction-tuned LLMs already possess a strong ability to generate high-quality retrieval chains. 5.2 Robustness and Generalization Different Retrievers We further investigate the inﬂuence of various text retrievers at test time. Instead of using the E5-large dense retriever, we substitute it with two weaker alternatives in a plug-and-play fashion: E5-base and BM25. Across all datasets, we observe consistent performance gains when investing more test-time compute, although stronger retrievers continue to outperform in terms of absolute performance. Improvements to text retriever quality represent an orthogonal dimension that can further amplify CoRAG’s performance gains. Weak-to-strong GeneralizationDue to the need of repeated sampling and autoregressive generation, the retrieval chain generation process costs more GPU hours than the model training. To mitigate this cost, one strategy is to employ weaker LLMs for retrieval chain generation and subsequently ﬁne-tune stronger LLMs on the augmented datasets, similar to the weak-to-strong generalization setting [4]. The results in Table 3 demonstrate that utilizing Llama-3B achieves very close performance compared to the 8B model, whereas Llama-1B exhibits a noticeable performance drop. Manual inspection re- veals that the 1B model frequently struggles to follow the given instructions, resulting in"
"chain_of_retrieval_p08_c003","chain_of_retrieval","chain_of_retrieval.pdf","8","3","in Table 3 demonstrate that utilizing Llama-3B achieves very close performance compared to the 8B model, whereas Llama-1B exhibits a noticeable performance drop. Manual inspection re- veals that the 1B model frequently struggles to follow the given instructions, resulting in sub-optimal retrieval chains. Employing weaker LLMs also lowers the barrier to adopting more computationally expensive tree search strategies during data generation, which show great potential in mathematical reasoning tasks [7]. In contrast, distilling from a stronger model like GPT-4o yields a further perfor- mance boost, indicating that the quality of the retrieval chains is crucial for the ﬁnal performance. 8"
"chain_of_retrieval_p09_c001","chain_of_retrieval","chain_of_retrieval.pdf","9","1","5.3 Does Chain-of-Retrieval Always Help? 2 4 6 8 10 Chain Length 90 91 92 93 94 95EM FEVER greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 87 88 89 90 91EM TQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 59 60 61 62 63 64 65 66Accuracy NQ greedy best-of-4 best-of-8 Figure 4: Scaling test-time compute across three datasets from the KILT benchmark. We report scores on the public validation set. Multi-hop QA datasets are speciﬁcally designed to evaluate complex reasoning capabilities and are expected to beneﬁt from the chain-of-retrieval mechanism. Table 1 presents empirical evidence supporting this assertion. In contrast, for tasks that a single retrieval step is typically sufﬁcient, the advantage tends to be marginal, as demonstrated in Figure 4. Datasets such as NQ [ 19] and TriviaQA [17] are known for their (mostly) single-hop nature. This phenomenon implies that decoding strategies should be adaptive based on the complexity of the query. Additional results on the full KILT benchmark are listed in Appendix Table 7, where similar observations for other task types also hold. 5.4 Learning to Stop at Test Time 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Early Stop Logit Bias 18 20 22 24 26 28EM EM Score L=6 L=0 3.9k 4.9k 5.9k 6.8k 7.8k T oken Consumption # T okens MuSiQue (Greedy Decoding) Figure 5: Learning to stop at test time. Larger logit bias values result in earlier stopping. L = 6 correspond to always performing 6"
"chain_of_retrieval_p09_c002","chain_of_retrieval","chain_of_retrieval.pdf","9","2","L=6 L=0 3.9k 4.9k 5.9k 6.8k 7.8k T oken Consumption # T okens MuSiQue (Greedy Decoding) Figure 5: Learning to stop at test time. Larger logit bias values result in earlier stopping. L = 6 correspond to always performing 6 retrieval steps, while L = 0 indicate no intermediate retrieval steps. Instead of always performing L retrieval steps, we explore a model variant that learns to stop at test time. After each retrieval step, the model is prompted to predict whether the information gathered thus far sufﬁces to answer the query. Note that this prompt itself also incurs token consumption and additional cost. The decoding space is constrained to two tokens: “Yes” and “No”. If the decoded output is “ Yes”, no further sub-queries are generated. By adjusting the logit bias of the “ Yes” token, we can control the early stopping behavior. During the training phase, an additional loss term is added for the stop prediction task. The target output is “ Yes” if the current retrieval chain encompasses the preﬁx that maximizes the likelihood of the ﬁnal answer, and “No” oth- erwise. The associated prompt template is in Appendix Section D. In Figure 5, we illustrate how the performance varies along with the token consumption on the MuSiQue dataset. While early stopping can save some amount of token quota, it comes at the cost of performance degradation. The optimal conﬁguration depends on the dataset characteristics and the quality expectations. 9"
"chain_of_retrieval_p10_c001","chain_of_retrieval","chain_of_retrieval.pdf","10","1","5.5 Does CoRAG Learn to Retrieve Better? To evaluate whether CoRAG improves retrieval quality beyond just answer accuracy, we measure retrieval recall across multiple datasets. We report Recall@k metrics for k∈{ 10, 20, 100}, compar- ing standard retrieval using E5large against our approach. We follow the evaluation protocol from DPR [18] for calculating recall based on answer matches, as not all datasets provide gold supporting paragraphs. For CoRAG, we utilize reciprocal rank fusion to merge multiple retrieval results from the chain into a single ranked list, from which recall is calculated. Table 4: Retrieval recall comparison between standard retrieval and CoRAG across multi-hop QA datasets. R@10 R@20 R@100 HotpotQA w/ E5large 59.1 65.2 76.8 w/ CoRAG 72.1 76.7 84.3 2WikiMultiHopQA w/ E5large 54.9 62.1 74.6 w/ CoRAG 81.4 84.8 88.8 Bamboogle w/ E5large 31.2 40.0 57.6 w/ CoRAG 59.2 68.0 75.2 MuSiQue w/ E5large 29.0 36.5 52.7 w/ CoRAG 47.1 54.6 68.4 The results in Table 4 demonstrate that CoRAG consistently improves recall across all datasets and recall thresholds. The improvements are particularly pronounced on more challenging datasets like MuSiQue and Bamboogle, where single-step retrieval struggles most. This indicates that CoRAG’s iterative query reformulation and decomposition strategy effectively addresses the limitations of traditional dense retrieval, enabling the model to gather more relevant information through multiple retrieval steps. 6 Conclusion In this work, we introduce CoRAG, a framework that teaches LLMs to conduct iterative retrieval and reasoning to answer complex queries. The intermediate retrieval chains are automatically generated via rejection sampling,"
"chain_of_retrieval_p10_c002","chain_of_retrieval","chain_of_retrieval.pdf","10","2","more relevant information through multiple retrieval steps. 6 Conclusion In this work, we introduce CoRAG, a framework that teaches LLMs to conduct iterative retrieval and reasoning to answer complex queries. The intermediate retrieval chains are automatically generated via rejection sampling, thereby alleviating the need for manual annotation. At test time, we offer multiple decoding strategies to manage the trade-off between performance and compute. Our experiments demonstrate that CoRAG-8B achieves state-of-the-art performance on both multi- hop QA datasets and the KILT benchmark, surpassing many baselines built with larger LLMs. A comprehensive analysis is conducted to understand its scaling behavior and generalization capability. In the future, we intend to extend CoRAG to more challenging and economically valuable RAG tasks, advancing towards building factual and trustworthy AI systems. 7 Limitations and Broader Impacts This study primarily investigates RAG tasks characterized by short and easy-to-verify answers, such as multi-hop QA and entity linking. However, real-world applications often necessitate addressing more complex tasks that demand generating long-form outputs. A signiﬁcant challenge in long-form generation lies in the absence of robust evaluation metrics within the current research landscape. Regarding broader impacts, the proposed framework aims to improve the factuality and groundedness of language model outputs. It is anticipated that this work can facilitate more efﬁcient and effective information retrieval for users. Nevertheless, the inherent risk of hallucination persists and warrants careful monitoring in practical deployments. References [1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learn- ing to retrieve, generate, and critique"
"chain_of_retrieval_p10_c003","chain_of_retrieval","chain_of_retrieval.pdf","10","3","effective information retrieval for users. Nevertheless, the inherent risk of hallucination persists and warrants careful monitoring in practical deployments. References [1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learn- ing to retrieve, generate, and critique through self-reﬂection. In The Twelfth International 10"
"chain_of_retrieval_p11_c001","chain_of_retrieval","chain_of_retrieval.pdf","11","1","Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Open- Review.net, 2024. URL https://openreview.net/forum?id=hSyW5go0v8. [2] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document iden- tiﬁers. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/cd88d62a2063fdaf7ce6f9068fb15dcd-Abstract-Conference.html. [3] Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, and Wei Wang. Ask the right questions: Active question reformulation with reinforcement learning. arXiv preprint arXiv:1705.07830, 2017. [4] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschen- brenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeffrey Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. InForty- ﬁrst International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ghNRg2mEgN. [5] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407. 21783. [6] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. ArXiv preprint, abs/2312.10997, 2023. URL https://arxiv. org/abs/2312.10997. [7] Xinyu Guan,"
"chain_of_retrieval_p11_c002","chain_of_retrieval","chain_of_retrieval.pdf","11","2","Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. ArXiv preprint, abs/2312.10997, 2023. URL https://arxiv. org/abs/2312.10997. [7] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. ArXiv preprint, abs/2501.04519, 2025. URL https://arxiv.org/abs/2501.04519. [8] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 6609–6625, Barcelona, Spain (Online), 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https://aclanthology.org/2020.coling-main.580. [9] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. Multi-task retrieval- augmented text generation with relevance sampling. ArXiv preprint, abs/2207.03030, 2022. URL https://arxiv.org/abs/2207.03030. [10] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. ArXiv preprint, abs/2410.21276, 2024. URL https://arxiv.org/abs/2410.21276. [11] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251): 1–43, 2023. [12] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card."
"chain_of_retrieval_p11_c003","chain_of_retrieval","chain_of_retrieval.pdf","11","3","with retrieval augmented language models. Journal of Machine Learning Research, 24(251): 1–43, 2023. [12] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. ArXiv preprint, abs/2412.16720, 2024. URL https://arxiv.org/abs/2412.16720. [13] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992, Singapore, 2023. Association 11"
"chain_of_retrieval_p12_c001","chain_of_retrieval","chain_of_retrieval.pdf","12","1","for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.495. URL https:// aclanthology.org/2023.emnlp-main.495. [14] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented generation with long-context llms. ArXiv preprint, abs/2406.15319, 2024. URL https: //arxiv.org/abs/2406.15319. [15] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Za- mani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [16] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efﬁcient retrieval-augmented generation research.ArXiv preprint, abs/2405.13576, 2024. URL https://arxiv.org/abs/2405.13576. [17] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min- Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology. org/P17-1147. [18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769–6781, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550. [19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural"
"chain_of_retrieval_p12_c002","chain_of_retrieval","chain_of_retrieval.pdf","12","2","[19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026. [20] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html. [21] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: A survey on generative information retrieval. ArXiv preprint, abs/2404.14851, 2024. URL https://arxiv.org/abs/2404.14851. [22] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. ArXiv preprint, abs/2501.05366, 2025. URL https://arxiv.org/abs/2501.05366. [23] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. RA-DIT: retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=22OTbutug9. [24] Xueguang Ma,"
"chain_of_retrieval_p12_c003","chain_of_retrieval","chain_of_retrieval.pdf","12","3","Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. RA-DIT: retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=22OTbutug9. [24] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. In Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang, editors, Proceedings of the 47th International ACM SIGIR Conference 12"
"chain_of_retrieval_p13_c001","chain_of_retrieval","chain_of_retrieval.pdf","13","1","on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024 , pages 2421–2425. ACM, 2024. doi: 10.1145/3626772.3657951. URL https://doi.org/10.1145/3626772.3657951. [25] Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-tau Yih, Barlas Oguz, Veselin Stoyanov, and Gargi Ghosh. Multi-task retrieval for knowledge-intensive tasks. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1098–1111, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.89. URL https: //aclanthology.org/2021.acl-long.89. [26] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. ArXiv preprint, abs/2112.09332, 2021. URL https://arxiv.org/abs/2112.09332. [27] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2523–2544, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL https:// aclanthology.org/2021.naacl-main.200. [28] Oﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika"
"chain_of_retrieval_p13_c002","chain_of_retrieval","chain_of_retrieval.pdf","13","2","2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL https:// aclanthology.org/2021.naacl-main.200. [28] Oﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687–5711, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.ﬁndings-emnlp.378. URL https://aclanthology.org/2023.findings-emnlp. 378. [29] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264. [30] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 9248–9274, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.ﬁndings-emnlp.620. URL https: //aclanthology.org/2023.findings-emnlp.620. [31] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024. URL https://arxiv.org/abs/2403.05530. [32] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554, 2022. doi: 10.1162/tacl_a_00475. URL https: //aclanthology.org/2022.tacl-1.31. [33] Harsh Trivedi, Niranjan Balasubramanian, Tushar"
"chain_of_retrieval_p13_c003","chain_of_retrieval","chain_of_retrieval.pdf","13","3","2024. URL https://arxiv.org/abs/2403.05530. [32] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554, 2022. doi: 10.1162/tacl_a_00475. URL https: //aclanthology.org/2022.tacl-1.31. [33] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 10014–10037, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.557. URL https://aclanthology.org/2023.acl-long.557. 13"
"chain_of_retrieval_p14_c001","chain_of_retrieval","chain_of_retrieval.pdf","14","1","[34] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. ArXiv preprint, abs/2212.03533, 2022. URL https://arxiv.org/abs/2212.03533. [35] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. ArXiv preprint, abs/2401.00368, 2024. URL https://arxiv.org/abs/2401.00368. [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. [37] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. ArXiv preprint, abs/2412.15115, 2024. URL https://arxiv.org/abs/2412.15115. [38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [39] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, Brussels,"
"chain_of_retrieval_p14_c002","chain_of_retrieval","chain_of_retrieval.pdf","14","2","Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259. [40] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Grifﬁths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. [41] Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation for large language models. ArXiv preprint, abs/2411.19443, 2024. URL https://arxiv.org/ abs/2411.19443. [42] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. ArXiv preprint, abs/2410.04343, 2024. URL https://arxiv.org/ abs/2410.04343. [43] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html. A Implementation Details Rejection Sampling For each"
"chain_of_retrieval_p14_c003","chain_of_retrieval","chain_of_retrieval.pdf","14","3","Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html. A Implementation Details Rejection Sampling For each training instance, we sample up to 16 retrieval chains, with the maximum length randomly selected from the interval [1, 5]. The sampling temperature is set to 0.7 for sub-query generation and 0 for sub-answer generation. Chain generation is terminated if the sub-answer matches the correct answer or if the average conditional log-likelihood of the correct 14"
"chain_of_retrieval_p15_c001","chain_of_retrieval","chain_of_retrieval.pdf","15","1","answer exceeds−0.05. For each sub-query, we utilize the E5-large retriever 2 to retrieve the top-5 most relevant documents from the KILT version of the Wikipedia corpus [25]. This corpus comprises 36 million passages. Table 5: Hyperparameters for training CoRAG. Multi-hop QA KILT Benchmark Initialization Llama-3.1-8B-Instruct Learning rate 5× 10−6 10−5 Batch size 256 1024 Epoch 1 1 Warmup steps 100 100 # Training samples 125k 660k # Retrieved passages 20 20 Max sequence length 3072 3072 Table 6: Statistics of the datasets used for multi-hop QA training. 2WikiMultihopQA HotpotQA Bamboogle MuSiQue # Training Samples 15, 000 90 , 447 - 19, 938 # Validation Samples 12, 576 7 , 405 125 2 , 417 Multi-Hop QA Training Hyperparameters The training set is the union of the 2WikiMultihopQA, HotpotQA, and MuSiQue datasets, comprising a total of 125k samples as shown in Table 6. The Bamboogle dataset, consisting of only 125 questions, is reserved for evaluation only. Additional hyperparameters are detailed in Table 5. To balance the three loss terms in Section 3.2, we set a sample ratio of 0.2 for both the sub-query and sub-answer generation tasks; this ratio is also applied to the KILT training. KILT Training Hyperparameters We utilize the ofﬁcial training set of the KILT benchmark, omitting the ELI5 and WoW datasets due to the lack of reliable evaluation metrics. To balance the task distribution, we only select 100k samples for large datasets like T-REx and Zero-Shot RE. In accordance with the benchmark’s guidelines, we also add 100k"
"chain_of_retrieval_p15_c002","chain_of_retrieval","chain_of_retrieval.pdf","15","2","ELI5 and WoW datasets due to the lack of reliable evaluation metrics. To balance the task distribution, we only select 100k samples for large datasets like T-REx and Zero-Shot RE. In accordance with the benchmark’s guidelines, we also add 100k samples from the BLINK dataset for entity linking. Rather than using off-the-shelf retrievers, we ﬁne-tune an E5-Mistral retriever following Wang et al., and a RankLLaMA re-ranker following Ma et al.. We adhere to the exact training hyperparameters outlined in the original papers, except that the training data is replaced with the KILT training set. For training the RankLLaMA re-ranker, the backbone is initialized with the Llama-3-8B-Base model, as opposed to Llama-2, to enhance performance. Retrieval and re-ranking scores are presented in Table 8. All training jobs are conducted using 8 A100 GPUs. The multi-hop QA task requires less than 6 hours of training, whereas the KILT training takes approximately 30 hours. When submitting to the KILT leaderboard, we select the optimal decoding strategy for each task based on validation set performance. Decoding Strategies In the context of best-of-N sampling, the temperature is set to 0.7 for sub-query generation. For sub-answer generation and ﬁnal answer prediction, the temperature is always set to 0 across all decoding strategies. Regarding tree search, we set the expansion size to 4 and the number of rollouts to 2. Given that tree search incurs a signiﬁcantly higher token consumption compared to other decoding strategies, we limit the rollouts to a maximum of 2 steps for each"
"chain_of_retrieval_p15_c003","chain_of_retrieval","chain_of_retrieval.pdf","15","3","set the expansion size to 4 and the number of rollouts to 2. Given that tree search incurs a signiﬁcantly higher token consumption compared to other decoding strategies, we limit the rollouts to a maximum of 2 steps for each expansion. To avoid the model from generating repetitive sub-queries endlessly, any generated sub-query identical to previous ones is discarded. Evaluation For multi-hop QA tasks, we evaluate the performance using the exact match (EM) and F1 scores [18]. For Self-RAG-7B, we reproduce the results utilizing the FlashRAG [16] toolkit with the ofﬁcial checkpoint released by the authors. 2https://huggingface.co/intfloat/e5-large-v2 15"
"chain_of_retrieval_p16_c001","chain_of_retrieval","chain_of_retrieval.pdf","16","1","For the KILT benchmark, we employ the ofﬁcial evaluation scripts provided by the organizers. For Open QA tasks, the main evaluation metric is the EM score, while other task types are evaluated using accuracy scores. The KILT benchmark also offers a variant of the evaluation protocol that requires the model not only to generate the correct answer but also to provide the correct supporting evidence. However, our method spreads the evidence documents across the retrieval chain, rendering it challenging to conform to such an evaluation protocol. B Additional Results Table 7: Downstream results on the public validation set of the KILT benchmark. System Entity Linking Slot Filling Open QA Fact AIDA WnWi WnCw T-REx zsRE NQ HoPo TQA FEVER CoRAG-8B (Ours) ⊿ L=1, greedy 90.4 86.0 76.8 87.0 82.1 62.5 56.4 88.4 91.4 ⊿ L=6, greedy 92.7 87.4 75.8 86.6 83.8 63.2 59.1 88.6 93.8 ⊿ L=6, best-of-4 92.5 87.4 75.8 86.3 83.5 62.6 59.6 88.7 93.9 ⊿ L=6, tree search 91.8 86.8 75.5 86.4 83.0 62.4 59.9 88.9 93.9 Table 8: Retrieval results (R-Precision) on the public validation set of the KILT benchmark. For re-ranking, we use the top-100 candidates from the ﬁne-tuned retriever as input. System Entity Linking Slot Filling Open QA Fact AIDA WnWi WnCw T-REx zsRE NQ HoPo TQA FEVER Fine-tuned E5mistral 92.9 86.7 76.0 80.5 95.3 77.7 66.7 78.9 90.9 ⊿ w/ re-ranking 93.3 88.0 77.1 83.2 97.6 78.2 78.2 81.5 92.3 Different Decoding Strategies on the KILT Benchmark In Table 7, we present the"
"chain_of_retrieval_p16_c002","chain_of_retrieval","chain_of_retrieval.pdf","16","2","NQ HoPo TQA FEVER Fine-tuned E5mistral 92.9 86.7 76.0 80.5 95.3 77.7 66.7 78.9 90.9 ⊿ w/ re-ranking 93.3 88.0 77.1 83.2 97.6 78.2 78.2 81.5 92.3 Different Decoding Strategies on the KILT Benchmark In Table 7, we present the results of various decoding strategies applied to the validation set of the KILT benchmark. Given that most tasks within the KILT benchmark are much easier for strong dense retrievers compared to multi-hop QA, the disparity in performance across different decoding strategies is less pronounced. This observation underscores the necessity of developing a system capable of adaptively selecting the optimal decoding strategy to effectively balance the trade-off between performance and test-time compute. 4 8 16 Rejection Sampling Size 66 68 70 72 74 76EM 2WikiMultihopQA L=6, greedy 4 8 16 Rejection Sampling Size 50 52 54 56 58 60 HotpotQA L=6, greedy 4 8 16 Rejection Sampling Size 40 42 44 46 48 50 52 Bamboogle L=6, greedy 4 8 16 Rejection Sampling Size 22 24 26 28 30 32 MuSiQue L=6, greedy Figure 6: Scaling rejection sampling compute for training data generation. We vary the number of sampled chains from 4 to 16 while maintaining all other hyperparameters ﬁxed. Scaling Compute for Training Data Generation Within our proposed framework, rather than investing more compute at test time, we can scale the compute for retrieval chain generation during rejection sampling. By increasing the number of sampled chains, we may identify better chains that contribute to higher-quality training data. However, as illustrated"
"chain_of_retrieval_p16_c003","chain_of_retrieval","chain_of_retrieval.pdf","16","3","than investing more compute at test time, we can scale the compute for retrieval chain generation during rejection sampling. By increasing the number of sampled chains, we may identify better chains that contribute to higher-quality training data. However, as illustrated in Figure 6, no deﬁnitive trend emerges indicating that increasing the number of sampled chains always leads to better performance. Conversely, the training loss consistently decreases as we scale up rejection sampling, suggesting that the training data becomes less noisy and easier to ﬁt. We hypothesize that the majority of sampled 16"
"chain_of_retrieval_p17_c001","chain_of_retrieval","chain_of_retrieval.pdf","17","1","chains are already of high quality and that LM ﬁne-tuning exhibits considerable robustness to noisy training data. 2 4 6 8 10 Chain Length 30 32 34 36 38 40 42 44 46EM 2WikiMultihopQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 34 36 38 40 42 HotpotQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 25 30 35 40 45 Bamboogle greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 8 10 12 14 16 18 MuSiQue greedy best-of-4 best-of-8 4k 8k 16k 32k 64k 128k # Avg. T okens 30.0 32.5 35.0 37.5 40.0 42.5 45.0 47.5EM greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 34 36 38 40 42 greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 25 30 35 40 45 50 greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 8 10 12 14 16 18 20 greedy best-of-4 best-of-8 pareto frontier Figure 7: Scaling test-time compute on multi-hop QA datasets with Llama-3.1-8B-Instruct. No ﬁne-tuning is performed on the model weights. Scaling Test-Time Compute without Model Fine-Tuning In Figure 7, we present the scaling results on multi-hop QA datasets using the Llama-3.1-8B-Instruct model directly without any ﬁne- tuning. The scaling curves are similar to those observed in Figure 3, but the absolute performance is signiﬁcantly lower, indicating that targeted ﬁne-tuning is essential for improving the scaling upper bound. 0.2 0.5"
"chain_of_retrieval_p17_c002","chain_of_retrieval","chain_of_retrieval.pdf","17","2","the Llama-3.1-8B-Instruct model directly without any ﬁne- tuning. The scaling curves are similar to those observed in Figure 3, but the absolute performance is signiﬁcantly lower, indicating that targeted ﬁne-tuning is essential for improving the scaling upper bound. 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 70.0 70.5 71.0 71.5 72.0 72.5EM 2WikiMultihopQA L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 54.0 54.5 55.0 55.5 56.0 56.5 HotpotQA L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 46 48 50 52 Bamboogle L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 26.5 27.0 27.5 28.0 28.5 29.0 29.5 30.0 MuSiQue L=6, best-of-4 Figure 8: Effects of varying the sampling temperature on multi-hop QA datasets. Effects of Sampling Temperature In best-of-N sampling, the sampling temperature controls the diversity and quality trade-off in the generated retrieval chains. A higher temperature results in more diverse chains, albeit with the potential introduction of increased noise. Figure 8 illustrates the lack of a consistent conclusion regarding the impact of sampling temperature on performance. For the MuSiQue and HotpotQA datasets, a lower temperature generally yields superior results, whereas for the 2WikiMultihopQA dataset, a medium temperature leads to the best performance. As a result, we stick to a temperature of 0.7 for both rejection sampling and test-time decoding for simplicity. Extension to Other Model Families To demonstrate that our CoRAG framework is agnostic to model families and not limited to Llama-based architectures, we conduct experiments using Qwen3- 4B and Qwen3-8B [38] models following"
"chain_of_retrieval_p17_c003","chain_of_retrieval","chain_of_retrieval.pdf","17","3","rejection sampling and test-time decoding for simplicity. Extension to Other Model Families To demonstrate that our CoRAG framework is agnostic to model families and not limited to Llama-based architectures, we conduct experiments using Qwen3- 4B and Qwen3-8B [38] models following the same training procedure. As shown in Table 9, CoRAG consistently outperforms the baseline ﬁne-tuned models across all datasets and model sizes, with improvements of over 10 EM points on average. This validates that the chain-of-retrieval mechanism 17"
"chain_of_retrieval_p18_c001","chain_of_retrieval","chain_of_retrieval.pdf","18","1","Table 9: Extension to the Qwen3 model families. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 Fine-tuned Qwen3-4B w/ E5large 49.3 55.3 45.0 57.9 32.8 43.1 13.4 23.8 CoRAG-Qwen3-4B (L=6, greedy) 69.3 74.1 51.6 64.2 49.6 62.5 24.0 34.5 Fine-tuned Qwen3-8B w/ E5large 52.1 57.9 47.1 60.0 33.6 47.6 15.3 26.3 CoRAG-Qwen3-8B (L=6, greedy) 70.0 74.8 52.8 66.0 49.6 63.7 25.2 35.9 is broadly applicable across different model architectures and conﬁrms the generalizability of our approach beyond speciﬁc model families. Case Analysis Table 11 presents several model predictions on the validation set of the HotpotQA dataset. We compare the performance of RAG without chain-of-retrieval against CoRAG. CoRAG effectively decompose the complex multi-hop queries into a sequences of simpler sub-queries and dynamically conducts query reformulation when the retrieved information proves unhelpful. In the fourth example, the model initially hallucinates some incorrect information but subsequently self-corrects by verifying the poet’s name and country of origin through additional retrieval steps. C Statistical Signiﬁcance We compute the 95% conﬁdence intervals for our main results in Table 1 and 2 using the bootstrap resampling method. On all datasets except the Bamboogle dataset, we observe that the performance of CoRAG-8B is signiﬁcantly better than the baselines. D Prompts Table 10: Task descriptions for each dataset. Dataset Task Description HotpotQA / 2WikiMulti- hopQA answer multi-hop questions NQ answer natural questions from Google search AidaYago 2 / WnWi / WnCw / Blink link the mention surrounded by [START_ENT] and [END_ENT] to the title of"
"chain_of_retrieval_p18_c002","chain_of_retrieval","chain_of_retrieval.pdf","18","2","for each dataset. Dataset Task Description HotpotQA / 2WikiMulti- hopQA answer multi-hop questions NQ answer natural questions from Google search AidaYago 2 / WnWi / WnCw / Blink link the mention surrounded by [START_ENT] and [END_ENT] to the title of the correct Wikipedia page FEVER verify if the claim is supported or refuted T-REx / Zero-Shot RE given head entity and relation separated by [SEP], ﬁnd the correct tail entity, return the title of its Wikipedia page Trivia QA answer trivia questions MuSiQue / Bamboogle answer multi-hop questions All prompts are listed in this section. Texts highlighted in blue denote placeholders to be ﬁlled in. The “ task description” ﬁeld is dataset-speciﬁc, with corresponding values detailed in Table 10. 18"
"chain_of_retrieval_p19_c001","chain_of_retrieval","chain_of_retrieval.pdf","19","1","Table 11: Examples from the validation set of the HotpotQA dataset. For conciseness, all retrieved documents at each step are omitted. Correct answers are highlighted in blue, while incorrect answers are highlighted in red. Query:What wrestling team is Mathew Thomas Rehwoldt a part of? RAG without Chain-of-Retrieval Final Answer:WWE CoRAG (Ours) Sub-query 1: What is Mathew Thomas Rehwoldt’s profession? Sub-answer 1: No relevant information found. Sub-query 2: What is Mathew Thomas Rehwoldt’s name in the wrestling industry? Sub-answer 2: Aiden English Sub-query 3: What wrestling team is Aiden English a part of? Sub-answer 3: The Vaudevillains Final Answer:The Vaudevillains Query:How many months apart are Johan Mjällby and Neil Lennon in age? RAG without Chain-of-Retrieval Final Answer:two months CoRAG (Ours) Sub-query 1: What is Johan Mjällby’s birthdate? Sub-answer 1: 9 February 1971 Sub-query 2: What is Neil Lennon’s birthdate? Sub-answer 2: 25 June 1971 Sub-query 3: What is the difference in months between 9 February 1971 and 25 June 1971? Sub-answer 3: 4 months Final Answer:4  Query:Thomas Parker Sanborn was a model for the protagonist in the only novel by a poet originally from what country? RAG without Chain-of-Retrieval Final Answer:Italian CoRAG (Ours) Sub-query 1: What country is the poet originally from? Sub-answer 1: Hungary, Romania, Switzerland, China, Ukraine, France, Turkey. Sub-query 2: What is the name of the poet who wrote the only novel? Sub-answer 2: Emily Brontë Sub-query 3: What country is the poet who wrote the only novel originally from? Sub-answer 3: United States Sub-query 4: What"
"chain_of_retrieval_p19_c002","chain_of_retrieval","chain_of_retrieval.pdf","19","2","Turkey. Sub-query 2: What is the name of the poet who wrote the only novel? Sub-answer 2: Emily Brontë Sub-query 3: What country is the poet who wrote the only novel originally from? Sub-answer 3: United States Sub-query 4: What is the name of the poet who wrote the only novel that Thomas Parker Sanborn was a model for? Sub-answer 4: George Santayana Sub-query 5: What poet wrote the only novel that Thomas Parker Sanborn was a model for? Sub-answer 5: George Santayana Sub-query 6: What country is the poet George Santayana originally from? Sub-answer 6: Spain. Final Answer:Spain Query:Which university is further North, University of New Hampshire or Stony Brook University? RAG without Chain-of-Retrieval Final Answer:University of New Hampshire CoRAG (Ours) Sub-query 1: What is the location of the University of New Hampshire? Sub-answer 1: Durham, New Hampshire. Sub-query 2: What is the location of Stony Brook University? Sub-answer 2: Stony Brook, NY . Sub-query 3: What is the latitude of Durham, New Hampshire? Sub-answer 3: No relevant information found. Sub-query 4: What is the latitude of Stony Brook, NY? Sub-answer 4: No relevant information found. Final Answer:University of New Hampshire 19"
"chain_of_retrieval_p20_c001","chain_of_retrieval","chain_of_retrieval.pdf","20","1","Prompt: Sub-query Generation You are using a search engine to answer the main query by iteratively searching the web. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search engine may not understand complex questions. ## Previous intermediate queries and answers {intermediate queries and answers} ## Task description {task description} ## Main query to answer {query} Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else. Prompt: Intermediate Answer Generation Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond “No relevant information found” if the documents do not contain useful information. ## Documents {retrieved documents} ## Query {sub-query} Respond with a concise answer only, do not explain yourself or output anything else. Prompt: Final Answer Generation Given the following intermediate queries and answers, generate a ﬁnal answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate. ## Documents {retrieved documents} ## Intermediate queries and answers {intermediate queries and answers} ## Task description {task description} ## Main query {query} Respond with an appropriate answer only, do not explain yourself or output anything else. 20"
"chain_of_retrieval_p21_c001","chain_of_retrieval","chain_of_retrieval.pdf","21","1","Prompt: Learning to Stop Given the following intermediate queries and answers, judge whether you have enough information to answer the main query. If you believe you have enough information, respond with “Yes”, otherwise respond with “No”. ## Intermediate queries and answers {intermediate queries and answers} ## Main query {query} Respond with “Yes” or “No” only, do not explain yourself or output anything else. 21"
"chain_of_retrieval_p22_c001","chain_of_retrieval","chain_of_retrieval.pdf","22","1","NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? Answer: [Yes] Justiﬁcation: The main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reﬂect how much the results can be expected to generalize to other settings. • It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justiﬁcation: We discuss the limitations of our work in Section 7. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate ""Limitations"" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-speciﬁcation,"
"chain_of_retrieval_p22_c002","chain_of_retrieval","chain_of_retrieval.pdf","22","2","The authors are encouraged to create a separate ""Limitations"" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-speciﬁcation, asymptotic approximations only holding locally). The authors should reﬂect on how these assumptions might be violated in practice and what the implications would be. • The authors should reﬂect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reﬂect on the factors that inﬂuence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efﬁciency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in"
"chain_of_retrieval_p22_c003","chain_of_retrieval","chain_of_retrieval.pdf","22","3","honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be speciﬁcally instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] 22"
"chain_of_retrieval_p23_c001","chain_of_retrieval","chain_of_retrieval.pdf","23","1","Justiﬁcation: No theoretical results are provided in the paper. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justiﬁcation: Implementation details are provided in Section A. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps"
"chain_of_retrieval_p23_c002","chain_of_retrieval","chain_of_retrieval.pdf","23","2","will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or veriﬁable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g.,"
"chain_of_retrieval_p23_c003","chain_of_retrieval","chain_of_retrieval.pdf","23","3","algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufﬁcient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? 23"
"chain_of_retrieval_p24_c001","chain_of_retrieval","chain_of_retrieval.pdf","24","1","Answer: [Yes] Justiﬁcation: We include implementation details in the paper and will release the code and data after publication. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they"
"chain_of_retrieval_p24_c002","chain_of_retrieval","chain_of_retrieval.pdf","24","2","possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justiﬁcation: See Section 4.1 and A. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical signiﬁcance Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate information about the statistical signiﬁcance of the experiments? Answer: [Yes] Justiﬁcation: We report conﬁdence intervals in Section C. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ- dence intervals, or statistical signiﬁcance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to"
"chain_of_retrieval_p24_c003","chain_of_retrieval","chain_of_retrieval.pdf","24","3","bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). 24"
"chain_of_retrieval_p25_c001","chain_of_retrieval","chain_of_retrieval.pdf","25","1","• It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not veriﬁed. • For asymmetric distributions, the authors should be careful not to show in tables or ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding ﬁgures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufﬁcient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justiﬁcation: We provide details on compute resources in Section A. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary"
"chain_of_retrieval_p25_c002","chain_of_retrieval","chain_of_retrieval.pdf","25","2","amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justiﬁcation: We have reviewed the NeurIPS Code of Ethics and our work conforms to it. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justiﬁcation: Please see Section 7. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations (e.g., deployment of technologies that could"
"chain_of_retrieval_p25_c003","chain_of_retrieval","chain_of_retrieval.pdf","25","3","work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc groups), privacy considerations, and security considerations. 25"
"chain_of_retrieval_p26_c001","chain_of_retrieval","chain_of_retrieval.pdf","26","1","• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efﬁciency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justiﬁcation: No safeguards are needed for our work as it does not involve high-risk data or models. Guidelines: • The answer NA means that"
"chain_of_retrieval_p26_c002","chain_of_retrieval","chain_of_retrieval.pdf","26","2","a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justiﬁcation: No safeguards are needed for our work as it does not involve high-risk data or models. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety ﬁlters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justiﬁcation: All assets used in this paper are properly credited and the licenses are respected. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name"
"chain_of_retrieval_p26_c003","chain_of_retrieval","chain_of_retrieval.pdf","26","3","not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. 26"
"chain_of_retrieval_p27_c001","chain_of_retrieval","chain_of_retrieval.pdf","27","1","• If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justiﬁcation: The documentation is provided in the supplemental material. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip ﬁle. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justiﬁcation: No crowdsourcing or human subjects were involved in this research. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is ﬁne, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection,"
"chain_of_retrieval_p27_c002","chain_of_retrieval","chain_of_retrieval.pdf","27","2","is ﬁne, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justiﬁcation: No human subjects were involved in this research. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary signiﬁcantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage 27"
"chain_of_retrieval_p28_c001","chain_of_retrieval","chain_of_retrieval.pdf","28","1","Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientiﬁc rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justiﬁcation: The core method development in this research does not involve LLMs as any important, original, or non-standard components. Guidelines: • The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. • Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM) for what should or should not be described. 28"
"gfm_rag_p01_c001","gfm_rag","gfm_rag.pdf","1","1","GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation Linhao Luo1∗, Zicheng Zhao2∗, Gholamreza Haffari1, Chen Gong3, Dinh Phung1, Shirui Pan4† 1Monash University, 2Nanjing University of Science and Technology, 3Shanghai Jiao Tong University,4Griffith University, {Linhao.Luo,gholamreza.haffari,dinh.phung}@monash.edu, zicheng.zhao@njust.edu.cn, chen.gong@sjtu.edu.cn, s.pan@griffith.edu.au Project page: https://rmanluo.github.io/gfm-rag Abstract Retrieval-augmented generation (RAG) has proven effective in integrating knowl- edge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their per- formance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds graph structure to explicitly model these relationships, enabling more effec- tive and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented gener- ation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any domain-specific fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate thatGFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement. 1 Introduction Recent advancements in large language models (LLMs) [ 47, 42, 70]"
"gfm_rag_p01_c002","gfm_rag","gfm_rag.pdf","1","2","QA datasets and seven domain-specific RAG datasets demonstrate thatGFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement. 1 Introduction Recent advancements in large language models (LLMs) [ 47, 42, 70] have greatly propelled the evolution of natural language processing, positioning them as foundational models for artificial general intelligence (AGI). Despite the remarkable reasoning ability [ 48], LLMs are still limited in accessing real-time information and lack of domain-specific knowledge, which is outside the pre-training corpus. To address these limitations, retrieval-augmented generation (RAG) [12] has become a popular paradigm in adding new knowledge to the static LLMs by retrieving relevant documents into the context of LLM generation. Existing RAG methods typically retrieve documents independently, making it difficult to capture com- plex relationships between pieces of knowledge [30, 5, 43]. This limitation hampers the performance of LLMs in integrating knowledge across document boundaries, particularly in multi-hop reasoning tasks [72, 63] and real-world applications like legal judgment [28] and medical diagnoses [25], which ∗Equal Contribution. †Corresponding author. 39th Conference on Neural Information Processing Systems (NeurIPS 2025)."
"gfm_rag_p02_c001","gfm_rag","gfm_rag.pdf","2","1","require reasoning over multiple sources. Although recent methods have expanded the retrieval process into multiple steps and incorporate LLM reasoning, they still encounter high computational costs due to iterative retrieval and reasoning with LLMs [64, 59, 26]. Recently, graph-enhanced retrieval augmented generation (GraphRAG) [51, 17] has emerged as a novel solution that builds a graph structure to explicitly model the intricate relationships between knowledge. This enables the development of a graph-enhanced retriever to identify relevant infor- mation using graphs. The structural nature of graphs allows GraphRAG to capture global context and dependencies among documents, significantly improving reasoning across multiple sources [9]. Methods like HippoRAG [16] enhance retrieval by employing a personalized PageRank algorithm to locate relevant knowledge with graphs. However, these algorithms rely solely on the graph structure, which is often noisy or incomplete, limiting their overall performance. Alternative methods [41, 18] incorporate graph neural networks (GNNs) into the retrieval process. These methods have shown impressive performance due to GNNs’ powerful multi-hop reasoning capabilities on graphs [ 73]. Nevertheless, they still face limitations in generalizability since they require training from scratch on new datasets. Nowadays, the search for a foundation GNN model that can transfer and generalize across different datasets has been an active research topic. Ideally, a foundation GNN or graph foundation model (GFM) can benefit from large-scale training and generalize across diverse graphs [40, 37]. Efforts have been made to identify transferable graph tokens (e.g., motifs, sub-trees, and relation graphs) [11, 66, 68] that can be shared among"
"gfm_rag_p02_c002","gfm_rag","gfm_rag.pdf","2","2","or graph foundation model (GFM) can benefit from large-scale training and generalize across diverse graphs [40, 37]. Efforts have been made to identify transferable graph tokens (e.g., motifs, sub-trees, and relation graphs) [11, 66, 68] that can be shared among different graphs for GFM design. However, these methods primarily focus on graph-related tasks (e.g., node classification and link prediction), leaving the design of a GFM to enhance LLMs’ reasoning ability unexplored. Documents KG-index Q GFM Retriever Retrieved Docs. Doc. Ranker Q LLM A Query Answer Figure 1: The overview framework of GFM-RAG. To bridge the gap, in this paper, we propose an effective, efficient, and general graph founda- tion model for retrieval augmented generation (GFM-RAG), thereby enhancing LLMs’ reason- ing ability. As shown in Figure 1, we create a knowledge graph index (KG-index) from doc- uments in each dataset. The KG-index con- sists of interconnected factual triples pointing to the original documents, which serves as a structural knowledge index across multiple sources, enhancing the integration of diverse knowledge for complex reasoning tasks [ 16]. Then, we present the graph foundation model retriever (GFM retriever), driven by a query-dependent GNN that captures complex query-knowledge relationships in a unified, transferable space of semantics and graph structure. Through multi-layer message passing, the GFM retriever enables efficient multi-hop retrieval in a single step, surpassing previous multi-step methods. The GFM retriever, with 8M parameters, undergoes a two-stage training: self-supervised KG completion pre-training and supervised document retrieval fine-tuning on large-scale datasets, including 60 knowledge graphs with"
"gfm_rag_p02_c003","gfm_rag","gfm_rag.pdf","2","3","retriever enables efficient multi-hop retrieval in a single step, surpassing previous multi-step methods. The GFM retriever, with 8M parameters, undergoes a two-stage training: self-supervised KG completion pre-training and supervised document retrieval fine-tuning on large-scale datasets, including 60 knowledge graphs with over 14M triples and 700k documents. This large-scale training ensures the generalizability of GFM retriever to be applied to unseen datasets without further training. In experiments, GFM-RAG achieves state-of-the-art performance across three multi-hop QA datasets, demonstrating its effectiveness and efficiency in multi-hop reasoning. It also generalizes well across seven RAG datasets from diverse domains, such as biomedical, customer service, and general knowledge, without requiring additional training. Furthermore, GFM-RAG follows the neural scaling law [19], whose performance benefits from training data and model size scaling, emphasizing its potential as a foundational model for future improvements. The main contributions of this paper are as follows: • We introduce a graph foundation model for retrieval augmented generation ( GFM-RAG), powered by a novel query-dependent GNN to enable efficient multi-hop retrieval within a single step. • We train a large-scale model with 8M parameters, marking the first graph foundation model (GFM) that can be applied directly to various unseen datasets for retrieval augmented generation. • We evaluate GFM-RAG on three multi-hop QA datasets and seven domain-specific RAG datasets, achieving state-of-the-art performance across all, demonstrating its effectiveness, efficiency, generalizability, and potential as a foundational model for further enhancement. 2"
"gfm_rag_p03_c001","gfm_rag","gfm_rag.pdf","3","1","2 Related Work Retrieval-augmented generation (RAG) [12] provides an effective way to integrate external knowl- edge into large language models (LLMs) by retrieving relevant documents to facilitate LLM genera- tion. Early works adopt the pre-trained dense embedding model to encode documents as separate vectors [30, 5, 34, 43], which are then retrieved by calculating the similarity to the query. Despite efficiency and generalizability, these methods struggle to capture complex document relationships. Subsequent studies have explored multi-step retrieval, where LLMs guide an iterative process to retrieve and reason over multiple documents [64, 24, 58]. However, this approach is computationally expensive. Graph-enhanced retrieval augmented generation (GraphRAG) [51, 17] is a novel approach that builds graphs to explicitly model the complex relationships between knowledge, facilitating comprehensive retrieval and reasoning. Early research focuses on retrieving information from existing knowledge graphs (KGs), such as WikiData [65] and Freebase [3], by identifying relevant facts or reasoning paths [33, 38, 50]. Recent studies have integrated documents with KGs to improve knowledge coverage and retrieval [9, 35]. A graph structure is built from these documents to aid in identifying relevant content for LLM generation [8]. Based on graphs, LightRAG [15] incorporates graph structures into text indexing and retrieval, enabling efficient retrieval of entities and their relationships. HippoRAG [ 16] enhances multi-hop retrieval by using a personalized PageRank algorithm to locate relevant knowledge with graphs. However, the graph structure can be noisy and incomplete, leading to suboptimal performance. Efforts to incorporate GNNs into graph-enhanced RAG [41, 18] have shown"
"gfm_rag_p03_c002","gfm_rag","gfm_rag.pdf","3","2","16] enhances multi-hop retrieval by using a personalized PageRank algorithm to locate relevant knowledge with graphs. However, the graph structure can be noisy and incomplete, leading to suboptimal performance. Efforts to incorporate GNNs into graph-enhanced RAG [41, 18] have shown impressive results due to the multi-hop graph reasoning capabilities of GNNs in handling incomplete graphs [73]. Nonetheless, these methods still limit in generalizability due to the lack of a graph foundational model. Graph Foundation models (GFM) aims to be a large-scale model that can generalize to various datasets [40, 37]. The main challenge in designing GFMs is identifying graph tokens that capture invariance across diverse graph data. For instance, ULTRA [11] employs four fundamental relational interactions in knowledge graphs (KGs) to create a GFM with 0.2M parameters for link prediction. OpenGraph [68] develops a graph tokenizer that converts graphs into a unified node token representa- tion, enabling transformer-like GFMs for tasks such as link prediction and node classification. GFT [66] introduces a transferable tree vocabulary to construct a GFM that demonstrates effectiveness across various tasks and domains in graph learning. Despite these successful efforts, most methods primarily focus on conventional graph-related tasks, and transformer-like GFMs [61, 60] struggle with large-scale graphs and capture logical association [52]. How to design a GNN-based GFM to enhance the reasoning of LLM remains an open question. 3 Approach The proposed GFM-RAG essentially implements a GraphRAG paradigm by constructing graphs from documents and using a graph-enhanced retriever to retrieve relevant documents. GFM-RAG Overview. Given a"
"gfm_rag_p03_c003","gfm_rag","gfm_rag.pdf","3","3","GNN-based GFM to enhance the reasoning of LLM remains an open question. 3 Approach The proposed GFM-RAG essentially implements a GraphRAG paradigm by constructing graphs from documents and using a graph-enhanced retriever to retrieve relevant documents. GFM-RAG Overview. Given a set of documents D = {D1, D2, . . . , D|D|}, we construct a knowl- edge graph G = {(e, r, e′) ∈ E × R × E} , where e, e′ ∈ E and r ∈ R denote the set of entities and relations extracted from D, respectively. For a user query q, we aim to design a graph-enhanced retriever to obtain relevant documents from D by leveraging the knowledge graph G. The whole GFM-RAG process can be formulated as: G = KG-index(D), (1) DK = GFM-Retriever(q, D, G), (2) a = LLM(q, DK). (3) In the first step, KG-index(·) constructs a knowledge graph index G from the document corpus D, followed by our proposed graph foundation model retriever (GFM-Retriever), which is pre-trained on large-scale datasets. It retrieves top-K documents based on any user query q and knowledge graph index G. The retrieved documents DK, along with the query q, are then input into a large language model (LLM) to generate the final answera. These three main components in GFM-RAG are illustrated in Figure 2 and will be detailed next. 3"
"gfm_rag_p04_c001","gfm_rag","gfm_rag.pdf","4","1","Graph Foundation Model for Retrieval Augmented Generation Barack Obama (born in August 4, 1961, Honolulu) is an American politician… He married to Michelle Obama. Honolulu is the capital and most populous city of the U.S. state of Hawaii, which is in the Pacific Ocean. Michelle Obama is served as the first lady of the United States from 2009 to 2017, being married to Barack Obama. USA is a country primarily located in North America. It is a federal union of 50 states, the federal capital district of Washington, D.C. born_in married_to politician_of city_of live_in Barack Obama Michelle Obama Washington, D.C. Honolulu USA capital_of Q: Barack Obama is the politician of which country? 0.1 0.9 0.2 0.6 Document Corpus Query Embedding ① KG-Index Construction ② Query Initialization ③ Query-dependent Message Passing Knowledge Graph Ent. Relevance Scores Doc. 1 Doc. 2 Doc. 3 Doc. 4 Graph Foundation GNN Model Doc. Ranker LLMQ A: USA Ent. to Doc. Inverted Index Retrieved Docs. ⑤ LLM Generation Q: (Barack Obamna, born_in, ?) A: Honolulu Triple Target Ent. Stage 1: Self-supervised KG Completion Pre-training Graph Foundation GNN Model Q: Where was Barack Obamna born in? A: Honolulu, USA Question Target Ents. Stage 2: Supervised Document Retrieval Finetuning Graph Foundation GNN Model Supporting Doc. ④ Doc. Ranking Training Knowledge Graphs Training Question-Doc. Pairs and KG-Indexes Figure 2: The detailed framework of GFM-RAG and training processes of graph foundation model. The GFM-RAG consists of three main components: A. KG-index construction, which constructs a knowledge graph index from document"
"gfm_rag_p04_c002","gfm_rag","gfm_rag.pdf","4","2","Training Knowledge Graphs Training Question-Doc. Pairs and KG-Indexes Figure 2: The detailed framework of GFM-RAG and training processes of graph foundation model. The GFM-RAG consists of three main components: A. KG-index construction, which constructs a knowledge graph index from document corpus ( 1 ); B. graph foundation model retriever (GFM retriever), which is pre-trained on large-scale datasets and could retrieve documents based on any user query and KG-index ( 2 3 ); and C. documents ranking and answer generation, which ranks retrieved documents and generates final answer ( 4 5 ). 3.1 KG-index Construction Conventional embedding-based index methods encode documents as separate vectors [ 30, 5, 43], which are limited in modeling the relationships between them. Knowledge graphs (KGs), on the other hand, explicitly capturing the relationships between millions of facts, can provide a structural index of knowledge across multiple documents [9, 16]. The structural nature of the KG-index aligns well with the human hippocampal memory indexing theory [62], where the KG-index functions like an artificial hippocampus to store associations between knowledge memories, enhancing the integration of diverse knowledge for complex reasoning tasks [16]. To construct the KG-index, given a set of documents D, we first extract entities E and relations R to form triples T from documents. Then, the entity to document inverted index M ∈ {0, 1}|E|×|D| is constructed to record the entities mentioned in each document. Such a process can be achieved by existing open information extraction (OpenIE) tools [1, 77, 49]. To better capture the connection"
"gfm_rag_p04_c003","gfm_rag","gfm_rag.pdf","4","3","entity to document inverted index M ∈ {0, 1}|E|×|D| is constructed to record the entities mentioned in each document. Such a process can be achieved by existing open information extraction (OpenIE) tools [1, 77, 49]. To better capture the connection between knowledge, we further conduct the entity resolution [ 13, 74] to add additional edges T + between entities with similar semantics, e.g., (USA, equivalent, United States of America ). Therefore, the final KG-index G is constructed as G = {(e, r, e′) ∈ T ∪ T +}. In implementation, we leverage an LLM [47] as the OpenIE tool (prompts are shown in Table 22) and a pre-trained dense embedding model [55] for entity resolution. Details can be found in Appendix D.1. 3.2 Graph Foundation Model (GFM) Retriever The GFM retriever is designed to retrieve relevant documents based on any user query and the constructed KG-index. While the KG-index offers a structured representation of knowledge, it still 4"
"gfm_rag_p05_c001","gfm_rag","gfm_rag.pdf","5","1","suffers from incompleteness and noise, resulting in suboptimal retrieval performance when solely relying on its structure [16]. Recently, graph neural networks (GNNs) [67] have shown impressive multi-hop reasoning ability by capturing the complex relationships between knowledge for retrieval or question answering [41, 18]. However, existing GNNs are limited in generalizability, as they are usually trained on specific graphs [40, 37], which limits their application to unseen corpora and KGs. Therefore, there is still a need for a graph foundation model that can be directly applied to unseen datasets and KGs without additional training. To address these issues, we propose the first graph foundation model-powered retriever (GFM retriever), which harnesses the graph reasoning ability of GNNs to capture the complex relationships between queries, documents, and knowledge graphs in a unified and transferable space. The GFM retriever employs a query-dependent GNN to identify relevant entities in graphs that will aid in locating pertinent documents. After pre-training on large-scale datasets, the GFM retriever can be directly applied to new corpora and KGs without further training. 3.2.1 Query-dependent GNN Conventional GNNs [14] follow the message passing paradigm, which iteratively aggregates informa- tion from neighbors to update entity representations. Such a paradigm is not suitable for the GFM retriever as it is graph-specific and neglects the relevance of queries. Recent query-dependent GNNs [78, 11] have shown promising results in capturing query-specific information and generalizability to unseen graphs, which is essential for the GFM retriever and can be formulated as: H L q = GNNq(q, G,"
"gfm_rag_p05_c002","gfm_rag","gfm_rag.pdf","5","2","the relevance of queries. Recent query-dependent GNNs [78, 11] have shown promising results in capturing query-specific information and generalizability to unseen graphs, which is essential for the GFM retriever and can be formulated as: H L q = GNNq(q, G, H 0), (4) where H 0 ∈ R|E|×d denotes initial entity features, and H L q denotes the updated entity representations conditioned on query q after L layers of query-dependent message passing. The query-dependent GNN is theoretically proven to exhibit multi-hop logical reasoning ability [21, 73, 52] (detailed in Appendix A), which is selected as the backbone of our GFM retriever. It allows the GFM retriever to dynamically adjust the message passing process based on user queries and find the most relevant information on the graph with multi-hop reasoning. The path interpretation for this multi-hop reasoning process is shown in Section 4.8. Query Initialization. Given a query q, we first encode it into a query embedding with a sentence embedding model: q = SentenceEmb(q), q ∈ Rd, (5) where d denotes the dimension of the query embedding. Then, for all the entities mentioned in the query eq ∈ E q ⊆ E , we initialize their entity features as q while others as zero vectors: H 0 = q, e ∈ E q, 0, otherwise. (6) Query-dependent Message Passing. The query-dependent message passing will propagate the information from the question entities to other entities in the KG to capture their relevance to the query. The message passing process can be"
"gfm_rag_p05_c003","gfm_rag","gfm_rag.pdf","5","3","∈ E q, 0, otherwise. (6) Query-dependent Message Passing. The query-dependent message passing will propagate the information from the question entities to other entities in the KG to capture their relevance to the query. The message passing process can be formulated as: Triple-level: h0 r = SentenceEmb(r), h0 r ∈ Rd, (7) ml+1 e = Msg(hl e, gl+1(hl r), hl e′), (e, r, e′) ∈ G , (8) Entity-level: hl+1 e = Update(hl e, Agg({ml+1 e′ |e′ ∈ N r(e), r ∈ R})), (9) where hl e, hl r denote the entity and relation embeddings at layer l, respectively. The relation em- beddings h0 r are also initialized using the same sentence embedding model as the query, reflecting their semantics (e.g., “born_in”), and updated by a layer-specific function gl+1(·), implemented as a 2-layer MLP. The Msg(·) is operated on all triples in the KG to generate messages, which is implemented with a non-parametric DistMult [71] following the architecture of NBFNet [78]. For each entity, we aggregate the messages from its neighbors Nr(e) with relation r using sum and update the entity representation with a single linear layer. 5"
"gfm_rag_p06_c001","gfm_rag","gfm_rag.pdf","6","1","After L layers message passing, a final MLP layer together with a sigmoid function maps the entity embeddings to their relevance scores to the query: Pq = σ(MLP(H L q )), Pq ∈ R|E|×1. (10) Generalizability. Since the query, entity, and relation embeddings are initialized using the same sentence embedding model with identical dimensions, the query-dependent GNN can be directly applied to different queries and KGs. This allows it to learn complex relationships between queries and entities by taking into account both the semantics and structure of the KG through training on large-scale datasets. 3.2.2 Training Process Training Objective. The training objective of the GFM retriever is to maximize the likelihood of the relevant entities to the query, which can be optimized by minimizing the binary cross-entropy (BCE) loss: LBCE = − 1 |Aq| X e∈Aq log Pq(e) − 1 |E -| X |E -| log(1 − Pq(e)), (11) where Aq denotes the set of target relevant entities to the query q, and E - ⊆ E \\ A q denotes the set of negative entities sampled from the KG. However, due to the sparsity of the target entities, the BCE loss may suffer from the gradient vanishing problem [36]. To address this issue, we further introduce the ranking loss [2] to maximize the margin between the positive and negative entities: LRANK = − 1 |Aq| X e∈Aq Pq(e)P e′∈E - Pq(e′) . (12) The final training objective is the weighted combination of the BCE loss and ranking loss: L"
"gfm_rag_p06_c002","gfm_rag","gfm_rag.pdf","6","2","[2] to maximize the margin between the positive and negative entities: LRANK = − 1 |Aq| X e∈Aq Pq(e)P e′∈E - Pq(e′) . (12) The final training objective is the weighted combination of the BCE loss and ranking loss: L = αLBCE + (1 − α)LRANK. (13) Self-supervised KG Completion Pre-training. To enhance the graph reasoning capability of the GFM retriever, we first pre-train it on a large-scale knowledge graph (KG) completion task. We sample a set of triples from the KG index and mask either the head or tail entity to create synthetic queries in the form q = ( e, r, ?) or (?, r, e′), with the masked entity serving as the target entity Aq = {e} or {e′}. The GFM retriever is then trained to predict the masked entity using both the query and the KG, as outlined in equation 13. Supervised Document Retrieval Fine-tuning. After self-supervised pre-training, we supervised fine-tune the GFM retriever on a labeled document retrieval task. In this task, queries q are natural language questions, and target entities Aq are extracted from labeled supporting documents Dq. The GFM retriever is trained to retrieve relevant entities from the KG index using the same training objective as in equation 13. 3.3 Documents Ranking and Answer Generation Given the entity relevance scores Pq ∈ R|E|×1 predicted by the GFM retriever, we first retrieve the top-T entities E T q with the highest relevance scores as: E T q = arg top-T (Pq), E T q"
"gfm_rag_p06_c003","gfm_rag","gfm_rag.pdf","6","3","Answer Generation Given the entity relevance scores Pq ∈ R|E|×1 predicted by the GFM retriever, we first retrieve the top-T entities E T q with the highest relevance scores as: E T q = arg top-T (Pq), E T q = {e1, . . . , eT }. (14) These retrieved entities are then used by the document ranker to obtain the final documents. To diminish the influence of popular entities, we weight the entities by the inverse of their frequency as entities mentioned in the document inverted index M ∈ { 0, 1}|E|×|D| and calculate the final document relevance scores by summing the weights of entity mentioned in documents: Fe = ( 1P d∈D M [e,d] , e ∈ E T q , 0, otherwise, (15) Pd = M ⊤Fe, Pd ∈ R|D|×1. (16) The top-K documents are retrieved based on the document relevance scores Pd and fed into the context of LLMs, with a retrieval augmented generation manner, to generate the final answer: DK = arg top-K(Pd), DK = {D1, . . . , DK}, (17) a = LLM(q, DK). (18) 6"
"gfm_rag_p07_c001","gfm_rag","gfm_rag.pdf","7","1","4 Experiment In experiments, we aim to address the following research questions: (1) How doesGFM-RAG perform in multi-hop retrieval and QA tasks? (Sections 4.2 and 4.3); (2) What are the efficiency and effectiveness of GFM-RAG in multi-hop retrieval? (Section 4.4); (3) How well does GFM-RAG generalize to unseen datasets as a foundation model? (Section 4.6); (4) How does the performance of GFM-RAG scale with training as a foundation model? (Section 4.7); (5) How to interpret GFM-RAG in multi-hop reasoning? (Section 4.8). 4.1 Experimental Setup Datasets. We first evaluate the effectiveness ofGFM-RAG on three widely-used multi-hop QA datasets, including HotpotQA [72], MuSiQue [63], and 2WikiMultiHopQA (2Wiki) [20]. We also evaluate the performance of GFM-RAG on seven RAG datasets from three domains, including biomedical [25], custom support [54, 44, 39, 4], and general knowledge [45, 27], to demonstrate the generalizability of GFM-RAG as the foundation model. The detailed statistics of the test datasets are shown in the Appendix B. Baselines. We compare against several widely used retrieval methods under three categories: (1) single-step naive methods: BM25 [53], Contriever [22], GTR [46], ColBERTv2 [55], RAPTOR [56], Proposition [6]; (2) graph-enhanced methods: GraphRAG (MS) [9], LightRAG [15], HippoRAG [16], SubgraphRAG [32], G-retriever [18]; (3) multi-step methods: Adaptive-RAG [23], FLARE [24], and IRCoT [64] framework that can be integrated with arbitrary retrieval methods to conduct multi-step retrieval and reasoning. The detailed introduction of the baselines are shown in the Appendix C. Metrics. For retrieval performance, we use recall@2 (R@2) and recall@5 (R@5) as evaluation metrics."
"gfm_rag_p07_c002","gfm_rag","gfm_rag.pdf","7","2","that can be integrated with arbitrary retrieval methods to conduct multi-step retrieval and reasoning. The detailed introduction of the baselines are shown in the Appendix C. Metrics. For retrieval performance, we use recall@2 (R@2) and recall@5 (R@5) as evaluation metrics. For final QA performance, we use the EM score and F1 score following previous works [16]. Implementation Details. The GFM retriever is implemented with 6 query-dependent message passing layers with the hidden dimension set to 512. The pre-trained all-mpnet-v2 [57] is adopted as the sentence embedding model and is frozen during training. The total parameters of the GFM retriever are 8M, which is trained on 8 NVIDIA A100s (80G) with batch size 4, learning rate 5e-4, and loss weight α = 0.3. The training data contains 60 KGs with over 14M triples constructed from 700k documents extracted from the training set. The statistics of training data are shown in Table 5, and the implementations are detailed in Appendix D. 4.2 Retrieval Performance We first evaluate the retrieval performance ofGFM-RAG against the baselines on three multi-hop QA datasets. As shown in Table 1, GFM-RAG achieves the best performance on all datasets, outperforming the SOTA IRCoT + HippoRAG by 16.8%, 8.3%, 19.8% in R@2 on HotpotQA, MuSiQue, and 2Wiki, respectively. The results demonstrate the effectiveness of GFM-RAG in multi-hop retrieval. From the result, we can observe that the naive single-step retrievers (e.g., BM25, RAPTOR) are outperformed by graph-enhanced HippoRAG, which highlights the significance of graph structure in multi-hop retrieval. Although GraphRAG (MS)"
"gfm_rag_p07_c003","gfm_rag","gfm_rag.pdf","7","3","demonstrate the effectiveness of GFM-RAG in multi-hop retrieval. From the result, we can observe that the naive single-step retrievers (e.g., BM25, RAPTOR) are outperformed by graph-enhanced HippoRAG, which highlights the significance of graph structure in multi-hop retrieval. Although GraphRAG (MS) and LightRAG use the graph structure, it struggles with multi- hop QA tasks as its retriever is designed for summarization and lacks multi-hop reasoning capability. With the help of LLMs, the multi-step retrieval pipeline IRCoT improves the performance of all single-step methods through iterative reasoning and retrieval. However,GFM-RAG still outperforms the multi-step methods by a large margin even with a single-step retrieval. This indicates that the GFM-RAG can effectively conduct the multi-hop reasoning in a single step (detailed in Section 4.8 and Appendix E.8), which is more efficient and effective than the multi-step retrieval pipeline (detailed in Section 4.4). 4.3 Question Answering Performance We then evaluate the QA performance ofGFM-RAG, as it is directly influenced by retrieval quality. We adopt the GPT-4o-mini [47] as LLM and use the top-5 retrieved documents for generating answers. From the results shown in Table 2, the single-step GFM-RAG has already achieved state-of-the-art 7"
"gfm_rag_p08_c001","gfm_rag","gfm_rag.pdf","8","1","Table 1: Retrieval performance comparison. Category Method HotpotQA MuSiQue 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 Single-step BM25 55.4 72.2 32.3 41.2 51.8 61.9 Contriever 57.2 75.5 34.8 46.6 46.6 57.5 GTR 59.4 73.3 37.4 49.1 60.2 67.9 ColBERTv2 64.7 79.3 37.9 49.2 59.2 68.2 RAPTOR 58.1 71.2 35.7 45.3 46.3 53.8 Proposition 58.7 71.1 37.6 49.3 56.4 63.1 GraphRAG (MS) 58.3 76.6 35.4 49.3 61.6 77.3 LightRAG 38.8 54.7 24.8 34.7 45.1 59.1 HippoRAG (Contriever) 59.0 76.2 41.0 52.1 71.5 89.5 HippoRAG (ColBERTv2) 60.5 77.7 40.9 51.9 70.7 89.1 SubgraphRAG 61.5 73.0 42.1 49.3 70.7 85.5 G-retriever 53.3 65.5 38.8 45.1 60.8 67.8 Multi-step Adaptive-RAG 61.0 76.4 35.1 44.7 44.7 61.4 FLARE 73.1 81.3 44.3 55.1 67.1 73.1 IRCoT + BM25 65.6 79.0 34.2 44.7 61.2 75.6 IRCoT + Contriever 65.9 81.6 39.1 52.2 51.6 63.8 IRCoT + ColBERTv2 67.9 82.0 41.7 53.7 64.1 74.4 IRCoT + HippoRAG (Contriever) 65.8 82.3 43.9 56.6 75.3 93.4 IRCoT + HippoRAG (ColBERTv2)67.0 83.0 45.3 57.6 75.8 93.9 Single-step GFM-RAG 78.3 87.1 49.1 58.2 90.8 95.6 Table 2: Question answering performance comparison. Category Retriever HotpotQA MuSiQue 2Wiki EM F1 EM F1 EM F1 Single-step None 30.4 42.8 12.5 24.1 31.0 39.0 ColBERTv2 43.4 57.7 15.5 26.4 33.4 43.3 GraphRAG (MS) 35.3 54.6 13.4 29.5 28.3 46.9 LightRAG 36.8 48.3 18.1 27.5 45.1 49.5 HippoRAG (ColBERTv2) 41.8 55.0 19.2 29.8 46.6 59.5 Multi-step Adaptive-RAG 45.5 59.6 13.8 25.6 48.9 62.8 FLARE 48.7 60.6 16.2 28.4 46.7 65.4 IRCoT (ColBERTv2) 45.5 58.4 19.1 30.5 35.4"
"gfm_rag_p08_c002","gfm_rag","gfm_rag.pdf","8","2","29.5 28.3 46.9 LightRAG 36.8 48.3 18.1 27.5 45.1 49.5 HippoRAG (ColBERTv2) 41.8 55.0 19.2 29.8 46.6 59.5 Multi-step Adaptive-RAG 45.5 59.6 13.8 25.6 48.9 62.8 FLARE 48.7 60.6 16.2 28.4 46.7 65.4 IRCoT (ColBERTv2) 45.5 58.4 19.1 30.5 35.4 45.1 IRCoT + HippoRAG (ColBERTv2) 45.7 59.2 21.9 33.3 47.7 62.7 Single-step GFM-RAG 51.6 66.9 30.2 40.4 69.8 77.7 Multi-step IRCoT +GFM-RAG 56.0 71.8 36.6 49.2 72.5 80.8 performance against all other baselines. Meanwhile, we also integrate GFM-RAG with IRCoT to conduct multi-step retrieval and reasoning, which further improves the performance by 8.5%, 21.2%, 3.9% in EM on three datasets, respectively. The results demonstrate the effectiveness and great compatibility of GFM-RAG with an arbitrary multi-step framework in multi-hop reasoning tasks. 4.4 Efficiency Analysis GFM-RAG achieves great efficiency in performing multi-step reasoning in a single step. As shown in Table 3, while the naive single-step methods get the best efficiency whose performance is not satisfying. Admittedly, the multi-step framework IRCoT could improve the performance, but it suffers from high computational costs due to the iterative retrieval and reasoning with LLMs. In contrast, GFM-RAG conducts multi-hop reasoning within a single-step GNN reasoning, which is more effective than single-step methods and more efficient than multi-step ones. 8"
"gfm_rag_p09_c001","gfm_rag","gfm_rag.pdf","9","1","Table 3: Retrieval efficiency and performance comparison. Method HotpotQA MuSiQue 2Wiki Time (s) R@5 Time (s) R@5 Time (s) R@5 ColBERTv2 0.035 79.3 0.030 49.2 0.029 68.2 HippoRAG 0.255 77.7 0.251 51.9 0.158 89.1 LightRAG 0.861 54.7 1.109 34.7 0.911 59.1 GraphRAG (MS) 2.759 76.6 3.037 49.3 1.204 77.3 IRCoT + ColBERTv2 1.146 82.0 1.152 53.7 2.095 74.4 IRCoT + HippoRAG 3.162 83.0 3.104 57.6 3.441 93.9 GFM-RAG 0.107 87.1 0.124 58.2 0.060 95.6 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047/uni00000034/uni00000024 /uni00000027/uni00000048/uni0000004f/uni00000058/uni00000046/uni0000004c/uni00000052/uni00000051/uni00000034/uni00000024 /uni00000028/uni00000030/uni00000044/uni00000051/uni00000058/uni00000044/uni0000004f /uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057/uni00000034/uni00000024/uni00000037/uni00000048/uni00000046/uni0000004b/uni00000034/uni00000024 /uni00000030/uni00000036/uni00000003/uni00000030/uni00000044/uni00000055/uni00000046/uni00000052 /uni0000002b/uni00000024/uni0000002a/uni00000035/uni0000002c/uni00000027 /uni00000018/uni0000001b/uni00000011/uni00000018 /uni0000001a/uni00000013/uni00000011/uni0000001b /uni00000019/uni00000013/uni00000011/uni00000019 /uni00000019/uni00000015/uni00000011/uni0000001a /uni00000017/uni00000019/uni00000011/uni00000019 /uni0000001a/uni00000014/uni00000011/uni00000013 /uni0000001b/uni00000017/uni00000011/uni0000001a /uni0000002a/uni00000029/uni00000030/uni00000010/uni00000035/uni00000024/uni0000002a/uni0000002b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000035/uni00000024/uni0000002a/uni0000002f/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000035/uni00000024/uni0000002a Figure 3: Model generalizability comparison. /uni00000013/uni00000011/uni00000013/uni0000001b/uni00000030 /uni00000013/uni00000011/uni00000015/uni00000030 /uni00000013/uni00000011/uni0000001a/uni00000030 /uni00000015/uni00000030 /uni0000001b/uni00000030 /uni0000004f/uni00000052/uni0000004a/uni00000014/uni00000013/uni0000000b/uni0000005c/uni0000001d/uni00000003/uni00000006/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni0000000c /uni00000016/uni0000004e /uni00000019/uni0000004e /uni00000014/uni00000015/uni0000004e /uni00000015/uni00000017/uni0000004e /uni00000017/uni00000018/uni0000004e /uni0000004f/uni00000052/uni0000004a/uni00000014/uni00000013/uni0000000b/uni0000005b/uni0000001d/uni00000003/uni00000006/uni00000027/uni00000044/uni00000057/uni00000044/uni0000000c /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni00000018/uni00000015 /uni00000013/uni00000011/uni00000018/uni00000017 /uni00000013/uni00000011/uni00000018/uni00000019 /uni00000013/uni00000011/uni00000018/uni0000001b /uni0000005d/uni0000001d/uni00000003/uni00000030/uni00000035/uni00000035 Neural Scaling Law z = 0.24x0.05 + 0.11y0.03, R2 = 0.95 Figure 4: Neural scaling law of GFM-RAG. 4.5 Ablation Study We conduct ablation studies to investigate the effectiveness of different components inGFM-RAG, in- cluding: different sentence embedding models (Appendix E.1), pre-training strategies (Appendix E.2), loss weighting strategies (Appendix E.3), ranking methods (Appendix E.4), training datasets (Ap- pendix E.5), and the construction of KG-index (Appendix E.9). The results show that GFM-RAG is not sensitive to different sentence embedding models, and the pre-training strategy, as well as the loss weighting strategy, are both crucial for the performance of GFM-RAG. 4.6 Model Generalizability To demonstrate the generalizability of GFM-RAG as a foundation model, we test the performance (R@5) of GFM-RAG on seven RAG datasets without any domain-specific fine-tuning. Specifically, we first build the KG-index from the documents in each dataset. Then,"
"gfm_rag_p09_c002","gfm_rag","gfm_rag.pdf","9","2","Model Generalizability To demonstrate the generalizability of GFM-RAG as a foundation model, we test the performance (R@5) of GFM-RAG on seven RAG datasets without any domain-specific fine-tuning. Specifically, we first build the KG-index from the documents in each dataset. Then, given the query, we use the pre-trained GFM retriever to retrieve the top-K documents with the help of the corresponding KG- index. As shown in Figure 3, GFM-RAG achieves the best performance on all datasets, outperforming the SOTA HippoRAG by 18.9% on average. The results demonstrate the generalizability ofGFM-RAG as the foundation model which can be directly applied to various unseen datasets without any domain-specific fine-tuning. Additionally, results in Appendix E.6 demonstrate GFM-RAG’s strong transferability for further performance improvement when fine-tuned on domain-specific datasets. 4.7 Model Neural Scaling Law We further investigate the neural scaling law ofGFM-RAG, which quantifies how model performance grows with the scale of training data and model parameter size. It has been validated in the recent 9"
"gfm_rag_p10_c001","gfm_rag","gfm_rag.pdf","10","1","Table 4: Path interpretations of GFM for multi-hop reasoning, where r−1 denotes the inverse of original relation. Question Whatfootball clubwas owned by the singer of ""Grow Some Funk of Your Own""? Answer Watford Football Club Sup. Doc. [ “Grow Some Funk of Your Own”, “Elton John”] Paths 1.095: (grow some funk of your own, is a song by, elton john)→(elton john, equivalent, sir elton hercules john)→(sir elton hercules john, named a standafter−1, watford football club) 0.915: (grow some funk of your own, is a song by, elton john)→(elton john, equivalent, sir elton hercules john)→(sir elton hercules john, owned,watford football club) Question When was the judge born who made notable contributions to the trial of the man who tortured, raped, and murdered eight student nurses fromSouth Chicago Community Hospital on the night ofJuly 13-14, 1966? Answer June 4, 1931 Sup. Doc. [ “Louis B. Garippo”, “Richard Speck”] Paths 0.797: (south chicago community hospital, committed crimes at−1, richard speck)→ (richard speck, equivalent, trial of richard speck)→(trial of richard speck, made contribu- tions during−1, louis b garippo) 0.412: (south chicago community hospital, were from−1, eight student nurses)→(eight student nurses, were from, south chicago community hospital)→(south chicago community hospital, committed crimes at−1, richard speck) foundation models [29, 7]. As shown in Figure 4, the performance of GFM-RAG (MRR: z) scales well with the training data (x) and the model size (y), which can be fitted by the power-law scaling law z ∝ 0.24x0.05 + 0.11y0.03. The results demonstrate the scalability of GFM-RAG as the"
"gfm_rag_p10_c002","gfm_rag","gfm_rag.pdf","10","2","performance of GFM-RAG (MRR: z) scales well with the training data (x) and the model size (y), which can be fitted by the power-law scaling law z ∝ 0.24x0.05 + 0.11y0.03. The results demonstrate the scalability of GFM-RAG as the foundation model and potential for further improvement. The detailed analysis of the neural scaling law is shown in Appendix E.7. 4.8 Path Interpretations GFM-RAG exhibits the multi-hop reasoning ability powered by the multi-layer GFM. We provide path interpretations of GFM-RAG for multi-hop reasoning in Table 4. Inspired by NBFNet [78], the paths’ importance to the final prediction can be quantified by the partial derivative of the prediction score with respect to the triples at each layer (hop), defined as: s1, s2, . . . , sL = arg top- k ∂pe(q) ∂sl . (19) The top-k path interpretations can be obtained by the top- k longest paths with beam search. We illustrate the path interpretations in Table 4. In the first example, GFM-RAG successfully deduces that the singer of the song has a football club named after him and that he owned it. In the second example, GFM-RAG identifies two paths related to the murder case and the judge presiding over the trial. These interpretations show that GFM-RAG exhibits the ability of multi-hop reasoning within single-step retrieval. We also illustrate the distribution the multi-hop prediction in Appendix E.8. 5 Conclusion In this paper, we introduce the first graph foundation model for retrieval augmented generation. By leveraging the knowledge graph index,GFM-RAG"
"gfm_rag_p10_c003","gfm_rag","gfm_rag.pdf","10","3","ability of multi-hop reasoning within single-step retrieval. We also illustrate the distribution the multi-hop prediction in Appendix E.8. 5 Conclusion In this paper, we introduce the first graph foundation model for retrieval augmented generation. By leveraging the knowledge graph index,GFM-RAG explicitly models the complex relationships between knowledge and documents, facilitating a more effective and efficient retrieval process. Powered by a query-dependent GNN pre-trained on large-scale datasets,GFM-RAG can effectively perform multi-hop reasoning over the graph structure to find relevant knowledge in a single step. Extensive experiments across three benchmark datasets and seven domain-specific datasets demonstrate that GFM-RAG significantly outperforms state-of-the-art methods in effectiveness, efficiency, and generalizability. Its alignment with scaling laws also suggests the potential for scaling to even larger datasets. In the future, we plan to conduct larger-scale training and further explore GFM-RAG’s capabilities in other challenging scenarios such as knowledge graph completion and question answering. 10"
"gfm_rag_p11_c001","gfm_rag","gfm_rag.pdf","11","1","Acknowledgments G Haffari is partly supported by the ARC Future Fellowship FT190100039 and DARPA Assured Neuro Symbolic Learning and Reasoning (ANSR) program under award number FA8750-23-2-1016. C Gong is supported by NSF of China (Nos: 62336003, 12371510). D Phung is supported by the Australian Research Council (ARC) Discovery Project DP250100262 and DP230101176. S Pan was partly funded by Australian Research Council (ARC) under grants FT210100097 and DP240101547 and the CSIRO – National Science Foundation (US) AI Research Collaboration Program. References [1] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344–354, 2015. [2] Aijun Bai, Rolf Jagerman, Zhen Qin, Le Yan, Pratyush Kar, Bing-Rong Lin, Xuanhui Wang, Michael Bendersky, and Marc Najork. Regression compatible listwise objectives for calibrated ranking with binary relevance. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 4502–4508, 2023. [3] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250, 2008. [4] Vittorio Castelli, Rishav Chakravarti, Saswati Dana, Anthony Ferritto, Radu Florian, Martin Franz, Dinesh Garg, Dinesh Khandelwal, Scott McCarley, Michael McCawley, Mohamed Nasr, Lin Pan, Cezar Pendus, John Pitrelli, Saurabh Pujar, Salim Roukos, Andrzej Sakrajda, Avi Sil,"
"gfm_rag_p11_c002","gfm_rag","gfm_rag.pdf","11","2","pages 1247–1250, 2008. [4] Vittorio Castelli, Rishav Chakravarti, Saswati Dana, Anthony Ferritto, Radu Florian, Martin Franz, Dinesh Garg, Dinesh Khandelwal, Scott McCarley, Michael McCawley, Mohamed Nasr, Lin Pan, Cezar Pendus, John Pitrelli, Saurabh Pujar, Salim Roukos, Andrzej Sakrajda, Avi Sil, Rosario Uceda-Sosa, Todd Ward, and Rong Zhang. The TechQA dataset. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1269–1278, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.117. URL https://aclanthology.org/2020.acl-main.117/. [5] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3- embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023. [6] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. Dense X retrieval: What retrieval granularity should we use? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15159–15177, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.845. URL https://aclanthology.org/2024.emnlp-main.845/. [7] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480–7512. PMLR, 2023. [8] Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang, and Anton Tsitsulin. Don’t forget to connect! improving rag with graph-based reranking. arXiv preprint arXiv:2405.18414, 2024. [9] Darren Edge, Ha Trinh, Newman Cheng,"
"gfm_rag_p11_c003","gfm_rag","gfm_rag.pdf","11","3","on Machine Learning, pages 7480–7512. PMLR, 2023. [8] Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang, and Anton Tsitsulin. Don’t forget to connect! improving rag with graph-based reranking. arXiv preprint arXiv:2405.18414, 2024. [9] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024. [10] Robert Friel, Masha Belyi, and Atindriyo Sanyal. Ragbench: Explainable benchmark for retrieval-augmented generation systems. arXiv preprint arXiv:2407.11005, 2024. [11] Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. Towards foundation models for knowledge graph reasoning. In The Twelfth International Conference on Learning Representations, 2024. 11"
"gfm_rag_p12_c001","gfm_rag","gfm_rag.pdf","12","1","[12] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023. [13] Dan Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia-Olano. Learning dense representations for entity retrieval. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 528–537, 2019. [14] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning , pages 1263–1272. PMLR, 2017. [15] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-augmented generation. 2024. [16] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. In The Thirty- eighth Annual Conference on Neural Information Processing Systems , 2024. URL https: //openreview.net/forum?id=hkujvAPVsg. [17] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halap- panavar, Ryan A Rossi, Subhabrata Mukherjee, Xianfeng Tang, et al. Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309, 2024. [18] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. Advances in Neural Information Processing Systems, 37:132876–132907, 2024. [19] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou."
"gfm_rag_p12_c002","gfm_rag","gfm_rag.pdf","12","2","G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. Advances in Neural Information Processing Systems, 37:132876–132907, 2024. [19] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. [20] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6609–6625, 2020. [21] Xingyue Huang, Miguel Romero, Ismail Ceylan, and Pablo Barceló. A theory of link predic- tion via relational weisfeiler-leman on knowledge graphs. Advances in Neural Information Processing Systems, 36:19714–19748, 2023. [22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. [23] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C Park. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 7029–7043, 2024. [24] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992, 2023. [25] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA:"
"gfm_rag_p12_c003","gfm_rag","gfm_rag.pdf","12","3","Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992, 2023. [25] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for biomedical research question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 2567–2577, Hong Kong, China, Novem- ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259/. 12"
"gfm_rag_p13_c001","gfm_rag","gfm_rag.pdf","13","1","[26] Ashutosh Joshi, Sheikh Muhammad Sarwar, Samarth Varshney, Sreyashi Nag, Shrivats Agrawal, and Juhi Naik. Reaper: Reasoning based retrieval planning for complex rag systems. InProceed- ings of the 33rd ACM International Conference on Information and Knowledge Management, pages 4621–4628, 2024. [27] Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. Hagrid: A human- llm collaborative dataset for generative information-seeking with attribution. arXiv preprint arXiv:2307.16883, 2023. [28] Xiaoxi Kang, Lizhen Qu, Lay-Ki Soon, Zhuang Li, and Adnan Trakic. Bridging law and data: Augmenting reasoning via a semi-structured dataset with irac methodology. arXiv preprint arXiv:2406.13217, 2024. [29] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [30] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769–6781, 2020. [31] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. [32] Mufei Li, Siqi Miao, and Pan Li. Simple is effective: The roles of graphs and large lan- guage models in knowledge-graph-based retrieval-augmented generation. In The Thirteenth International Conference on Learning Representations, 2025. [33] Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and"
"gfm_rag_p13_c002","gfm_rag","gfm_rag.pdf","13","2","is effective: The roles of graphs and large lan- guage models in knowledge-graph-based retrieval-augmented generation. In The Thirteenth International Conference on Learning Representations, 2025. [33] Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and Bing Yin. Graph reasoning for question answering with triplet retrieval. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3366–3375, 2023. [34] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. [35] Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, et al. Kag: Boosting llms in professional domains via knowledge augmented generation. arXiv preprint arXiv:2409.13731, 2024. [36] Zhutian Lin, Junwei Pan, Shangyu Zhang, Ximei Wang, Xi Xiao, Shudong Huang, Lei Xiao, and Jie Jiang. Understanding the ranking loss for recommendation with sparse user feedback. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5409–5418, 2024. [37] Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S Yu, et al. Graph foundation models: Concepts, opportunities and challenges. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [38] LINHAO LUO, Yuan-Fang Li, Reza Haf, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. In The Twelfth International Conference on Learning Representations, 2024. [39] Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark"
"gfm_rag_p13_c003","gfm_rag","gfm_rag.pdf","13","3","2025. [38] LINHAO LUO, Yuan-Fang Li, Reza Haf, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. In The Twelfth International Conference on Learning Representations, 2024. [39] Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. Expertqa: Expert-curated questions and attributed answers. arXiv preprint arXiv:2309.07852, 2023. [40] Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Mikhail Galkin, and Jiliang Tang. Position: Graph foundation models are already here. In Forty-first International Conference on Machine Learning, 2024. 13"
"gfm_rag_p14_c001","gfm_rag","gfm_rag.pdf","14","1","[41] Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning. arXiv preprint arXiv:2405.20139, 2024. [42] Meta. Build the future of ai with meta llama 3, 2024. URL https://llama.meta.com/ llama3/. [43] Gabriel de Souza P Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. Nv-retriever: Improving text embedding models with effective hard-negative mining. arXiv preprint arXiv:2407.15831, 2024. [44] Abhilash Nandy, Soumya Sharma, Shubham Maddhashiya, Kapil Sachdeva, Pawan Goyal, and NIloy Ganguly. Question answering over electronic devices: A new benchmark dataset and a multi-task learning based QA framework. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4600–4609, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.392. URL https://aclanthology.org/2021.findings-emnlp.392/. [45] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human generated machine reading comprehension dataset. November 2016. [46] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9844–9855, 2022. [47] OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. [48] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/ . [49] Liu Pai, Wenyang Gao, Wenjie Dong, Lin Ai, Ziwei Gong, Songfang Huang, Li Zongsheng, Ehsan Hoque, Julia Hirschberg, and Yue Zhang. A survey on open information extraction"
"gfm_rag_p14_c002","gfm_rag","gfm_rag.pdf","14","2","[48] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/ . [49] Liu Pai, Wenyang Gao, Wenjie Dong, Lin Ai, Ziwei Gong, Songfang Huang, Li Zongsheng, Ehsan Hoque, Julia Hirschberg, and Yue Zhang. A survey on open information extraction from rule-based model to large language model. Findings of the Association for Computational Linguistics: EMNLP 2024, pages 9586–9608, 2024. [50] Pranoy Panda, Ankush Agarwal, Chaitanya Devaguptapu, Manohar Kaul, and Prathosh Ap. Holmes: Hyper-relational knowledge graphs for multi-hop question answering using llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13263–13282, 2024. [51] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. Graph retrieval-augmented generation: A survey. arXiv preprint arXiv:2408.08921, 2024. [52] Haiquan Qiu, Yongqi Zhang, Yong Li, et al. Understanding expressivity of gnn in rule learning. In The Twelfth International Conference on Learning Representations, 2024. [53] Stephen E Robertson and Steve Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR’94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University, pages 232–241. Springer, 1994. [54] Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh Menon, Md Parvez, and Zhe Feng. DelucionQA: Detecting hallucinations in domain- specific question answering. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Find- ings of the Association for Computational Linguistics: EMNLP 2023 , pages 822–835, Sin- gapore,"
"gfm_rag_p14_c003","gfm_rag","gfm_rag.pdf","14","3","Wang, Rakesh Menon, Md Parvez, and Zhe Feng. DelucionQA: Detecting hallucinations in domain- specific question answering. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Find- ings of the Association for Computational Linguistics: EMNLP 2023 , pages 822–835, Sin- gapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-emnlp.59. URL https://aclanthology.org/2023.findings-emnlp.59/. 14"
"gfm_rag_p15_c001","gfm_rag","gfm_rag.pdf","15","1","[55] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3715–3734, 2022. [56] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, 2024. [57] SBERT. Sentence-transformers all-mpnet-base-v2, 2021. URL https://huggingface.co/ sentence-transformers/all-mpnet-base-v2 . [58] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. DRAGIN: Dynamic retrieval augmented generation based on the real-time information needs of large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12991–13013, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.702. URL https://aclanthology.org/2024. acl-long.702/. [59] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. In The Twelfth International Conference on Learning Representations, 2024. [60] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 491–500, 2024. [61] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin,"
"gfm_rag_p15_c002","gfm_rag","gfm_rag.pdf","15","2","Graphgpt: Graph instruction tuning for large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 491–500, 2024. [61] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, and Chao Huang. Higpt: Heterogeneous graph language model. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2842–2853, 2024. [62] Timothy J Teyler and Pascal DiScenna. The hippocampal memory indexing theory. Behavioral neuroscience, 100(2):147, 1986. [63] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554, 2022. [64] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10014–10037, 2023. [65] Denny Vrandeˇci´c and Markus Krötzsch. Wikidata: a free collaborative knowledgebase. Com- munications of the ACM, 57(10):78–85, 2014. [66] Zehong Wang, Zheyuan Zhang, Nitesh V Chawla, Chuxu Zhang, and Yanfang Ye. GFT: Graph foundation model with transferable tree vocabulary. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id= 0MXzbAv8xy. [67] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4–24, 2020. [68] Lianghao Xia, Ben Kao, and Chao Huang. Opengraph: Towards open graph foundation models. arXiv preprint arXiv:2403.01121, 2024. [69]"
"gfm_rag_p15_c003","gfm_rag","gfm_rag.pdf","15","3","and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4–24, 2020. [68] Lianghao Xia, Ben Kao, and Chao Huang. Opengraph: Towards open graph foundation models. arXiv preprint arXiv:2403.01121, 2024. [69] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023. 15"
"gfm_rag_p16_c001","gfm_rag","gfm_rag.pdf","16","1","[70] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [71] Bishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of the International Conference on Learning Representations (ICLR) 2015, 2015. [72] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, 2018. [73] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. Qa-gnn: Reasoning with language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 535–546,"
"gfm_rag_p16_c002","gfm_rag","gfm_rag.pdf","16","2","Antoine Bosselut, Percy Liang, and Jure Leskovec. Qa-gnn: Reasoning with language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 535–546, 2021. [74] Alexandros Zeakis, George Papadakis, Dimitrios Skoutas, and Manolis Koubarakis. Pre- trained embeddings for entity resolution: an experimental analysis. Proceedings of the VLDB Endowment, 16(9):2225–2238, 2023. [75] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Making a miracl: Multilingual information retrieval across a continuum of languages. arXiv preprint arXiv:2210.09984, 2022. [76] Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys, 56(4):1–62, 2023. [77] Shaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long, Jingyang Li, and Jian Sun. A survey on neural open information extraction: Current status and future directions. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI- 22, pages 5694–5701. International Joint Conferences on Artificial Intelligence Organization, 7 2022. doi: 10.24963/ijcai.2022/793. URL https://doi.org/10.24963/ijcai.2022/793. Survey Track. [78] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. Advances in Neural Information Processing Systems, 34:29476–29490, 2021. Appendix Table of Contents A Query-dependent GNNs for Multi-hop Reasoning and Retrieval 17 B Datasets 17 B.1 Multi-hop QA Datasets . . . . . . . . . . . . ."
"gfm_rag_p16_c003","gfm_rag","gfm_rag.pdf","16","3","in Neural Information Processing Systems, 34:29476–29490, 2021. Appendix Table of Contents A Query-dependent GNNs for Multi-hop Reasoning and Retrieval 17 B Datasets 17 B.1 Multi-hop QA Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 16"
"gfm_rag_p17_c001","gfm_rag","gfm_rag.pdf","17","1","B.2 Domain-specific RAG Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C Baselines 19 D Implementations and Training Details 20 D.1 Training Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 D.2 Model Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 D.3 Training Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 E Additional Experiments 21 E.1 Effectiveness of Different Sentence Embeddings . . . . . . . . . . . . . . . . . 21 E.2 Effectiveness of Different Training Strategies . . . . . . . . . . . . . . . . . . . 22 E.3 Effectiveness of Loss Weights . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.4 Effectiveness of Ranking Methods . . . . . . . ."
"gfm_rag_p17_c002","gfm_rag","gfm_rag.pdf","17","2",". . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.4 Effectiveness of Ranking Methods . . . . . . . . . . . . . . . . . . . . . . . . 23 E.5 Ablation Study of Training Datasets . . . . . . . . . . . . . . . . . . . . . . . . 23 E.6 Model Transferability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.7 Details of Model Neural Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . 24 E.8 Visualization of the Distribution of Multi-hop Prediction . . . . . . . . . . . . . 25 E.9 Cost and Impact of LLMs on KG-index Construction . . . . . . . . . . . . . . . 26 F Prompts 27 G Limitations 27 A Query-dependent GNNs for Multi-hop Reasoning and Retrieval We provide a detailed explanation about why query-dependent GNNs can be used for multi-hop reasoning and retrieval. Recent studies [21, 52] have theoretically proven that query-dependent GNNs are effective for capturing the multi-hop"
"gfm_rag_p17_c003","gfm_rag","gfm_rag.pdf","17","3","Query-dependent GNNs for Multi-hop Reasoning and Retrieval We provide a detailed explanation about why query-dependent GNNs can be used for multi-hop reasoning and retrieval. Recent studies [21, 52] have theoretically proven that query-dependent GNNs are effective for capturing the multi-hop logical associations on KGs to answer queries, such as: ∃y : politician_of(Barack Obama, y) ← work_in(Barack Obama, z1)∧city_of(z1, y), (20) where the right part denotes the logical associations can be executed to answer the query on the left, i.e., “politician_of(Barack Obama,y)”. This query is semantic equivalent to the nature language question: “ Barack Obama is the politician of which country? ”. By treating the input question as a “soft query” (query in nature language), we apply the query-dependent GNN (GFM) to bridge the gap between nature language and logical query. The GFM tries to understand the semantic of the questions and learn to conduct complex logical reasoning (e.g., multi-hop reasoning) on KGs for retrieval [73]. The learned logical associations for reasoning are shown in Section 4.8. Table 5: Statistics of the query-doc pairs and KGs used for training. Dataset #Q-doc Pair #Document #KG #Entity #Relation #Triple HotpotQA 20,000 204,822 20 1,930,362 967,218 6,393,342 MuSiQue 20,000 410,380 20 1,544,966 900,338 4,848,715 2Wiki 20,000 122,108 20 916,907 372,554 2,883,006 Total 60,000 737,310 60 4,392,235 2,240,110 14,125,063 B Datasets B.1 Multi-hop QA Datasets Three multi-hop QA datasets are used in our experiments: HotpotQA [ 72], MuSiQue [ 63], and 2WikiMultiHopQA (2Wiki) [20]. We provide a brief overview of these datasets below. 17"
"gfm_rag_p18_c001","gfm_rag","gfm_rag.pdf","18","1","Table 6: Statistics of the datasets and constructed KG-indexes used for testing. Dataset Domain #Test #Document #Entity #Relation #Triple HotpotQA Multi-hop 1,000 9,221 87,768 45,112 279,112 MuSiQue Multi-hop 1,000 11,656 100,853 55,944 319,618 2Wiki Multi-hop 1,000 6,119 48,779 20,748 160,950 PubMedQA Biomedical 2,450 5,932 42,389 20,952 149,782 DelucionQA Customer Support 184 235 2,669 2,298 6,183 TechQA Customer Support 314 769 10,221 4,606 57,613 ExpertQA Customer Support 203 808 11,079 6,810 16,541 EManual Customer Support 132 102 695 586 1,329 MS Marco General Knowledge 423 3,481 24,740 17,042 63,995 HAGRID General Knowledge 1,318 1,975 23,484 18,653 48,969 • HotpotQA [72] is a multi-hop QA dataset that requires reasoning over multiple documents to answer questions. The dataset consists of 97k question-answer pairs, where each question is associated with up to 2 supporting and several distracting documents. The questions are designed to be answerable using multiple pieces of information from the supporting documents. • MuSiQue [63] is a challenging multi-hop QA dataset with 25k 2-4 hop questions. It requires coherent multi-step reasoning to answer questions that span multiple documents. • 2WikiMultiHopQA (2Wiki) [20] is a multi-hop QA dataset that requires reasoning over multiple Wikipedia articles to answer questions. The dataset consists of 192k questions, which are designed to be answerable using information from 2 or 4 articles. In experiments, we adhere to the official data split to obtain the training samples and follow existing methods [64, 16] to use the same 1,000 samples from each validation set to avoid data leakage. We merge"
"gfm_rag_p18_c002","gfm_rag","gfm_rag.pdf","18","2","2 or 4 articles. In experiments, we adhere to the official data split to obtain the training samples and follow existing methods [64, 16] to use the same 1,000 samples from each validation set to avoid data leakage. We merge the candidate passages as the document corpus for KG-index construction. The statistics of the training and test data are presented in Table 5 and Table 6, respectively. B.2 Domain-specific RAG Datasets To test the generalizability of GFM-RAG, we evaluate it on seven domain-specific RAG datasets [10] including, (1) biomedical: PubMedQA [25]; (2) customer support: DelucionQA [54], TechQA [4], ExpertQA [39], EManual [44]; (3) general knowledge: MS Marco [45], HAGRID [27]. We provide a brief overview of these datasets below. • PubMedQA [25] is a collection of PubMed research abstracts with corresponding questions paired with 4 abstract chunks. • DelucionQA [54] is a domain-specific RAG dataset leveraging Jeep’s 2023 Gladiator model manual as the source of knowledge, where each question is associated with 4 context documents and only 1 relevant passage. • TechQA [4] is a collection of real-world user questions posted on IBMDeveloper and Devel- operWorks forums, along with 10 technical support documents relating to each question. • ExpertQA [39] is a collection of curated questions from domain experts in various fields of science, arts, and law. The dataset also contains expert-curated passages relevant to each question. • EManual [ 44] is a question-answering dataset comprising consumer electronic device manuals and realistic questions about them composed by human annotators, where"
"gfm_rag_p18_c003","gfm_rag","gfm_rag.pdf","18","3","various fields of science, arts, and law. The dataset also contains expert-curated passages relevant to each question. • EManual [ 44] is a question-answering dataset comprising consumer electronic device manuals and realistic questions about them composed by human annotators, where each question is related with up to 3 context documents. • MS Marco [45] is an open-domain question-answering dataset sourced from Bing search engine user query logs. Each question is associated with 10 context passages retrieved via Bing web search. • HAGRID [27] is a multi-lingual information retrieval dataset with questions and passages from MIRACL [75]. 18"
"gfm_rag_p19_c001","gfm_rag","gfm_rag.pdf","19","1","In experiments, we use test sets constructed by RAGBench [10] and merge all the candidate passages as document corpus for KG-index construction. The statistics of the test dataset are detailed in Table 6. C Baselines In experiments, we compare with several widely used retrieval methods under three categories: (1) single-step naive methods: BM25 [53], Contriever [22], GTR [46], ColBERTv2 [55], RAPTOR [56], Proposition [6]; (2) graph-enhanced methods: GraphRAG (MS) [9], LightRAG [15], HippoRAG [16]; (3) multi-step methods: Adaptive-RAG [ 23], FLARE [ 24], and IRCoT [ 64]. The detailed introduction of the baselines is as follows. Single-step Naive Methods are widely adopted in real-world applications due to their great efficiency and generalizability. • BM25 [53] is a classic information retrieval method based on the probabilistic model that ranks a set of documents based on the query terms frequency appearing in each document. • Contriever [22] trains a dense retriever with contrastive learning on a large-scale corpus to retrieve relevant documents for a given query. • GTR [46] develops a scale-up T5-based dense retriever that could generalize across different datasets and domains. • ColBERTv2 [55] is a state-of-the-art dense retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the retrieval quality. • RAPTOR [56] is an LLM-augmented retriever that recursively embeds, clusters, and sum- marizes chunks of text, constructing a tree with differing levels of summarization to enable accurate retrieval. • Proposition [6] enhances the performance of dense retrievers by leveraging LLMs to generate"
"gfm_rag_p19_c002","gfm_rag","gfm_rag.pdf","19","2","is an LLM-augmented retriever that recursively embeds, clusters, and sum- marizes chunks of text, constructing a tree with differing levels of summarization to enable accurate retrieval. • Proposition [6] enhances the performance of dense retrievers by leveraging LLMs to generate a natural language proposition that captures the essential information of the document. Graph-enhanced Methods design a retriever that is built upon a graph structure to conduct effective retrieval and reasoning. • GraphRAG (MS) [9] is a graph-enhanced retrieval method originally proposed by Microsoft. It builds a graph structure from the document corpus and use hierarchical community detection to cluster the documents into communities and generate a summary for each community. The summary together with the original documents are retrieved by the retriever for LLM generation. • LightRAG [15] is an innovative graph-enhanced RAG method that incorporates graph structures into text indexing and retrieval, enabling efficient retrieval of entities and their relationships. It employs a dual-level retrieval system to gather both low-level and high-level knowledge for LLM generation. • HippoRAG [16] is a state-of-the-art, training-free graph-enhanced retriever that uses the Personalized PageRank algorithm to assess entity relevance to a query and performs multi- hop retrieval on a document-based knowledge graph. It can be directly applied to various datasets. Multi-step Methods are designed to conduct multi-hop reasoning by iteratively retrieving and reasoning over documents, which can be integrated with arbitrary retrieval methods. • Adaptive-RAG [23] proposes an adaptive multi-step retrieval method that can dynamically select the most suitable retrieval strategy based on"
"gfm_rag_p19_c003","gfm_rag","gfm_rag.pdf","19","3","designed to conduct multi-hop reasoning by iteratively retrieving and reasoning over documents, which can be integrated with arbitrary retrieval methods. • Adaptive-RAG [23] proposes an adaptive multi-step retrieval method that can dynamically select the most suitable retrieval strategy based on the complexity of the query. • FLARE [24] introduces a multi-step retrieval method that actively decide when and how to retrieve documents. It also predicts the future content to the guide the retrieval in next steps. • IRCoT [64] is a powerful multi-step retrieval pipeline that integrates the retrieval with the chain-of-thought (CoT) reasoning of LLMs. It guides the retrieval with CoT and in turn using retrieved documents to improve CoT. IRCoT can be compatible with arbitrary retrievers to conduct multi-step retrieval and reasoning. 19"
"gfm_rag_p20_c001","gfm_rag","gfm_rag.pdf","20","1","Table 7: The detailed implementation and training settings of GFM-RAG. Setting GFM-RAG KG-index Construction OpenIE GPT-4o-mini Entity resolution ColBERTv2 τ 0.8 GFM Model # Layer 6 Hidden dim 512 Message DistMult Aggregation Sum gl(·) 2-layer MLP Sentence embedding model all-mpnet-v2 Doc. ranker entities T 20 KGC Pre-training α 1 Optimizer AdamW Learning rate 5e-4 Batch size 4 Training steps 30,000 # Negative sample 128 Supervised Retrieval Fine-tuning α 0.3 Optimizer AdamW Learning rate 5e-4 Batch size 4 Training epochs 5 # Negative sample E \\ A q D Implementations and Training Details D.1 Training Data Construction We extract 60,000 samples from the training set of HotpotQA, MuSiQu, and 2Wiki to construct KG-indexes and conduct large-scale training. Specifically, we merge the candidate passages as the document corpus. In the KG-index construction, we use the GPT-4o-mini [ 47] with the OpenIE prompts described in HippoRAG [16] to extract the entities, relations, and triples from the document corpus. Then, we use the ColBERTv2 [55] to conduct the entity resolution by computing the similarity between entities as s(ei, ej) = Emb.(ei)⊤Emb.(ej), (21) where a new triple (ei, equivalent, ej) is generated if s(ei, ej) > τ and ei ̸= ej. We set the threshold τ as 0.8 in our experiments. We divide the samples into groups of approximately 1k questions and 10k documents each to control the constructed KG-index size. In the end, we obtain 60 different KG-indexes and associated question-document pairs for model training. D.2 Model Settings In GFM-RAG, the GFM is implemented"
"gfm_rag_p20_c002","gfm_rag","gfm_rag.pdf","20","2","into groups of approximately 1k questions and 10k documents each to control the constructed KG-index size. In the end, we obtain 60 different KG-indexes and associated question-document pairs for model training. D.2 Model Settings In GFM-RAG, the GFM is implemented as a 6-layer query-dependent GNN with the hidden dimension of 512, DistMult message function, and sum aggregation. The relation update function gl(·) is implemented as a 2-layer MLP. We use the all-mpnet-v2 as the sentence embedding model with a dimension of 768. The total training parameters of the GFM is 8M. In the retrieval stage, we select top T = 20 entities for the document ranker. D.3 Training Settings In KG completion pre-training, we randomly sample triples (e, r, t) from knowledge graphs and mask out either the head or the tail entity to create a synthetic queryq = (e, r, ?) and answer a = {e} in a self-supervised manner. For example, given a triple (Barack Obama , born_in, Honolulu), we 20"
"gfm_rag_p21_c001","gfm_rag","gfm_rag.pdf","21","1","Table 8: Comparison of different sentence embedding models used in GFM-RAG. Sentence Embedding Model HotpotQA MuSique 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 sentence-transformers/all-mpnet-base-v2 70.2 82.1 46.0 55.1 81.1 85.6 BAAI/bge-large-en 68.1 81.1 45.9 55.9 80.7 86.3 Alibaba-NLP/gte-Qwen2-1.5B-instruct 69.9 81.5 46.0 55.0 79.8 86.2 Alibaba-NLP/gte-Qwen2-7B-instruct 68.5 81.5 45.5 55.1 80.8 85.6 nvidia/NV-Embed-v2 69.2 81.4 46.3 54.9 80.3 85.5 Table 9: Comparison of GFM-RAG with pre-trained and fine-tuned sentence embedding models. Method HotpotQA MuSiQue 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 GFM-RAG 78.3 87.1 49.1 58.2 90.8 95.6 all-mpnet-v2 (pre-trained) 59.4 73.3 33.2 46.3 48.5 59.4 all-mpnet-v2 (finetuned) 67.0 82.3 41.7 55.0 65.1 76.7 can create a query as (Barack Obama , born_in, ?), which is encoded as a sentence embedding and fed into the GFM to predict the target entity Honolulu on graphs. In supervised document retrieval fine-tuning, we obtain natural language questions and supporting documents from the multi-hop QA datasets. For each question, we identify the entities from its supporting documents as the targets. For instance, given the question “Where was Barack Obama born in?”, we can extract two entities such as [Honolulu, USA] from its supporting documents (e.g., Doc. 2 in Figure 2). The GFM is trained to maximize the likelihood of these two target entities. In the self-supervised KG completion pre-training, the GFM is trained on the mixture of 60 constructed KG-indexes for 30,000 steps. Then, we conduct the supervised document retrieval fine-tuning on the labeled question-document pairs for 5 epochs. The weight α between losses is"
"gfm_rag_p21_c002","gfm_rag","gfm_rag.pdf","21","2","self-supervised KG completion pre-training, the GFM is trained on the mixture of 60 constructed KG-indexes for 30,000 steps. Then, we conduct the supervised document retrieval fine-tuning on the labeled question-document pairs for 5 epochs. The weight α between losses is set to 0.3. We use AdamW optimizer, learning rate of 5e-4 with batch sizes of both training stages set to 4. Each batch contains only one KG-index and training samples associated to it, where we randomly sample from different KG-indexes during training. The model is trained on 8 NVIDIA A100s (80G) with 14 hours pre-training and 5 hours supervised fine-tuning. The detailed settings are summarized in Table 7. E Additional Experiments E.1 Effectiveness of Different Sentence Embeddings In this section, we first study the effectiveness of different sentence embeddings in the GFM. We compare the all-mpnet-v2 [ 57], bge-large-en [ 69], gte-Qwen2-1.5B-instruct and gte-Qwen2-7B- instruct [34] as well as NV-Embed-v2 [31]. We download the official pre-trained model from the Huggingface3. The details of the models are shown in Table 8. From the results, we can observe that the performance variance between different sentence embeddings is relatively small, where the all- mpnet-v2 achieves the best performance with respect to 3 metrics. This indicates that GFM-RAG is not sensitive to the choice of sentence embedding models. In experiments, we use the all-mpnet-v2 as the default sentence embedding model due to its efficiency. However, it has relative smaller context-size (512) which limits the length of input text. We leave the exploration of larger"
"gfm_rag_p21_c003","gfm_rag","gfm_rag.pdf","21","3","of sentence embedding models. In experiments, we use the all-mpnet-v2 as the default sentence embedding model due to its efficiency. However, it has relative smaller context-size (512) which limits the length of input text. We leave the exploration of larger context-size sentence embedding models (e.g., NV-Embed-v2 with 32k context) for future work. Then, we expand our ablation study to compareGFM-RAG with variants without GNN and using solely the pre-trained all-mpnet-v2 embeddings and those fine-tuned on multi-hop QA data, respectively. The results are shown in Table 9. We can observe that GNN plays a crucial role in retrieval. The sentence embedding model all-mpnet-v2 is pre-trained on large-scale text data and could potentially see the QA data. However, it is not specifically trained for the multi-hop QA task, which leads to 3https://huggingface.co/ 21"
"gfm_rag_p22_c001","gfm_rag","gfm_rag.pdf","22","1","Table 10: Effectiveness of KGC pre-training and supervised retrieval fine-tuning inGFM-RAG. Method HotpotQA MuSique 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 GFM-RAG 78.3 87.1 49.1 58.2 89.1 92.8 GFM-RAG w/o Retrieval Fine-tune 21.0 32.8 18.3 25.9 44.6 53.4 GFM-RAG w/o KGC Pre-train 77.8 86.5 48.3 58.3 88.3 92.5 Table 11: Knowledge graph completion result of different training strategies. Method MRR Hits@1 Hits@3 Hits@10 GFM-RAG 0.193 0.138 0.221 0.293 GFM-RAG w/o Retrieval Fine-tune 0.304 0.234 0.323 0.451 GFM-RAG w/o KGC Pre-train 0.029 0.007 0.022 0.067 suboptimal performance in capturing the relationship between question and supporting documents. The fine-tuned all-mpnet-v2 achieves better performance than the pre-trained one by supervised fine-tuning on the multi-hop QA data, but still inferior to GFM-RAG. This indicates that the GNN can effectively capture the relationship between knowledge and conduct multi-hop reasoning, which is not achievable by simply using the sentence embedding model. E.2 Effectiveness of Different Training Strategies In this section, we first study the effectiveness of the two training tasks used inGFM-RAG. We compare the performance by only conducting the KG completion pre-training ( GFM-RAG w/o Fine-tune) and supervised document retrieval fine-tuning (GFM-RAG w/o Pre-train). The results are shown in Table 10. The results show that removing the supervised document retrieval fine-tuning significantly decreases the performance of GFM-RAG. This highlights the importance of supervised fine-tuning, as it enables the model to understand users’ queries and better capture the relevance between questions and knowledge for improved retrieval. Although the pre-training has a relatively small impact on the"
"gfm_rag_p22_c002","gfm_rag","gfm_rag.pdf","22","2","of GFM-RAG. This highlights the importance of supervised fine-tuning, as it enables the model to understand users’ queries and better capture the relevance between questions and knowledge for improved retrieval. Although the pre-training has a relatively small impact on the final performance, its primary purpose is to learn the general graph reasoning ability, following previous studies like ULTRA [11]. This would enhance the generalization and robustness of the GFM, which could be beneficial to its performance on other tasks, such as knowledge graph completion. To further validate this, we conduct an ablation study to compare GFM-RAG with different training strategies on the knowledge graph completion task. We report the knowledge graph completion (KGC) performance on the KG-index from the test set of the HotpotQA dataset. The results are shown in Table 11. From the knowledge graph completion results, we can observe that the GFM-RAG undergoes only the pre-training (GFM-RAG w/o Fine-tune) achieves the best performance, which indicates that the pre- training is effective in learning the general graph reasoning ability. The performance ofGFM-RAG with only supervised fine-tuning (GFM-RAG w/o Pre-train) is significantly lower than that ofGFM-RAG with pre-training. This indicates that the supervised fine-tuning is only learning the specific downstream task, which would limit the generalization ability of GFM-RAG as the foundation model. The GFM trained with both pre-training and supervised fine-tuning achieves the second-best performance on the knowledge graph completion task and the best performance on the multi-hop QA task. This indicates that both training strategies are essential for"
"gfm_rag_p22_c003","gfm_rag","gfm_rag.pdf","22","3","the foundation model. The GFM trained with both pre-training and supervised fine-tuning achieves the second-best performance on the knowledge graph completion task and the best performance on the multi-hop QA task. This indicates that both training strategies are essential for GFM-RAG to learn the general graph reasoning ability and benefit specific downstream tasks. E.3 Effectiveness of Loss Weights In this section, we examine the effectiveness of the weights assigned to the BCE loss and ranking loss in training GFM-RAG. We compare performance by varying the weight α between the two losses: L = αLBCE + (1 − α)LRANK, with results presented in Table 12. The findings indicate that using only either the BCE loss or ranking loss leads to suboptimal performance ( α = 0 or 1). The best 22"
"gfm_rag_p23_c001","gfm_rag","gfm_rag.pdf","23","1","Table 12: Effectiveness (MRR) for the weight α of two losses. α HotpotQA MuSique 2Wiki 0 0.5189 0.3252 0.4425 1 0.5096 0.3214 0.4282 0.7 0.5202 0.3249 0.4348 0.3 0.5243 0.3260 0.4490 performance occurs when α is set to 0.3, which aligns with previous studies [36] suggesting that a smaller weight for BCE loss is preferable when positive samples are rare in the training data. E.4 Effectiveness of Ranking Methods In this section, we investigate the effectiveness of different ranking methods based on inverted index used in GFM-RAG. We compare four ranking methods including (1) IDF + Top-T Pred: Our proposed method (eqs. (14) to (16)), which maps the top-T entities predicted by GFM to documents using inverse document frequency (IDF)-weighted scores. (2) IDF + All Pred: Uses all predicted entities from GFM and weights them by IDF (w/o eq. (14)). (3) Top-T Pred: Uses only the top-T predicted entities without applying IDF weighting (w/o eq. (15)). (4) All Pred: Use all entity predictions and directly map to document scores (w/o eqs. (14) and (15)). The results are shown in Table 13. The results show that the proposed IDF + Top-k Pred performs the best. This indicates that the inverted index is a crucial component of GFM-RAG, which serves as a bridge between structured reasoning over KGs and the unstructured documents required by LLMs, necessitating a careful design. We acknowledge the potential alternatives, and as a promising future direction, we plan to explore end-to-end models that can jointly reason over structured"
"gfm_rag_p23_c002","gfm_rag","gfm_rag.pdf","23","2","bridge between structured reasoning over KGs and the unstructured documents required by LLMs, necessitating a careful design. We acknowledge the potential alternatives, and as a promising future direction, we plan to explore end-to-end models that can jointly reason over structured and unstructured knowledge without relying on an explicit inverted index. Table 13: Comparison of different ranking methods. Ranking Method HotpotQA MuSiQue 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 IDF + Top-T Pred (GFM-RAG) 78.3 87.1 49.1 58.2 90.8 95.6 IDF + All Pred (w/o eq. (14)) 68.1 71.4 35.8 41.2 86.0 87.5 Top-T Pred (w/o eq. (15)) 71.6 78.6 46.3 52.5 74.7 78.1 All Pred (w/o eqs. (14) and (15)) 77.6 82.9 41.1 46.9 88.6 90.4 E.5 Ablation Study of Training Datasets We further conducted ablation studies where GFM-RAG is trained separately on each dataset, and we report performance across all three benchmarks. Results are shown Table 14. These results show that GFM-RAG not only performs well on the trained datasets, but also generalizes well to other datasets. More importantly, the model trained on multi-domain datasets performs competitively across all datasets, validating its ability to generalize effectively across domains and benefit from training on diverse KGs by learning generalizable reasoning ability across domains. E.6 Model Transferability In this section, we evaluate GFM-RAG’s transferability by conducting domain-specific fine-tuning on the training split of dataset on each domain. As shown in 15, GFM-RAG performs well in zero- shot generalization, with further improvements achieved through domain-specific fine-tuning. This highlights its transferability when adapted"
"gfm_rag_p23_c003","gfm_rag","gfm_rag.pdf","23","3","evaluate GFM-RAG’s transferability by conducting domain-specific fine-tuning on the training split of dataset on each domain. As shown in 15, GFM-RAG performs well in zero- shot generalization, with further improvements achieved through domain-specific fine-tuning. This highlights its transferability when adapted to domain-specific datasets. 23"
"gfm_rag_p24_c001","gfm_rag","gfm_rag.pdf","24","1","Table 14: Ablation study of GFM-RAG trained on each dataset. Best results are highlighted in bold. The second best is underlined. Test Dataset HotpotQA MuSiQue 2Wiki Training Dataset R@2 R@5 R@2 R@5 R@2 R@5 HotpotQA 79.3 87.8 46.9 57.2 86.6 92.4 MusiQue 68.8 81.8 47.6 57.5 84.4 89.6 2Wiki 72.2 77.9 46.6 55.5 89.3 93.2 All 78.3 87.1 49.1 58.2 90.8 95.6 Table 15: Model performance (R@5) and transferability comparsion. Model DelucionQA EManual ExpertQA TechQA MS Marco HAGRID HippoRAG (zero-shot) 59.0 50.0 55.1 39.5 51.1 75.5 LightRAG (zero-shot) 46.1 46.2 59.4 36.8 48.3 75.9 GFM-RAG(zero-shot) 70.8 60.6 62.7 46.6 71.0 84.7 GFM-RAG(domain-specific fine-tuning) 82.7 75.9 60.8 49.5 77.5 86.6 E.7 Details of Model Neural Scaling In this section, we provide more details on the neural scaling experiments. We evaluate the changes of the model performance with respect to different parameter sizes and training data sizes. In GFM-RAG, the model parameter sizes are primarily influenced by the hidden dimension of the GFM. Thus, we vary the dimension from 32 to 512 which results in the model parameter sizes ranging from 0.08M to 8M. The detailed settings are shown in Table 16. We test models with different sizes on different scales of training data ranging from 3k to 45k samples. We separately report the fitted trend line of performance changing with model parameter size and training data size in Figure 5. From the trend line, we can observe that the performance of GFM-RAG increases with the model parameter size and training data"
"gfm_rag_p24_c002","gfm_rag","gfm_rag.pdf","24","2","report the fitted trend line of performance changing with model parameter size and training data size in Figure 5. From the trend line, we can observe that the performance of GFM-RAG increases with the model parameter size and training data size. Meanwhile, with the larger model parameter size a larger training data size is required to achieve the best performance. This indicates that the performance of GFM-RAG can be further improved by scaling up the model size and training data simultaneously. To further investigate architectural design, we varied the number of GNN layers from 1 to 8 while keeping the hidden dimension fixed (512), and evaluated model performance across all datasets. The results are shown in Table 17. We observe that performance generally improves with deeper GNN layers, which we attribute to both the increased model sizes and the ability to capture more complex multi-hop associations. This trend aligns with the neural scaling laws observed in foundation models, where larger parameter counts typically yield better generalization. Interestingly, we find that performance peaks around 4 layers in some cases. As discussed in Appendix A and Section 4.8, GFM-RAG is designed to capture logical associations from KGs through multi-hop message passing. However, since the maximum number of reasoning hops required by our datasets is 4, additional layers beyond this offer limited benefit, likely due to the absence of higher-hop training signals. This finding supports our hypothesis that GFM-RAG effectively learns query-relevant multi-hop reasoning paths, and that deeper architectures may not improve performance"
"gfm_rag_p24_c003","gfm_rag","gfm_rag.pdf","24","3","datasets is 4, additional layers beyond this offer limited benefit, likely due to the absence of higher-hop training signals. This finding supports our hypothesis that GFM-RAG effectively learns query-relevant multi-hop reasoning paths, and that deeper architectures may not improve performance without datasets requiring more complex reasoning. In summary, these results demonstrate the effectiveness and interpretability of the proposed GNN-based architecture, and confirm that both model capacity Table 16: The hidden dimension with corresponding model size and training batch size for scaling law analysis. Hidden Dim. Parameter Size Batch size (A100, 80G) 32 78,977 40 64 215,297 20 128 659,969 20 256 2,237,441 8 512 8,144,897 4 24"
"gfm_rag_p25_c001","gfm_rag","gfm_rag.pdf","25","1","/uni00000013/uni00000011/uni00000013/uni0000001b/uni00000030/uni00000013/uni00000011/uni00000015/uni00000030/uni00000013/uni00000011/uni0000001a/uni00000030/uni00000015/uni00000030/uni0000001b/uni00000030 /uni00000006/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni0000000b/uni0000004f/uni00000052/uni0000004a/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000c /uni00000013/uni00000011/uni00000017/uni0000001c /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni00000018/uni00000014 /uni00000013/uni00000011/uni00000018/uni00000015 /uni00000013/uni00000011/uni00000018/uni00000016 /uni00000013/uni00000011/uni00000018/uni00000017 /uni00000013/uni00000011/uni00000018/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000019 /uni00000013/uni00000011/uni00000018/uni0000001a /uni00000013/uni00000011/uni00000018/uni0000001b /uni00000013/uni00000011/uni00000018/uni0000001c/uni00000030/uni00000035/uni00000035 Model Scaling /uni00000016/uni0000004e /uni00000019/uni0000004e /uni00000014/uni00000015/uni0000004e /uni00000015/uni00000017/uni0000004e /uni00000016/uni00000013/uni0000004e /uni00000016/uni00000019/uni0000004e /uni00000017/uni00000018/uni0000004e /uni00000016/uni0000004e/uni00000018/uni0000004e/uni00000014/uni00000013/uni0000004e/uni00000015/uni00000013/uni0000004e/uni00000016/uni00000013/uni0000004e/uni00000018/uni00000013/uni0000004e /uni00000006/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni0000000b/uni0000004f/uni00000052/uni0000004a/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni0000000c /uni00000013/uni00000011/uni00000017/uni0000001c /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni00000018/uni00000014 /uni00000013/uni00000011/uni00000018/uni00000015 /uni00000013/uni00000011/uni00000018/uni00000016 /uni00000013/uni00000011/uni00000018/uni00000017 /uni00000013/uni00000011/uni00000018/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000019 /uni00000013/uni00000011/uni00000018/uni0000001a /uni00000013/uni00000011/uni00000018/uni0000001b /uni00000013/uni00000011/uni00000018/uni0000001c/uni00000030/uni00000035/uni00000035 Data Scaling /uni00000013/uni00000011/uni00000013/uni0000001b/uni00000030 /uni00000013/uni00000011/uni00000015/uni00000030 /uni00000013/uni00000011/uni0000001a/uni00000030 /uni00000015/uni00000030 /uni0000001b/uni00000030 Figure 5: The illustration of the model and data scaling law of GFM-RAG. Table 17: The different number of layers with corresponding model size and performance for scaling law analysis. Hidden Dim. = 512 Averge HotpotQA MuSiQue 2Wiki L-Layer R@2 R@5 R@2 R@5 R@2 R@5 R@2 R@5 1-layer (3M) 53.9 66.7 59.3 74.2 40.7 50.2 61.8 75.7 2-layer (4M) 69.9 78.6 73.6 85.4 47.6 57.0 88.6 93.3 4-layer (6M) 72.2 80.1 78.4 87.8 49.3 60.1 88.8 92.5 6-layer (8M) 71.9 79.6 78.0 87.0 48.4 58.7 89.3 93.1 8-layer (10M) 73.0 79.9 79.7 87.8 49.7 59.1 89.5 92.8 and logical expressibility contribute to GFM-RAG ’s strong performance. We recognize the potential of other architectural designs and aim to explore them in the future, inspiring the community to do the same. E.8 Visualization of the Distribution of Multi-hop Prediction In this section, we visualize the distribution of the number of hops in the multi-hop reasoning process of GFM-RAG. We calculate the number of hops in the ground-truth reasoning path required for each question in the test set of HotpotQA, MuSiQue, and 2Wiki. Then, we compare the distribution of the number of hops in the reasoning path of the ground-truth and the predicted reasoning path by GFM-RAG as well as HippoRAG."
"gfm_rag_p25_c002","gfm_rag","gfm_rag.pdf","25","2","for each question in the test set of HotpotQA, MuSiQue, and 2Wiki. Then, we compare the distribution of the number of hops in the reasoning path of the ground-truth and the predicted reasoning path by GFM-RAG as well as HippoRAG. The results are shown in Figure 6. We can observe that the distribution of GFM-RAG is closely aligned to the ground-truth, which indicates that GFM-RAG can effectively conduct the multi-hop reasoning within a single step. Meanwhile, the distribution of HippoRAG is relatively different from the ground-truth, especially in 2Wiki dataset. This indicates that HippoRAG may not be able to effectively capture the complex relationship to conduct multi-hop reasoning on graphs. Table 18: The cost of the KG-index construction. LLM Price per 10k docs. Total Price GPT-4o-mini $2.93 $216 25"
"gfm_rag_p26_c001","gfm_rag","gfm_rag.pdf","26","1","/uni00000015/uni00000017/uni00000019/uni0000001b /uni0000002b/uni00000052/uni00000053/uni00000056 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013/uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000029/uni00000030/uni00000010/uni00000035/uni00000024/uni0000002a/uni00000003/uni00000059/uni00000056/uni00000003/uni0000002a/uni00000037/uni00000003/uni00000030/uni00000024/uni00000028/uni0000001d/uni00000003/uni00000015/uni00000011/uni00000013/uni00000016 /uni0000002b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000035/uni00000024/uni0000002a/uni00000003/uni00000059/uni00000056/uni00000003/uni0000002a/uni00000037/uni00000003/uni00000030/uni00000024/uni00000028/uni0000001d/uni00000003/uni00000017/uni00000011/uni0000001a/uni00000014 /uni0000002b/uni00000052/uni00000057/uni00000053/uni00000052/uni00000057/uni00000034/uni00000024 /uni00000015/uni00000017/uni00000019/uni0000001b /uni0000002b/uni00000052/uni00000053/uni00000056 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013/uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000029/uni00000030/uni00000010/uni00000035/uni00000024/uni0000002a/uni00000003/uni00000059/uni00000056/uni00000003/uni0000002a/uni00000037/uni00000003/uni00000030/uni00000024/uni00000028/uni0000001d/uni00000003/uni00000017/uni00000011/uni0000001c/uni00000013 /uni0000002b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000035/uni00000024/uni0000002a/uni00000003/uni00000059/uni00000056/uni00000003/uni0000002a/uni00000037/uni00000003/uni00000030/uni00000024/uni00000028/uni0000001d/uni00000003/uni0000001b/uni00000011/uni00000014/uni00000016 /uni00000030/uni00000058/uni00000036/uni0000004c/uni00000034/uni00000058/uni00000048 /uni00000015/uni00000017/uni00000019/uni0000001b /uni0000002b/uni00000052/uni00000053/uni00000056 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013/uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000029/uni00000030/uni00000010/uni00000035/uni00000024/uni0000002a/uni00000003/uni00000059/uni00000056/uni00000003/uni0000002a/uni00000037/uni00000003/uni00000030/uni00000024/uni00000028/uni0000001d/uni00000003/uni00000017/uni00000011/uni00000018/uni00000019 /uni0000002b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000035/uni00000024/uni0000002a/uni00000003/uni00000059/uni00000056/uni00000003/uni0000002a/uni00000037/uni00000003/uni00000030/uni00000024/uni00000028/uni0000001d/uni00000003/uni00000014/uni00000019/uni00000011/uni00000017/uni00000017 /uni00000015/uni0000003a/uni0000004c/uni0000004e/uni0000004c /uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000010/uni00000057/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni0000000b/uni0000002a/uni00000037/uni0000000c/uni0000002a/uni00000029/uni00000030/uni00000010/uni00000035/uni00000024/uni0000002a/uni0000002b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000035/uni00000024/uni0000002a Figure 6: Statistics of the prediction hops of GFM-RAG and HippoRAG against the ground-truth. Table 19: Token cost comparison for index construction Method # Tokens per 10k documents LightRAG 55M GraphRAG 76M GFM-RAG 48M E.9 Cost and Impact of LLMs on KG-index Construction In this section, we first analyze the cost of the KG-index construction. In experiments, we utilize GPT- 4o-mini4 for OpenIE extraction and construct the KG-index for 737,310 documents. The cost is shown in Tables 18 and 19. Specifically, we find that constructing the KG-index requires approximately 48M tokens per 10k documents, which costs around $2.6 using GPT-4o-mini. LightRAG and GraphRAG cost 57M tokens and 76M tokens, respectively. Compared to other methods, GFM-RAG is more cost-effective as it does not require generating community-level summaries. In addition, we compare the graph index construction time of GFM-RAG in Table 20. Results show that GFM-RAG benefits from the quick index process during retrieval, as it does not construct a traditional vector database to store documents and entities. Admittedly, using an LLM for KG index construction incurs computational costs. However, KG construction has been extensively studied, and numerous alternative methods exist that do not rely on LLMs [76]. Our implementation offers an easy interface for integration with any KG construction tools. We would explore the use of other"
"gfm_rag_p26_c002","gfm_rag","gfm_rag.pdf","26","2","costs. However, KG construction has been extensively studied, and numerous alternative methods exist that do not rely on LLMs [76]. Our implementation offers an easy interface for integration with any KG construction tools. We would explore the use of other KG construction methods in future work. We further analyze the impact of LLMs used for KG-index construction on the performance of GFM-RAG. We conduct experiments using different LLMs for KG-index construction, including GPT-4o-mini and GPT-3.5-turbo5. Then, we reevaluate the performance of GFM-RAG and HippoRAG with the constructed KG-index. The results are shown in Table 21. From the results, the performance of both methods on the KG extracted by GPT-4o-mini is higher than the ones by GPT-3.5-turbo. This supports the opinion that GPT-4o-mini generally outperforms GPT-3.5-turbo in constructing high quality KG-index, which is crucial for the graph-enhanced retrieval. However, the performance 4https://platform.openai.com/docs/models/o4-mini 5https://platform.openai.com/docs/models/gpt-3-5-turbo Table 20: Graph Indexing time comparison. Method Indexing time (s) LightRAG 1430.32 GraphRAG (MS) 1796.43 GFM-RAG 93.55 26"
"gfm_rag_p27_c001","gfm_rag","gfm_rag.pdf","27","1","Table 21: Comparison of the model performance under the KG-index constructed by different LLMs. Method HotpotQA MuSiQue 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 GFM-RAG (gpt-4o-mini) 78.3 87.1 49.1 58.2 90.8 95.6 HippoRAG (gpt-4o-mini) 62.2 79.3 41.7 53.6 72.1 89.5 GFM-RAG (gpt-3.5-trubo) 75.6 84.7 46.1 55.8 85.2 90.4 HippoRAG (gpt-3.5-trubo) 60.5 77.7 40.9 51.9 70.7 89.1 of GFM-RAG is significantly higher than HippoRAG under both KG-indexes. This indicates that GFM-RAG is more robust to the quality of the KG-index, demonstrating the effectiveness of the GFM in graph reasoning and retrieval. F Prompts In experiments, we follow the prompts used in HippoRAG [ 16] to extract the triples from the document corpus, which is shown in Table 22. G Limitations The limitations of GFM-RAG are as follows: (1) The construction of KG-index can be costly and time-consuming, especially when using LLMs for OpenIE extraction. We would explore the use of efficient KG construction methods in future work and optimize the construction process. (2) The model size of the GFM-RAG is relatively small (8M) compared to other foundation models like large language models with billions of parameters. Although it is not faired to directly compare the GNN-based model with transformer-based LLMs, we would explore the scaling of GFM-RAG in future work to improve its performance and generalizability. (3) GFM-RAG is only evaluated on multi-hop QA tasks and KG completion tasks. We would explore the capabilities of GFM-RAG in other tasks such as knowledge graph question answering and knowledge graph reasoning in future"
"gfm_rag_p27_c002","gfm_rag","gfm_rag.pdf","27","2","improve its performance and generalizability. (3) GFM-RAG is only evaluated on multi-hop QA tasks and KG completion tasks. We would explore the capabilities of GFM-RAG in other tasks such as knowledge graph question answering and knowledge graph reasoning in future work to validate its effectiveness as a foundation model. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 27"
"gfm_rag_p28_c001","gfm_rag","gfm_rag.pdf","28","1","OpenIE Prompt ## I n s t r u c t i o n : Your task is to c on st ruc t an RDF ( Resource D e s c r i p t i o n Fr am ew ork ) graph from the given passages and named entity lists . Respond with a JSON list of triples , with each triple r e p r e s e n t i n g a r e l a t i o n s h i p in the RDF graph . Pay a tt en tio n to the f ol low in g r e q u i r e m e n t s : - Each triple should contain at least one , but p r e f e r a b l y two , of the named entities in the list for each passage . - Clearly resolve pronouns to their specific names to maintain clarity . Convert the pa ra gr aph into a JSON dict , it has a named entity list and a triple list . ## One - Shot D e m o n s t r a t i o n : Pa ra gr ap h : ‘‘‘ Radio City Radio City is India ’ s first private FM radio station and was started on 3 July 2001. It plays Hindi , English and regional songs . Radio City recently forayed into New Media in"
"gfm_rag_p28_c002","gfm_rag","gfm_rag.pdf","28","2","h : ‘‘‘ Radio City Radio City is India ’ s first private FM radio station and was started on 3 July 2001. It plays Hindi , English and regional songs . Radio City recently forayed into New Media in May 2008 with the launch of a music portal - P l a n e t R a d i o c i t y . com that offers music related news , videos , songs , and other music - related features . ‘‘‘ { "" n a m e d _ e n t i t i e s "": ["" Radio City "" , "" India "" , ""3 July 2001"" , "" Hindi "" , "" English "" , "" May 2008"" , "" P l a n e t R a d i o c i t y . com ""] } { "" triples "": [ ["" Radio City "" , "" located in "" , "" India ""] , ["" Radio City "" , "" is "" , "" private FM radio station ""] , ["" Radio City "" , "" started on "" , ""3 July 2001""] , ["" Radio City "" , "" plays songs in "" , "" Hindi ""] , ["" Radio City "" , "" plays songs in "" , "" English ""] ["" Radio City "" , "" forayed into "" , "" New Media ""] , ["" Radio City "" , "" launched "" , """
"gfm_rag_p28_c003","gfm_rag","gfm_rag.pdf","28","3",", ["" Radio City "" , "" plays songs in "" , "" English ""] ["" Radio City "" , "" forayed into "" , "" New Media ""] , ["" Radio City "" , "" launched "" , "" P l a n e t R a d i o c i t y . com ""] , ["" P l a n e t R a d i o c i t y . com "" , "" launched in "" , "" May 2008""] , ["" P l a n e t R a d i o c i t y . com "" , "" is "" , "" music portal ""] , ["" P l a n e t R a d i o c i t y . com "" , "" offers "" , "" news ""] , ["" P l a n e t R a d i o c i t y . com "" , "" offers "" , "" videos ""] , ["" P l a n e t R a d i o c i t y . com "" , "" offers "" , "" songs ""] ] } ## Input Convert the pa ra gr aph into a JSON dict , it has a named entity list and a triple list . Pa ra gr ap h : ‘‘‘ INPUT PASSAGE ‘‘‘ Table 22: The prompt for OpenIE extraction. 28"
"gfm_rag_p29_c001","gfm_rag","gfm_rag.pdf","29","1","NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: The main claims in the abstract and introduction accurately reflect the paper’s contributions and scope. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of the work in Appendix G. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate ""Limitations"" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,"
"gfm_rag_p29_c002","gfm_rag","gfm_rag.pdf","29","2","The authors are encouraged to create a separate ""Limitations"" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in"
"gfm_rag_p29_c003","gfm_rag","gfm_rag.pdf","29","3","honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] 29"
"gfm_rag_p30_c001","gfm_rag","gfm_rag.pdf","30","1","Justification: The paper does not include theoretical results. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have detailed data construction process, model settings, and training process in Appendix D to ensure the reproducibility of our results. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If"
"gfm_rag_p30_c002","gfm_rag","gfm_rag.pdf","30","2","experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the"
"gfm_rag_p30_c003","gfm_rag","gfm_rag.pdf","30","3","of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? 30"
"gfm_rag_p31_c001","gfm_rag","gfm_rag.pdf","31","1","Answer: [Yes] Justification: We have uploaded the code to an anonymous link in the paper. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of"
"gfm_rag_p31_c002","gfm_rag","gfm_rag.pdf","31","2","(appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have detailed experiment settings in Appendix D. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The experiments are conducted with a fixed random seed and no error bars are reported. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should"
"gfm_rag_p31_c003","gfm_rag","gfm_rag.pdf","31","3","The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). 31"
"gfm_rag_p32_c001","gfm_rag","gfm_rag.pdf","32","1","• It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The resources used for the experiments are detailed in Appendix D.3. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper"
"gfm_rag_p32_c002","gfm_rag","gfm_rag.pdf","32","2","provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper complies with the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The proposed method focuses on the technical aspects of the problem and do not include societal impacts. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses"
"gfm_rag_p32_c003","gfm_rag","gfm_rag.pdf","32","3","performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 32"
"gfm_rag_p33_c001","gfm_rag","gfm_rag.pdf","33","1","• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper utilizes existing datasets and pretrained models that are already released which have safeguards in place. Guidelines: • The answer NA means"
"gfm_rag_p33_c002","gfm_rag","gfm_rag.pdf","33","2","a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper utilizes existing datasets and pretrained models that are already released which have safeguards in place. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have properly cited all code, data, and models used in our research and complied with the licensing agreements and terms of use set by the original authors. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which"
"gfm_rag_p33_c003","gfm_rag","gfm_rag.pdf","33","3","set by the original authors. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. 33"
"gfm_rag_p34_c001","gfm_rag","gfm_rag.pdf","34","1","• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: There are no new assets introduced in the paper. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human"
"gfm_rag_p34_c002","gfm_rag","gfm_rag.pdf","34","2","subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that"
"gfm_rag_p34_c003","gfm_rag","gfm_rag.pdf","34","3","that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 34"
"gfm_rag_p35_c001","gfm_rag","gfm_rag.pdf","35","1","16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: The usage of the LLM is described and discussed in Appendices D and D.1. Guidelines: • The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. • Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM) for what should or should not be described. 35"
"graphflow_rag_p01_c001","graphflow_rag","graphflow_rag.pdf","1","1","Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need? Junchi Yu1, Yujie Liu2, Jindong Gu1, Philip Torr1, Dongzhan Zhou2∗ 1Department of Engineering Science, University of Oxford, UK 2Shanghai Artificial Intelligence Laboratory, China junchi.yu@eng.ox.ac.uk, liuyujie1@pjlab.org.cn, jindong.gu@eng.ox.ac.uk , philip.torr@eng.ox.ac.uk, zhoudongzhan@pjlab.org.cn Abstract Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) en- hances large language models (LLMs) with structural and textual external knowl- edge. Yet, existing KG-based RAG methods struggle to retrieve accurate and diverse information when handling complex queries. By modeling KG-based retrieval as a multi-step decision process, Process Reward Models (PRMs) offer a promising solution to align the retrieval behavior with the query-specific knowledge requirements. However, PRMs heavily rely on process-level supervision signals that are expensive and hard to obtain on KGs. To address this challenge, we propose GraphFlow, a framework that efficiently retrievesaccurate and diverse knowledge required for complex queries from text-rich KGs. GraphFlow employs a detailed balance objective with local exploration to jointly optimize a retrieval policy and a flow estimator. The flow estimator factorizes the outcome reward of the retrieval results into the intermediate retrieval steps. Such reward factorization guides the retrieval policy to retrieve candidates from KGs in proportion to their outcome reward. This allows GraphFlow to explore relevant regions of KGs that yield diverse and accurate results. We evaluate GraphFlow on STaRK benchmark, which includes real-world queries from multiple domains over text-rich KGs. GraphFlow outperforms strong KG-based RAG baselines including GPT-4o by 10% perfor- mance gain on both retrieval accuracy and diversity metrics. GraphFlow also shows"
"graphflow_rag_p01_c002","graphflow_rag","graphflow_rag.pdf","1","2","results. We evaluate GraphFlow on STaRK benchmark, which includes real-world queries from multiple domains over text-rich KGs. GraphFlow outperforms strong KG-based RAG baselines including GPT-4o by 10% perfor- mance gain on both retrieval accuracy and diversity metrics. GraphFlow also shows strong generalization by effectively retrieving information from unseen KGs to support new-domain queries, highlighting its effectiveness and robustness 2. 1 Introduction Retrieval-Augmented Generation (RAG) [36] has emerged as a promising framework to reduce the hallucination of Large Language Models (LLMs) by mitigating the gap between model knowledge and factual knowledge [28, 89, 26]. Traditional RAG usually employs an unstructured vector-indexed database as the external knowledge source, where the text corpus is indexed using pretrained encoders to support retrieval. Recent work has explored knowledge graphs (KGs) [68] as a structural alternative to the external knowledge source of RAG [59]. KGs enjoy several advantages over the vector-indexed database in traditional RAG, such as representing relational information with graph structures [88], integrating knowledge from heterogeneous resources [59], and enhancing interpretability by neural- symbolic reasoning [87]. Thus, KG-based RAG has demonstrated great potential in enhancing LLMs in many domains, including medical diagnosis [67], biochemistry [77], and physics [72]. ∗Corresponding Author 2Code is available https://github.com/Samyu0304/GraphFlow 39th Conference on Neural Information Processing Systems (NeurIPS 2025)."
"graphflow_rag_p02_c001","graphflow_rag","graphflow_rag.pdf","2","1","Recent KG-based RAG methods employ two approaches to retrieve information from KGs when receiving an input query [47, 19, 48]. Retrieval-based approaches [21, 37, 80] leverage pretrained language models [62] to encode KG text into embeddings, and use retriever models [31] to identify relational triplets or subgraphs in KGs that most support the query. And agent-based methods treat LLMs as searching agents to navigate across KGs and retrieve a relational path with supporting information for a given query [68, 47, 50]. While KG-based RAG methods show promise in retrieving structural information for simple relational queries, their effectiveness is limited in more complex ones. As shown in Figure 1, structural information in KGs alone is often sufficient for many relational queries. For example, retrieving the relation triplet (Alice, daughter, Bob) adequately answers the question ""Who is the father of Alice?"". 𝑄: Who is the father of Alice? 𝑄: List the papers published by University A related to topic B? University A Researcher Researcher Affiliation Affiliation Authorship Paper Authorship Authorship Authorship Paper Paper Paper Jane Bob John Alice Jack Mike Katie Daughter Student Friend Teacher Teacher Sam Student Friend Author name: Igor A. Abrikosov Author paper count: 409 Author citation count: 7624 -Paper title: Spectral properties of the three-dimensional Hubbard Model -Abstract: We present momentum- resolved single particle spectra for the three-dimensional Hubbard model for the paramagnetic and antiferromagnetically ordered phase obtained within the dynamical cluster approximation. The effective cluster problem … -Publication date: 2011-06-07 -Venue: Physical Review B -Paper title :"
"graphflow_rag_p02_c002","graphflow_rag","graphflow_rag.pdf","2","2","We present momentum- resolved single particle spectra for the three-dimensional Hubbard model for the paramagnetic and antiferromagnetically ordered phase obtained within the dynamical cluster approximation. The effective cluster problem … -Publication date: 2011-06-07 -Venue: Physical Review B -Paper title : Spectral properties of the three - dimensional Hubbard Model Figure 1: Comparison of the retrieval tasks between relational and complex queries. However, complex queries typically require leveraging both structural and textual information during retrieval [50]. Consider the paper searching query ""Please list the papers published by Uni- versity A relevant to research topic B"". Addressing such a query requires understanding authorship and affiliation rela- tionships, as well as text de- scriptions of paper content, research topics, institutions, and authors. The fusion of relational and text knowledge presents a significant challenge for accurate retrieval with KG-based RAG methods. Another challenge is the diversity of retrieval targets in complex queries. Unlike relational queries corresponding to a single deterministic retrieval target (e.g., Alice daughter − − − − − − →Bob), complex queries require retrieving a diverse set of candidates. For example, the paper searching query in Figure 1 corresponds to multiple retrieval targets. Therefore, KG-based RAG must emphasize both retrieval accuracy and diversity when supporting complex queries. Unfortunately, our empirical findings reveal that existing KG-based RAG methods face challenges in achieving this goal. To overcome the above challenge, it is essential to align the retrieval process of KG-based RAG with the diversity and accuracy demands for complex queries. Process Reward Models (PRMs)"
"graphflow_rag_p02_c003","graphflow_rag","graphflow_rag.pdf","2","3","that existing KG-based RAG methods face challenges in achieving this goal. To overcome the above challenge, it is essential to align the retrieval process of KG-based RAG with the diversity and accuracy demands for complex queries. Process Reward Models (PRMs) [54, 90, 92, 85] offer a promising framework for this goal. By providing step-wise guidance, PRMs have been widely used in LLM alignment [40], reasoning [7] and planning [6] when treating these tasks as the multi-step decision. In KG-based RAG, the retrieval process can be naturally viewed as a multi-step decision process, where an agent traverses a KG and expands its retrieval trajectory at each step. PRMs can provide step-wise guidance for the agent to retrieve desired information for complex queries. However, training a PRM needs high-quality preference datasets with fine-grained and process-level reward signals [57, 76, 27]. In KG-based RAG, assessing the process-level reward at each step in retrieval trajectories is expensive. Only the outcome reward is easily available (i.e., whether the retrieval trajectory can support a query or not). Proposed Work. We present GraphFlow, a novel framework for supporting complex queries by retrieving accurate and diverse knowledge from knowledge graphs (KGs), without relying on process-level reward supervision. Inspired by GFlowNet [3], GraphFlow formulates the problem of retrieving from KGs as learning a retrieval policy that generates retrieval trajectories with probabilities proportional to their outcome rewards. Thus, the retrieval trajectory that better supports the query is retrieved with a higher probability, leading to diverse and accurate retrieval results. To"
"graphflow_rag_p02_c004","graphflow_rag","graphflow_rag.pdf","2","4","KGs as learning a retrieval policy that generates retrieval trajectories with probabilities proportional to their outcome rewards. Thus, the retrieval trajectory that better supports the query is retrieved with a higher probability, leading to diverse and accurate retrieval results. To achieve this, GraphFlow jointly trains the retrieval policy with a flow estimator, which assigns non-negative flow values to partial trajectories. These flow values decompose the final outcome reward across intermediate retrieval steps, thereby providing rich supervision signals without requiring explicit process-level rewards. The retrieval policy is guided by these flow values and receives process-level supervision “for free”. We adopt the detailed balance objective [64] to co-train the retrieval policy and the flow estimator. To further enhance training efficiency, we introduce a local exploration strategy that reduces visits to low-reward regions of the KG. Thus, GraphFlow effectively explores high-reward regions of the retrieval space, leading to more accurate and diverse retrievals that better support complex query. 2"
"graphflow_rag_p03_c001","graphflow_rag","graphflow_rag.pdf","3","1","We evaluate the effectiveness of GraphFlow on the STaRK benchmark [74], which involves retrieving from text-rich KGs for real-world queries across multiple domains. Extensive experiments demon- strate that GraphFlow consistently produces high-quality and diverse retrieval results, outperforming both Process Reward Models (PRMs) and existing KG-based RAG methods. Notably, GraphFlow surpasses strong KG-based RAG baselines instantiated with GPT-4o, achieving an average improve- ment of 10% in both retrieval accuracy and diversity metrics. In addition, GraphFlow enjoys strong generalization capabilities and can retrieve from unseen KGs to support queries in new domains. 2 Preliminary and Notations 2.1 KG-based RAG We denote a knowledge graph (KG) as G = {V, E} where V and E are the sets of nodes and edges. The node Vi is associated with a short text description of an entity (e.g. Vi = ‘Alice’). And the edge eij denotes the relationship between node Vi and Vj. For example, eij = ‘daughter’represents the relationship between Vi = ‘Alice’ andVj = ‘Bob’. Retrieval-based Approach. For an input query Q, the retrieval-based method [21, 37, 13, 48, 80, 4] first encodes the texts in nodes and edges into embeddings using a pretrained LM [ 62]. Then, a retriever Ret(·) is employed to retrieve a subgraph Gsub from G [84] that can support answering Q: max Gsub P (A|Q, Gsub), G sub = Ret(G). (1) Here A is the answer to the input query. Some works employ non-parametric retrievers, such as dense retriever with vector similarity [1] and Prize-Collecting Steiner Tree (PCST)"
"graphflow_rag_p03_c002","graphflow_rag","graphflow_rag.pdf","3","2","support answering Q: max Gsub P (A|Q, Gsub), G sub = Ret(G). (1) Here A is the answer to the input query. Some works employ non-parametric retrievers, such as dense retriever with vector similarity [1] and Prize-Collecting Steiner Tree (PCST) algorithm [21] to retrieve from KGs. Other works train parameterized retriever models based on Multi-layer Perceptron (MLP) and Graph Neural Network (GNN) [82] to retrieve from KGs. Agent-based Approach. Recent work formulates retrieval on KGs as a multi-step decision process [68, 47, 38] and employs LLM agents to search on KGs due to their superior capability in plan- ning. For an input query Q, the agent LLM(·) starts from an initial node V0 and searches T steps incrementally in a KG to produce a retrieval trajectory τ = V0 → · · · → VT −1 → VT to support Q: max τ ∈T P (A|Q, τ), τ = LLM(G). (2) The inital node V0 can be identified using Entity name recognition (ENR) [18] or vector similarity matching [63]. At step t, the searching agent expands the trajectory conditioned on the input query Q, the partial trajectory at t step τ≤t = V0 → · · · → Vt−1 → Vt: Vt+1 ∼ PLLM(Vt+1|Q, τ≤t), V t+1 ∈ N (Vt). (3) Here PLLM is the policy instantiated by an LLM, and N (Vt) is the neighborhood node set of Vt. 2.2 Process Reward Models Process Reward Models (PRMs) have emerged as a promising framework for aligning large language models (LLMs)"
"graphflow_rag_p03_c003","graphflow_rag","graphflow_rag.pdf","3","3","(3) Here PLLM is the policy instantiated by an LLM, and N (Vt) is the neighborhood node set of Vt. 2.2 Process Reward Models Process Reward Models (PRMs) have emerged as a promising framework for aligning large language models (LLMs) with human preferences [85, 73, 40]. For a multi-step decision problem, denotes ∈ S as the state anda ∈ A as the action. Training a PRM requires a preference datasetD = {(a+ i , a− i , si) | i = 1, · · · , N}, where a+ i and a− i are positive and negative actions at state si, respectively. Such datasets can be constructed through human supervision [ 40], rule-based heuristics [ 55], or LLM- generated annotations [76]. The goal of PRM is to learn a scoring function rθ(a, s) : A × S → R that assigns real-valued preference scores to action–state pairs by minimizing the following objective: LPRM = −E(a+ i ,a− i ,si)∼D log[σ(rθ(a+ i , si) − rθ(a− i , si))], (4) where σ(·) denotes the sigmoid function. Once trained, the PRM can be used to provide step-wise preference signals for LLM alignment [ 40, 73]. Additionally, it can directly guide the multi-step decision with a soft policy P (st+1|st) ∝ erθ(st,at) [7, 73, 76, 92]. 3"
"graphflow_rag_p04_c001","graphflow_rag","graphflow_rag.pdf","4","1","𝑃 𝑠𝑡+1|𝑠𝑡 𝐹(𝑠0) 𝐹(𝑠3) 𝐹(𝑠2)𝐹(𝑠1) 𝜏1 𝜏2 𝑃(𝜏2) ∝ 𝑅(𝜏2) 𝑃(𝜏1) ∝ 𝑅(𝜏1) LLM Flow Head Policy Head 𝐹(𝑠𝑡) 𝑃 𝑠𝑡+1|𝑠𝑡 𝑠𝑡 (𝑠𝑡, 𝑎𝑡) Policy_prompt = T emplate('''###Information trajectory you have visited: {{history}} ###Question: {{question}} ###Candidate Information: {{candidate}} Please predict the score of the candidate to help you find the answer to the question.''') Flow_prompt = T emplate('''###Information trajectory you have visited: {{history}} ###Question: {{question}} Please predict the reward of the Information trajectory to the question.''') (a). Illustration of GraphFlow. (b). LLM-based Implementation of GraphFLow. Figure 2: An overview of the proposed GraphFlow framework. (a). GraphFlow employs a flow estimator F (·) to factorize the outcome reward R(τ) of a retrieval trajectory τ to flow value F (st). The flow value guides to learn a policy P (st+1|st) that leads to accurate and diverse retrieval results for complex queries. (b) We introduce an LLM-based implementation of GraphFlow to enhance KG-based RAG on text-rich KGs. 3 Method 3.1 KG-based RAG as Multi-step Decision Problem Formulation. Given an input query Q, our goal is to retrieve a set of K target nodes V∗ = {Vi | i = 1, · · · , K} from a text-rich knowledge graph (KG) G = {V, E, D} such that the associated documents D∗ = {D∗ i | i = 1 , · · · , K} can support answering Q. Here, Vi ∈ V is a node, Eij ∈ E is an edge between Vi and Vj indicating their relation. Each Di denotes the"
"graphflow_rag_p04_c002","graphflow_rag","graphflow_rag.pdf","4","2","{D∗ i | i = 1 , · · · , K} can support answering Q. Here, Vi ∈ V is a node, Eij ∈ E is an edge between Vi and Vj indicating their relation. Each Di denotes the textual document associated with node Vi (e.g., the content of a paper). Agent-based Retrieval as a Multi-step Decision Process. To effectively leverage both relational and text information in the KG, we employ the agent-based approach initiated with an LLM due to its superior text understanding and planning ability. We formulate the agent-based retrieval as a multi-step decision problem, consisting of the following components: • State. The agent starts retrieval from the initial state s0 = ( Q, {D0}), where D0 is the document associated with the source node V0 from which the retrieval process is initiated. At step t, the agent arrives at node Vt and the current state is defined as st = (Q, {Dj}t j=0), where {Dj}t j=0 is the set of documents collected along the partial retrieval trajectory τ≤t = V0 → · · · → Vt. • Action. Given state st, the agent selects an action at ∈ A(st), corresponding to moving from Vt to a adjacent node Vt+1 ∈ N(Vt) along edge Et,t+1. The agent then retrieves the documents Dt+1 associated with Vt+1. • Transition. The agent transits to state st+1 = (Q, {Dj}t+1 j=0). This process continues until either the document Dt+1 is deemed sufficient to support the query Q, or a predefined maximum"
"graphflow_rag_p04_c003","graphflow_rag","graphflow_rag.pdf","4","3","then retrieves the documents Dt+1 associated with Vt+1. • Transition. The agent transits to state st+1 = (Q, {Dj}t+1 j=0). This process continues until either the document Dt+1 is deemed sufficient to support the query Q, or a predefined maximum number of steps is reached. • Reward. Upon termination, the agent receives a reward R(τ) for the retrieval trajectory τ. The reward is calculated whether the document DT associated with the terminal node VT of trajectory τ can the query (i.e. DT ∈ D∗). Energy-based Modeling for Accurate and Diverse Retrieval. As shown in Figure 2 (a), the goal of GraphFlow is to learn the policy P (st+1 | st) that can effectively retrieve accurate and diverse information from a knowledge graph (KG) to support answering an input query. To this end, we formulate the retrieval process as an energy-based distribution over trajectories: P (τ) = TY t=0 P (st+1 | st) ∝ R(τ). (5) The equality in Eq. 5 is due to the Markov property of the state transition. In contrast to the objectives of prior KG-based RAG methods in Eq. 1 and Eq. 2 that maximize the likelihood of the most relevant information in KG, the objective of GraphFlow in Eq. 5 reflects the intuition that high-reward retrieval trajectories (i.e., trajectories ending in high-quality supporting documents) should be sampled more frequently. Thus, GraphFlow naturally promotes diverse yet 4"
"graphflow_rag_p05_c001","graphflow_rag","graphflow_rag.pdf","5","1","Table 1: Performance of retrieval accuracy of KG-based RAG methods on STaRK benchmark. Our GraphFlow outperforms baselines with higher hit rates and MRR scores. GraphFlow also surpasses strong baselines implemented with GPT-4o on most metrics. Method Dataset STaRK-AMAZON STaRK-MAG STaRK-PRIME Metric Hit@1 ↑ Hit@5↑ MRR↑ Hit@1↑ Hit@5↑ MRR↑ Hit@1↑ Hit@5↑ MRR↑ Retrieval -based DenseRetriver 6.10 15.85 10.61 24.44 40.23 32.41 5.43 13.07 8.99 G-Retriever 6.10 11.59 8.54 24.44 31.95 28.08 5.43 8.94 6.95 SubgraphRAG 8.03 12.43 9.90 9.30 25.59 16.11 4.82 8.00 6.17 Agent- based (w/o Rerank) ToG+LLaMA3 4.21 6.16 5.25 12.0 14.09 12.67 21.92 34.0 26.61 ToG+GPT4o 20.67 41.38 30.90 23.33 56.67 36.38 16.67 39.77 27.02 SFT 8.16 15.30 13.54 26.53 28.57 29.10 27.5 40.07 33.06 PRM 20.09 26.25 28.16 26.05 28.0 28.52 21.01 46.72 31.25 GraphFlow 19.63 44.17 31.66 29.32 58.64 41.32 39.84 71.71 54.58 Agent- based (Rerank) ToG+LLaMA3 4.21 6.16 5.25 12.0 14.09 12.67 21.92 34.0 26.61 ToG+GPT4o 27.58 51.72 39.08 26.67 56.67 39.65 53.33 63.73 57.78 SFT 12.24 30.61 21.54 27.55 44.89 36.37 23.75 52.5 35.98 PRM 21.25 42.50 31.97 27.31 44.09 33.69 22.86 28.24 26.94 GraphFlow 47.85 65.03 55.49 39.09 57.51 47.82 51.39 72.11 61.37 accurate retrieval results since the retrieval trajectories resulting in highly relevant documents are more likely to be explored. Moreover, GraphFlow also enjoys good generalization ability by avoiding strict likelihood maximization and does not overfit to a few dominant candidates. 3.2 Flow Estimation as Credit Assignment A major challenge in learning the policy P (st+1 | st) to satisfy Eq. 5"
"graphflow_rag_p05_c002","graphflow_rag","graphflow_rag.pdf","5","2","also enjoys good generalization ability by avoiding strict likelihood maximization and does not overfit to a few dominant candidates. 3.2 Flow Estimation as Credit Assignment A major challenge in learning the policy P (st+1 | st) to satisfy Eq. 5 is the lack of process-level supervision. When collecting retrieval trajectories τ ∈ T for training, only the outcome reward R(τ) is observable, indicating whether the final retrieved document supports answering the query. Annotating the process-level reward signals for every intermediate state and action is expensive. This gives rise to the credit assignment problem [56, 91], which attributes the terminal reward of a trajectory back to the intermediate decisions. To address this, we adopt the GFlowNet framework [3], which implicitly performs credit assignment by estimating a non-negative flow value for each state. Rather than directly maximizing the reward or value of a full trajectory, GFlowNets introduce a flow function F (s) : S → R≥0 for each intermediate state s. The learning objective enforces a local consistency constraint between transitions, which is known as the detailed balance condition: F (st) · P (st+1 | st) = F (st+1) · PB(st | st+1), (6) where P (st+1 | st) is the forward policy we want to obtain, and PB(st | st+1) is the backward policy. When this condition holds for all transitions, the retrieval trajectory induced by the policy P (st+1 | st) satisfies the objective in Eq. 5, leading to diverse and accurate retrieval results on KGs. While alternative GFlowNet objectives,"
"graphflow_rag_p05_c003","graphflow_rag","graphflow_rag.pdf","5","3","the backward policy. When this condition holds for all transitions, the retrieval trajectory induced by the policy P (st+1 | st) satisfies the objective in Eq. 5, leading to diverse and accurate retrieval results on KGs. While alternative GFlowNet objectives, such as trajectory balance [52] or subtrajectory balance [51], can also promote diversity, they require computation over entire trajectories or sub-trajectories. In KG-based RAG, the retrieval trajectory involves multi-hop transitions, and each node is associated with long documents. These objectives are computation-intensive and often lead to out-of-memory (OOM) issues. To ensure scalability and efficiency, we thus adopt the detailed balance objective that operates on state transitions. Detailed Balance with Local Exploration. Enforcing the detailed balance condition globally across all transitions in a knowledge graph (KG) is computationally inefficient, since the vast state space makes many nodes and transitions unreachable during training. To address this, we introduce a local exploration strategy that focuses the detailed balance objective on the neighborhoods of states observed in sampled trajectories. For a retrieval trajectory τ = V0 → · · · → VT with reward R(τ), we apply local exploration to each non-terminal state st = (Q, {Dj}t j=0) where t ̸= T . Specifically, the agent takes an exploratory action a′ t ∈ A(st) that moves from node Vt to a neighboring node V ′ t+1 ∈ N (Vt) different from the original next node Vt+1. This results in a new exploratory state s′ t+1 = (Q, {Dj}t j=0 ∪ {D′ t+1}), corresponding to"
"graphflow_rag_p05_c004","graphflow_rag","graphflow_rag.pdf","5","4","that moves from node Vt to a neighboring node V ′ t+1 ∈ N (Vt) different from the original next node Vt+1. This results in a new exploratory state s′ t+1 = (Q, {Dj}t j=0 ∪ {D′ t+1}), corresponding to the partial trajectory τ ′ ≤t+1 = V0 → · · · → Vt → V ′ t+1. 5"
"graphflow_rag_p06_c001","graphflow_rag","graphflow_rag.pdf","6","1","Table 2: Performance of retrieval diversity of KG-based RAG methods. GraphFlow retrieves more correct documents to support queries with high diversity. Method Dataset STaRK-AMAZON STaRK-MAG STaRK-PRIME Metric R@20↑ D-R@20↑ R@20↑ D-R@20↑ R@20↑ D-R@20↑ Retrieval -based DenseRetriver 13.63 13.63 41.80 41.80 13.92 13.92 G-Retriever 5.35 5.35 25.37 25.37 6.75 6.75 SubgraphRAG 6.53 6.53 27.83 26.95 6.49 6.49 Agent -based ToG+LLaMA3 2.61 2.61 6.77 6.77 33.84 33.84 ToG+GPT4o 25.81 23.70 48.03 47.71 54.35 54.35 SFT 25.22 24.97 37.48 35.90 47.72 45.36 PRM 35.72 18.94 36.73 36.73 45.97 45.97 GraphFlow 36.15 36.15 57.18 57.18 79.71 79.59 By performing k such explorations, we generate k exploratory actions {a′ t,1, · · · , a′ t,k} and their resulting states {s′ t+1,1, · · · , s′ t+1,k}. With the ground-truth next state denoted as s′ t+1,0 = st+1, we obtain k + 1 transitions from st to candidate next states for optimizing the detailed balance objective. The forward policy is then defined as P (st+1 = s′ t+1,i|st) = e rθ (st ,a′ t,i ) Pk i=0 e rθ (st ,a′ t,i ) . Here rθ(s, a) is a learned process reward function parameterized by a neural network with parameters θ. Since the retrieval process is inherently irreversible (i.e., backtracking is disallowed), we follow prior work [22] and set the backward policy PB(st | st+1) = 1 in Eq. 5. We yield the following objective for state st by taking the log function to both sides of Eq. 5: LDBLE(st) = kX i=0 [log"
"graphflow_rag_p06_c002","graphflow_rag","graphflow_rag.pdf","6","2","prior work [22] and set the backward policy PB(st | st+1) = 1 in Eq. 5. We yield the following objective for state st by taking the log function to both sides of Eq. 5: LDBLE(st) = kX i=0 [log F (st) − log F (s′ t+1,i) + logP (st+1 = s′ t+1,i|st)]2 = kX i=0 [log F (st) − log F (s′ t+1,i) + rθ(st, a′ t,i) − log kX i=0 erθ(st,a′ t,i)]2. (7) Boundary Condition. We impose boundary conditions on the initial and terminal states to ensure proper propagation of flow values along the retrieval trajectory:log F (s0) = log F (sT ) = 1. Here s0 is the initial state and sT is the terminal state. The reason is that we only collect retrieval trajectories that reach target documents during model training. With such boundary conditions, we ensure that the total incoming and outgoing flow is consistent across the trajectory and enable the flow estimator to correctly distribute the outcome reward of the terminal state to the intermediate states. Termination Condition. To allow the retrieval policy P (st+1 | st) to decide when to stop, we introduce a special self-loop action that retrieves the current node again. At each step, this action is included among the candidate actions in Eq. 7. Hence, Eq. 7 can also be applied for the terminal state. If the policy chooses to retrieve the current document (i.e., selects the self-loop), the trajectory is terminated, indicating that the current document is relevant to"
"graphflow_rag_p06_c003","graphflow_rag","graphflow_rag.pdf","6","3","actions in Eq. 7. Hence, Eq. 7 can also be applied for the terminal state. If the policy chooses to retrieve the current document (i.e., selects the self-loop), the trajectory is terminated, indicating that the current document is relevant to the query. Otherwise, the policy continues to explore the KG. This mechanism enables the agent to adaptively determine when to stop retrieval based on its experience, rather than relying on a fixed number of steps. Difference Between GraphFlow, SFT, and PRM . SFT and PRM learn the retrieval policy by treating the action leading to the ground-truth next state st+1 as a positive sample, and exploratory actions leads to {s′ t+1,1, · · · , s′ t+1,k} as negative ones, akin to behavior cloning [ 70]. GraphFlow generalizes this by learning state-dependent flow values F (s) and factorizes the outcome reward via Eq. 7. When setting log F (st) = 1 and log F (s′ t+1,i) = 0, GraphFlow reduces to behavior cloning. However, such as a hard objective limits generalization and cannot learn a policy leading to diverse and accurate retrieval results for complex queries. 3.3 Instantiating GraphFlow with LLMs We implement GraphFlow with an LLM due to its ability of text understanding and decision-making, as shown in Figure 2 (b). The state and state-action pair are decorated with a flow prompt and policy prompt template, which are encoded using a shared LLM. The embeddings of the final tokens are used as representations of the state and the state–action"
"graphflow_rag_p06_c004","graphflow_rag","graphflow_rag.pdf","6","4","2 (b). The state and state-action pair are decorated with a flow prompt and policy prompt template, which are encoded using a shared LLM. The embeddings of the final tokens are used as representations of the state and the state–action pair, respectively. On top of the shared 6"
"graphflow_rag_p07_c001","graphflow_rag","graphflow_rag.pdf","7","1","Table 3: Quantitative retrieval quality of different KG-based retrieval methods. Method STaRK-Amazon STaRK-MAG STaRK-PRIME Step-∆Seper ↑ Answer-∆Seper ↑ Step-∆Seper ↑ Answer-∆Seper ↑ Step-∆Seper ↑ Answer-∆Seper ↑ ToG+GPT-4o 0.031± 0.109 0.092 ± 0.128 0.041± 0.172 0.065 ± 0.150 0.065± 0.125 0.148 ± 0.165 ToG+LLaMA3 0.010± 0.118 0.046 ± 0.149 0.068± 0.108 0.105 ± 0.146 0.009± 0.102 0.021 ± 0.106 SFT 0.079± 0.151 0.141 ± 0.160 0.035± 0.101 0.084 ± 0.095 0.062± 0.132 0.158 ± 0.183 PRM 0.029± 0.089 0.071 ± 0.112 0.037± 0.117 0.060 ± 0.115 0.057± 0.106 0.131 ± 0.174 G-Retriever — 0.024 ± 0.110 — 0.012 ± 0.089 — 0.029 ± 0.117 SubgraphRAG — 0.021 ± 0.093 — 0.039 ± 0.076 — 0.046 ± 0.083 GraphFlow 0.097± 0.158 0.219 ± 0.257 0.081± 0.137 0.145 ± 0.112 0.091± 0.147 0.206 ± 0.192 encoder, we employ two separate multi-layer perceptrons (MLPs) as the policy head and the flow head, respectively. The policy head predicts the forward transition probability, while the flow head estimates logarithm of the flow value of the state. During model training, we apply LoRA [ 23] to inject learnable adapters into the frozen backbone of the LLM, and update the parameters of the flow head and the policy head. This design enables joint optimization of policy learning and flow estimation in a parameter-efficient manner, while also capturing rich contextual information through the LLM encoder. We present detailed implementation in Supplementary Material due to space limit. 4 Experiment 4.1 Dataset We employ the STaRK [74] benchmark to validate the"
"graphflow_rag_p07_c002","graphflow_rag","graphflow_rag.pdf","7","2","flow estimation in a parameter-efficient manner, while also capturing rich contextual information through the LLM encoder. We present detailed implementation in Supplementary Material due to space limit. 4 Experiment 4.1 Dataset We employ the STaRK [74] benchmark to validate the retrieval quality of the proposed GraphFlow to support complex queries. STaRK is a recently proposed benchmark designed to evaluate the retrieval performance of KG-based RAG methods on text-rich KGs spanning three domains: • STaRK-AMAZON is an e-commerce KG where the nodes contain detailed product infor- mation and the edges denotes the properties of products and co-purchase between products. The retrieval task is to retrieve the diverse products to satisfy the recommendation query. • STaRK-MAGis an academic graph constructed based on OGB [24] and Microsoft Aca- demic Graph [66]. The nodes contain author information, institute, and publications. The retrieval task is to address academic queries such as paper searching. • STaRK-PRIME is a biomedical KG where the nodes are associated with the detailed description of drugs, disease, genes, and pathways, and the edges are their relationship. The retrieval task is to address the biomedical query. The StaRK benchmark challenges KG-based RAG methods by complex queries corresponding to diverse retrieval targets and fusion of text and structure information that complicates accurate retrieval. 4.2 Baseline and Evaluation Metrics Baseline. We choose representative retrieval-based and agent-based baselines with explicit retrieval results (i.e., the retrieved node index) on the STaRK benchmark [74]. All detailed implementations are shown in Supplementary Material due to space limit. For"
"graphflow_rag_p07_c003","graphflow_rag","graphflow_rag.pdf","7","3","4.2 Baseline and Evaluation Metrics Baseline. We choose representative retrieval-based and agent-based baselines with explicit retrieval results (i.e., the retrieved node index) on the STaRK benchmark [74]. All detailed implementations are shown in Supplementary Material due to space limit. For retrieval-based baselines, we consider Dense-Retriever [30], G-Retriever [21], and Sub- graphRAG [37]. Dense-Retriever is implemented with SentenceBERT [62] to encode both questions and the documents of KG nodes into dense embeddings and retrieve the documents with top vector similarity. G-Retriever employs the Prize-Collecting Steiner Tree (PCST) [2] algorithm to extract a subgraph from KGs relevant to the query. Since computing PCST on STaRK benchmark is infeasible, we follow the hybrid setting [33, 34] that first identifies a source node in KG via Dense-Retriever and only computes PCST around the ego-graph up to 2 hops around the identified node. We also adopt the same hybrid setting for other baselines to ensure computational feasibility on STaRK. SubgraphRAG integrates a learnable subgraph retrieval module to retrieve from KG. For agent-based methods, we consider ToG [68], SFT, and PRM [40] as baselines. ToG employs an LLM agent to search from the KG to retrieve supporting documents. We instantiate ToG using both LLaMA3-8B-Instruct and GPT-4o as backbone models, denoted as ToG+LLaMA3 and ToG+GPT4o, 7"
"graphflow_rag_p08_c001","graphflow_rag","graphflow_rag.pdf","8","1","(a). Generalization Results without Rerank (b). Generalization Results with Rerank Figure 3: Generalization Performance of KG-based RAG methods. GraphFlow shows superior cross-domain generalization performance, especially under the rerank setting (best viewed in color). respectively. SFT and PRM are two popular approaches that fine-tune the LLM agent to enhance RAG in recent works [76, 12, 25]. SFT, PRM, and GraphFlow are deployed with LLaMA3-8B-Instruct [17]. For agent-based methods, we use the agent to rerank all the retrieved results. Evaluation Metrics. All KG-based RAG methods retrieve every input query 20 times and generate 20 retrieval results for diversity and accuracy evaluation following the standard setting in STaRK [74]. We employ the following metrics to evaluate the retrieval performance. Hit@k denotes whether the ground truth is retrieved in the top-k results. We employ Hit@1 and Hit@5 to measure the retrieval precision of the different KG-based RAG methods. Mean Reciprocal Rank (MRR) measures the average of reciprocal ranks of the first ground-truth item in the retrieval results and encourages the ground-truth item to be retrieved in a higher rank. Recall@k (R@k) is a standard metric to measure the percentage of ground-truth items that appear in the top-k retrieved results. We employ Recall@20 (R@20) for evaluation. De-duplicate Recall@k (D-R@k) measures the percentage of unique ground-truth items that appear in the top-k retrieved results. This metric is used to evaluate the diversity of the correctly retrieved results. We use De-duplicate Recall@20 (D-R@20). 4.3 Main Results Accuracy. Table 1 presents the retrieval accuracy of various KG-based RAG"
"graphflow_rag_p08_c002","graphflow_rag","graphflow_rag.pdf","8","2","items that appear in the top-k retrieved results. This metric is used to evaluate the diversity of the correctly retrieved results. We use De-duplicate Recall@20 (D-R@20). 4.3 Main Results Accuracy. Table 1 presents the retrieval accuracy of various KG-based RAG methods on STaRK. Figure 4: GraphFlow shows improved re- trieval diversity on different difficulty levels of retrieval queries on STaRK-PRIME. GraphFlow consistently outperforms other KG-based RAG approaches on most metrics. In particular, it achieves higher Hit rates and MRR scores than the strong baseline ToG+GPT-4o with an average 10% im- provement in retrieval accuracy. Interestingly, ToG’s performance is highly sensitive to the choice of back- end model. When instantiated with LLaMA3-8B, ToG shows a significant drop in performance compared to using GPT-4o. Additionally, rerank has no effect in the ToG+LLaMA3-8B setup, as all retrieved results receive equally high scores, leaving the ranking unchanged. Two finetuned agent-based baselines, SFT and PRM, outperform ToG without finetuning, but still fall short of GraphFlow. While PRM training can leverage curated preference datasets with fine-grained process-level re- wards, such labeling is prohibitively expensive. Instead, GraphFlow achieves high-quality retrieval with only outcome rewards of retrieval trajectories. For retriever-based approaches, DenseRetriever, G-Retriever, and SubgraphRAG show moderate performance but are generally inferior to agent-based 8"
"graphflow_rag_p09_c001","graphflow_rag","graphflow_rag.pdf","9","1","methods. Overall, retriever-based methods remain more lightweight but trail behind agent-based approaches in retrieval accuracy. Diversity. Table 2 reports the retrieval diversity of different KG-based RAG methods on the STaRK benchmark. We evaluate both Recall@20 (R@20) and its de-duplicated variant (D-R@20), which better captures retrieval diversity. GraphFlow achieves the highest retrieval diversity across all datasets, outperforming both retriever-based and agent-based baselines. Its results not only match more ground-truth contents but also avoid redundancy. Notably, GraphFlow exceeds the strongest baseline (ToG+GPT-4o) by a large margin on the STaRK-PRIME dataset, highlighting its ability to retrieve results that are both relevant and diverse. Compared with PRM and SFT, GraphFlow also demonstrates superior diversity. In contrast, retriever-based methods retrieve less diverse content and cover fewer retrieval targets. 4.4 Quantifying Retrieval Quality We further employ the Seper score (∆Seper) [9] to quantify the retrieval quality of different KG-based retrieval methods. The Seper score is a recently proposed metric for evaluating retrieval utility by measuring semantic perplexity reduction after retrieval: ∆Seper = pM(a|q, d) − pM(a|q). Here, q is the question, d is the document associated with the retrieval item, andM is an LLM used for question answering. In our case, we use LLaMA3–8B–Instruct to instantiate M to keep consistent with the retrieval model. Since there is no ground-truth answer for the questions in the STaRK benchmark, we use the title or summarized description of the ground-truth retrieval item as a. We design the following metrics for comprehensive evaluation and report their mean and standard deviation"
"graphflow_rag_p09_c002","graphflow_rag","graphflow_rag.pdf","9","2","is no ground-truth answer for the questions in the STaRK benchmark, we use the title or summarized description of the ground-truth retrieval item as a. We design the following metrics for comprehensive evaluation and report their mean and standard deviation (std). • Step-∆Seper: the Seper score that quantity the retrieval quality of intermediate retrieval. • Answer-∆Seper: the Seper score that quantity the retrieval quality of the final result. As shown in Table 3, GraphFlow consistently achieves higherStep-∆Seper and Answer-∆Seper than all the baselines, demonstrating stronger information utility during retrieval. These results further confirm that GraphFlow can significantly improve the information utility when retrieving from text- rich KGs. Notice that all the methods have high variance in Step-∆Seper and Answer-∆Seper. The reason is the high variance in natural language entailment when calculating Seper scores. Moreover, we observe that some retrieval samples have negative Seper scores for all methods, indicating a negative impact on question answering when retrieving bad contents. 4.5 Further Discussion Cross-domain Generalization. Figure 3 reports the cross-domain generalization ability of different KG-based RAG methods. We use Hit@1 to evaluate the retrieval accuracy Compared with Sub- graphRAG using a small model for retrieval, SFT, PRM, and GraphFlow that finetune the LLM show better cross-domain generalization ability due to the over-parameterization [35, 15]. GraphFlow demonstrates superior cross-domain generalization, since it avoids from likelihood maximization objectives used by SFT and PRM. Instead, GraphFlow adaptively assigns the outcome reward of the retrieval trajectory to the flow values of intermediate states and guides the"
"graphflow_rag_p09_c003","graphflow_rag","graphflow_rag.pdf","9","3","[35, 15]. GraphFlow demonstrates superior cross-domain generalization, since it avoids from likelihood maximization objectives used by SFT and PRM. Instead, GraphFlow adaptively assigns the outcome reward of the retrieval trajectory to the flow values of intermediate states and guides the retrieval policy, leading to better generalization ability. More results are shown in Supplementary Material due to space limit. Performance on Hard Cases. We categorize the retrieval queries with different numbers of retrieval targets into 4 difficulty levels. Figure 4 shows the D-R@20 scores of KG-based RAG methods on retrieval queries on STaRK-PRIME at different levels. GraphFlow outperforms the other agent-based approaches by a large margin by covering more diverse and accurate retrieval targets, especially on the hard cases containing more than 15 retrieval targets. The performance on hard cases shows the superior performance of GraphFlow in retrieving more relevant and diverse results. More results of hard cases are shown in Supplementary Materials due to space limit. 5 Related Work KG-based RAG. Knowledge graphs (KGs) are widely used as knowledge sources in retrieval- augmented generation (RAG) [20? ] systems to enhance large language models (LLMs) with both relational and textual information for answering complex queries [14, 13, 8, 75]. A core challenge in 9"
"graphflow_rag_p10_c001","graphflow_rag","graphflow_rag.pdf","10","1","KG-based RAG lies in retrieving relevant knowledge from KGs in response to a given query. Recent methods addressing this problem can be broadly categorized into two approaches. Retrieval-based methods [80, 53, 21, 61, 93] leverage pretrained language models to encode the textual content in KGs into embeddings and use small models, such as MLP and GNN, to retrieve relevant information. In contrast, agent-based methods [68, 88, 50, 47, 45] employ LLMs as agents that iteratively traverse the KG to locate supporting evidence. While both paradigms have shown promise in knowledge graph question answering (KGQA) [37], their effectiveness in retrieving diverse and high-quality candidates for complex queries remains limited. Furthermore, complex queries often require retrieval from text-rich KGs, necessitating the joint consideration of both relational structure and text content. Although recent studies [ 50, 34] have begun to explore this setting and tackle complex queries, enhancing the diversity and accuracy of KG-based RAG is still underexplored. Process Reward Models. Process Reward Models (PRMs) [40, 32] have shown great promise in guiding LLMs with process supervision and have been adopted in many domains such as complex reasoning [7], alignment [58], and planning [6]. The key to PRMs is to construct a preference dataset with process supervision [60]. Previous works obtain the process supervision from human feedback and LLM evaluation. However, fine-grained process-level supervision is expensive for KG-based RAG due to the potentially vast search space of KGs and the difficulty of accessing the intermediate state during retrieval. Although early explorations are made"
"graphflow_rag_p10_c002","graphflow_rag","graphflow_rag.pdf","10","2","supervision from human feedback and LLM evaluation. However, fine-grained process-level supervision is expensive for KG-based RAG due to the potentially vast search space of KGs and the difficulty of accessing the intermediate state during retrieval. Although early explorations are made to use PRM to guide the retrieval process of RAG on unstructured knowledge bases [39, 65, 25, 76], they still need a preference dataset with process-level supervision. How to guide the retrieval process of KG-based RAG on structured KGs without process supervision data is still challenging. GFlowNet. GFlowNet [3] aims to sample diverse and high-quality candidates from an unnormalized density and has received increasing attention in sampling from discrete and vast spaces [16, 46, 11, 10, 49]. The goal of GFlowNet is to learn a policy that can lead to the terminal states with the likelihood in proportion to their rewards [86]. Some objects are proposed to optimize GFlowNet by regularizing the state flows and their transitions [69, 52, 51]. Recently, GFlowNet has also been introduced to improve the generative performance of LLMs and diffusion models by promoting diversity in the decoding process [71, 22, 81, 29, 43] . Differently, our work focuses on aligning the retrieval results of KG-based RAG with the knowledge required for real-world queries by estimating the state flow in multi-step retrieval. Moreover, we introduce a local exploration strategy to avoid visiting less-valued states, thus efficiently optimizing the detailed balance. 6 Conclusion We introduce GraphFlow, a novel framework that enhances existing KG-based RAG methods by enabling"
"graphflow_rag_p10_c003","graphflow_rag","graphflow_rag.pdf","10","3","the state flow in multi-step retrieval. Moreover, we introduce a local exploration strategy to avoid visiting less-valued states, thus efficiently optimizing the detailed balance. 6 Conclusion We introduce GraphFlow, a novel framework that enhances existing KG-based RAG methods by enabling accurate and diverse retrieval from text-rich KGs. By jointly optimizing a retrieval policy and a flow estimator via a detailed balance objective, GraphFlow effectively aligns the retrieval process with query-specific knowledge demands without explicit process-level reward. Extensive evaluation on the STaRK benchmark demonstrates that GraphFlow not only surpasses strong baselines deployed with GPT-4o, but also generalizes well to unseen KGs. These findings underscore the effectiveness and robustness of GraphFlow in supporting complex queries using textual and structured knowledge. Our future work will incorporate causality into KG-based RAG to improve the reasoning ability of LLMs [44, 5, 83], reduce forgetting [41, 42], and explore their scientific applications [78]. Acknowledgments and Disclosure of Funding This work is supported by the UKRI grant: Turing AI Fellowship EP/W002981/1. This work is jointly supported by the Shanghai Municipal Science and Technology Major Project and Shanghai Artificial Intelligence Laboratory. The authors would like to thank PCs, ACs, and all the reviewers for their insightful comments and suggestions. References [1] Micheal Abaho and Yousef H Alfaifi. Select and augment: enhanced dense retrieval knowledge graph augmentation (abstract reprint). In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 22690–22690, 2024. 10"
"graphflow_rag_p11_c001","graphflow_rag","graphflow_rag.pdf","11","1","[2] Aaron Archer, MohammadHossein Bateni, MohammadTaghi Hajiaghayi, and Howard Karloff. Improved approximation algorithms for prize-collecting steiner tree and tsp. SIAM journal on computing, 40(2):309–332, 2011. [3] Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research, 24(210):1–55, 2023. [4] Weijie Chen, Ting Bai, Jinbo Su, Jian Luan, Wei Liu, and Chuan Shi. Kg-retriever: Ef- ficient knowledge indexing for retrieval-augmented large language models. arXiv preprint arXiv:2412.05547, 2024. [5] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of- distribution generalization on graphs. Advances in Neural Information Processing Systems , 35:22131–22148, 2022. [6] Sanjiban Choudhury. Process reward models for llm agents: Practical framework and directions. arXiv preprint arXiv:2502.10325, 2025. [7] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [8] Yuanning Cui, Zequn Sun, and Wei Hu. A prompt-based knowledge graph foundation model for universal in-context reasoning. Advances in Neural Information Processing Systems, 37:7095– 7124, 2024. [9] Lu Dai, Yijie Xu, Jinhui Ye, Hao Liu, and Hui Xiong. Seper: Measure retrieval utility through the lens of semantic perplexity reduction. In The Thirteenth International Conference on Learning Representations, 2025. [10] Tristan Deleu, António Góis, Chris Emezue, Mansi Rankawat, Simon Lacoste-Julien, Stefan Bauer, and Yoshua Bengio. Bayesian structure learning with generative flow networks. In Uncertainty in"
"graphflow_rag_p11_c002","graphflow_rag","graphflow_rag.pdf","11","2","lens of semantic perplexity reduction. In The Thirteenth International Conference on Learning Representations, 2025. [10] Tristan Deleu, António Góis, Chris Emezue, Mansi Rankawat, Simon Lacoste-Julien, Stefan Bauer, and Yoshua Bengio. Bayesian structure learning with generative flow networks. In Uncertainty in Artificial Intelligence, pages 518–528. PMLR, 2022. [11] Tristan Deleu, Mizu Nishikawa-Toomey, Jithendaraa Subramanian, Nikolay Malkin, Laurent Charlin, and Yoshua Bengio. Joint bayesian inference of graphical structure and parameters with a single generative flow network. Advances in Neural Information Processing Systems, 36:31204–31231, 2023. [12] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Ji-Rong Wen, and Zhicheng Dou. Understand what llm needs: Dual preference alignment for retrieval-augmented generation. In Proceedings of the ACM on Web Conference 2025, pages 4206–4225, 2025. [13] Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, and Chihang Wang. Advanced rag models with graph structures: Optimizing complex knowledge reasoning and text generation. In 2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC), pages 626–630. IEEE, 2024. [14] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024. [15] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835–10866. PMLR, 2023. [16] Pouya M Ghari, Alex Tseng, Gökcen Eraslan, Romain Lopez, Tommaso Biancalani, Gabriele Scalia, and Ehsan Hajiramezanali. Generative flow networks assisted biological sequence editing. In NeurIPS 2023"
"graphflow_rag_p11_c003","graphflow_rag","graphflow_rag.pdf","11","3","reward model overoptimization. In International Conference on Machine Learning, pages 10835–10866. PMLR, 2023. [16] Pouya M Ghari, Alex Tseng, Gökcen Eraslan, Romain Lopez, Tommaso Biancalani, Gabriele Scalia, and Ehsan Hajiramezanali. Generative flow networks assisted biological sequence editing. In NeurIPS 2023 Generative AI and Biology (GenBio) Workshop, 2023. [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 11"
"graphflow_rag_p12_c001","graphflow_rag","graphflow_rag.pdf","12","1","[18] Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li, Jiawei Tang, Dapeng Li, and Yingyou Wen. Knowledgenavigator: Leveraging large language models for enhanced reasoning over knowledge graph. Complex & Intelligent Systems, 10(5):7063–7076, 2024. [19] Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua, Bo Long, Hui Liu, and Jiliang Tang. Rag vs. graphrag: A systematic evaluation and key insights. arXiv preprint arXiv:2502.11371, 2025. [20] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halap- panavar, Ryan A Rossi, Subhabrata Mukherjee, Xianfeng Tang, et al. Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309, 2024. [21] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. Advances in Neural Information Processing Systems, 37:132876–132907, 2024. [22] Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations, 2024. [23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [24] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020. [25] Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Julia Hockenmaier, and Tong Zhang."
"graphflow_rag_p12_c002","graphflow_rag","graphflow_rag.pdf","12","2","Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020. [25] Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Julia Hockenmaier, and Tong Zhang. Rag-rl: Advancing retrieval-augmented generation via rl and curriculum learning.arXiv preprint arXiv:2503.12759, 2025. [26] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qiang- long Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):1–55, 2025. [27] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [28] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. [29] Haoqiang Kang, Enna Sachdeva, Piyush Gupta, Sangjae Bae, and Kwonjoon Lee. Gflowvlm: Enhancing multi-step reasoning in vision-language models with generative flow networks.arXiv preprint arXiv:2503.06514, 2025. [30] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 6769–6781, 2020. [31] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [32] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick,"
"graphflow_rag_p12_c003","graphflow_rag","graphflow_rag.pdf","12","3","[31] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [32] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. [33] Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N Ioannidis, Huzefa Rangwala, and Christos Faloutsos. Hybgrag: Hybrid retrieval-augmented generation on textual and relational knowledge bases. arXiv preprint arXiv:2412.16311, 2024. 12"
"graphflow_rag_p13_c001","graphflow_rag","graphflow_rag.pdf","13","1","[34] Yongjia Lei, Haoyu Han, Ryan A Rossi, Franck Dernoncourt, Nedim Lipka, Mahantesh M Halappanavar, Jiliang Tang, and Yu Wang. Mixture of structural-and-textual retrieval over text-rich graph knowledge bases. arXiv preprint arXiv:2502.20317, 2025. [35] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018. [36] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459–9474, 2020. [37] Mufei Li, Siqi Miao, and Pan Li. Retrieval or reasoning: The roles of graphs and large language models in efficient knowledge-graph-based retrieval-augmented generation. In The Thirteenth International Conference on Learning Representations, 2025. [38] Vincent Li, Yule Fu, Tim Knappe, Kevin Han, and Kevin Zhu. Automating mathematical proof generation using large language model agents and knowledge graphs. arXiv preprint arXiv:2503.11657, 2025. [39] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. [40] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. InThe Twelfth International Conference on Learning Representations, 2023. [41] Runqi Lin, Chaojian Yu, Bo Han, and Tongliang Liu. On the over-memorization during natural, robust and catastrophic overfitting. In The Twelfth International Conference on Learning Representations, 2024. [42]"
"graphflow_rag_p13_c002","graphflow_rag","graphflow_rag.pdf","13","2","step by step. InThe Twelfth International Conference on Learning Representations, 2023. [41] Runqi Lin, Chaojian Yu, Bo Han, and Tongliang Liu. On the over-memorization during natural, robust and catastrophic overfitting. In The Twelfth International Conference on Learning Representations, 2024. [42] Runqi Lin, Chaojian Yu, and Tongliang Liu. Eliminating catastrophic overfitting via abnormal adversarial examples regularization. Advances in Neural Information Processing Systems , 36:67866–67885, 2023. [43] Vijay Lingam, Behrooz Omidvar Tehrani, Sujay Sanghavi, Gaurav Gupta, Sayan Ghosh, Linbo Liu, Jun Huan, and Anoop Deoras. Enhancing language model agents using diversity of thoughts. In The Thirteenth International Conference on Learning Representations, 2025. [44] Chenxi Liu, Yongqiang Chen, Tongliang Liu, Mingming Gong, James Cheng, Bo Han, and Kun Zhang. Discovery of the hidden world with large language models. Advances in Neural Information Processing Systems, 37:102307–102365, 2024. [45] Hao Liu, Zhengren Wang, Xi Chen, Zhiyu Li, Feiyu Xiong, Qinhan Yu, and Wentao Zhang. Hoprag: Multi-hop reasoning for logic-aware retrieval-augmented generation. arXiv preprint arXiv:2502.12442, 2025. [46] Stephen Zhewen Lu, Ziqing Lu, Ehsan Hajiramezanali, Tommaso Biancalani, Yoshua Bengio, Gabriele Scalia, and Michał Koziarski. Cell morphology-guided small molecule generation with gflownets. In ICML 2024 Workshop on Structured Probabilistic Inference {\\&} Generative Modeling, 2024. [47] LINHAO LUO, Yuan-Fang Li, Reza Haf, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. In The Twelfth International Conference on Learning Representations, 2024. [48] Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Dinh Phung, Chen Gong, and Shirui Pan. Gfm-rag: Graph foundation model for retrieval augmented generation. arXiv preprint arXiv:2502.01113,"
"graphflow_rag_p13_c003","graphflow_rag","graphflow_rag.pdf","13","3","and interpretable large language model reasoning. In The Twelfth International Conference on Learning Representations, 2024. [48] Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Dinh Phung, Chen Gong, and Shirui Pan. Gfm-rag: Graph foundation model for retrieval augmented generation. arXiv preprint arXiv:2502.01113, 2025. 13"
"graphflow_rag_p14_c001","graphflow_rag","graphflow_rag.pdf","14","1","[49] Pouya M Ghari, Alex Tseng, Gokcen Eraslan, Romain Lopez, Tommaso Biancalani, Gabriele Scalia, and Ehsan Hajiramezanali. Gflownet assisted biological sequence editing. Advances in Neural Information Processing Systems, 37:106841–106869, 2024. [50] Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. Think-on-graph 2.0: Deep and faithful large language model reasoning with knowledge-guided retrieval augmented generation. arXiv preprint arXiv:2407.10805, 2025. [51] Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning, pages 23467–23483. PMLR, 2023. [52] Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems, 35:5955–5967, 2022. [53] Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning. arXiv preprint arXiv:2405.20139, 2024. [54] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. Advances in Neural Information Processing Systems, 37:124198–124235, 2024. [55] Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian D Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng. Rule based rewards for language model safety. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [56] Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective multiagent rl with global rewards. Advances in neural information processing systems , 31, 2018. [57] Thang Nguyen, Peter"
"graphflow_rag_p14_c002","graphflow_rag","graphflow_rag.pdf","14","2","Annual Conference on Neural Information Processing Systems, 2024. [56] Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective multiagent rl with global rewards. Advances in neural information processing systems , 31, 2018. [57] Thang Nguyen, Peter Chin, and Yu-Wing Tai. Reward-rag: Enhancing rag with reward driven supervision. arXiv preprint arXiv:2410.03780, 2024. [58] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744, 2022. [59] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering, 36(7):3580–3599, 2024. [60] Miao Peng, Nuo Chen, Zongrui Suo, and Jia Li. Rewarding graph reasoning process makes llms more generalized reasoners. arXiv preprint arXiv:2503.00845, 2025. [61] Zhangchi Qiu, Linhao Luo, Zicheng Zhao, Shirui Pan, and Alan Wee-Chung Liew. Graph retrieval-augmented llm for conversational recommendation systems. arXiv preprint arXiv:2503.06430, 2025. [62] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, 2019. [63] Diego Sanmartin. Kg-rag: Bridging the gap between knowledge and creativity. arXiv preprint arXiv:2405.12035, 2024. [64] Max W Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet"
"graphflow_rag_p14_c003","graphflow_rag","graphflow_rag.pdf","14","3","Processing (EMNLP-IJCNLP), pages 3982–3992, 2019. [63] Diego Sanmartin. Kg-rag: Bridging the gap between knowledge and creativity. arXiv preprint arXiv:2405.12035, 2024. [64] Max W Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. InInternational conference on machine learning, pages 30956–30975. PMLR, 2023. 14"
"graphflow_rag_p15_c001","graphflow_rag","graphflow_rag.pdf","15","1","[65] Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, and Ninghao Liu. Searchrag: Can search engines be helpful for llm-based medical question answering? arXiv preprint arXiv:2502.13233, 2025. [66] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web, pages 243–246, 2015. [67] Xiaorui Su, Yibo Wang, Shanghua Gao, Xiaolong Liu, Valentina Giunchiglia, Djork-Arné Clevert, and Marinka Zitnik. Knowledge graph based agent for complex, knowledge-intensive qa in medicine. In The Thirteenth International Conference on Learning Representations, 2025. [68] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. In The Twelfth International Conference on Learning Representations, 2024. [69] Alexander Tong, Kilian FATRAS, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based genera- tive models with minibatch optimal transport. Transactions on Machine Learning Research, 2022. [70] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4950– 4957, 2018. [71] Siddarth Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, et al. Amortizing intractable inference in diffusion models for vision, language, and control. In The Thirty-eighth Annual Conference on Neural Information"
"graphflow_rag_p15_c002","graphflow_rag","graphflow_rag.pdf","15","2","Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, et al. Amortizing intractable inference in diffusion models for vision, language, and control. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [72] Haiyuan Wan, Chen Yang, Junchi Yu, Meiqi Tu, Jiaxuan Lu, Di Yu, Jianbao Cao, Ben Gao, Jiaqing Xie, Aoran Wang, et al. Deepresearch arena: The first exam of llms’ research abilities via seminar-grounded tasks. arXiv preprint arXiv:2509.01396, 2025. [73] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. [74] Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis Ioannidis, Karthik Subbian, James Y Zou, and Jure Leskovec. Stark: Benchmarking llm retrieval on textual and relational knowledge bases. Advances in Neural Information Processing Systems, 37:127129–127153, 2024. [75] Yu Xia, Junda Wu, Sungchul Kim, Tong Yu, Ryan A Rossi, Haoliang Wang, and Julian McAuley. Knowledge-aware query expansion with large language models for textual and relational retrieval. arXiv preprint arXiv:2410.13765, 2024. [76] Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, et al. Rag-gym: Optimizing reasoning and search agents with process supervision. arXiv preprint arXiv:2502.13957, 2025. [77] Cheng Yang, Jiaxuan Lu, Haiyuan Wan, Junchi Yu, and Feiwei Qin. From what to why: A multi-agent system for evidence-based chemical"
"graphflow_rag_p15_c003","graphflow_rag","graphflow_rag.pdf","15","3","Wang, Minjia Zhang, et al. Rag-gym: Optimizing reasoning and search agents with process supervision. arXiv preprint arXiv:2502.13957, 2025. [77] Cheng Yang, Jiaxuan Lu, Haiyuan Wan, Junchi Yu, and Feiwei Qin. From what to why: A multi-agent system for evidence-based chemical reaction condition reasoning. arXiv preprint arXiv:2509.23768, 2025. [78] Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, and Dongzhan Zhou. Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. arXiv preprint arXiv:2410.07076, 2024. [79] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Ad- vances in neural information processing systems, 36:11809–11822, 2023. 15"
"graphflow_rag_p16_c001","graphflow_rag","graphflow_rag.pdf","16","1","[80] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. Qa-gnn: Reasoning with language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 535–546, 2021. [81] Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Efficient training of llm policy with divergent thinking. arXiv preprint arXiv:2406.05673, 2024. [82] Junchi Yu, Jie Cao, and Ran He. Improving subgraph recognition with variational graph information bottleneck. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19396–19405, 2022. [83] Junchi Yu, Jian Liang, and Ran He. Mind the label shift of augmentation-based graph ood generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11620–11630, 2023. [84] Junchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, and Ran He. Graph information bottleneck for subgraph recognition. In International Conference on Learning Representations, 2021. [85] Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, et al. Versaprm: Multi-domain process reward model via synthetic reasoning data. arXiv preprint arXiv:2502.06737, 2025. [86] Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra V olokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. In International Conference on Machine Learning, pages 26412–26428. PMLR, 2022. [87] Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and Haipeng Ding. Neural, symbolic and neural-symbolic reasoning on knowledge"
"graphflow_rag_p16_c002","graphflow_rag","graphflow_rag.pdf","16","2","Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. In International Conference on Machine Learning, pages 26412–26428. PMLR, 2022. [87] Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and Haipeng Ding. Neural, symbolic and neural-symbolic reasoning on knowledge graphs. AI Open, 2:14–35, 2021. [88] Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, and Xiao Huang. Knowgpt: Knowledge graph based prompting for large language models. Advances in Neural Information Processing Systems, 37:6052–6080, 2024. [89] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. [90] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [91] Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit assignment for cooperative multi-agent reinforcement learning. Advances in neural information processing systems, 33:11853–11864, 2020. [92] Jiachen Zhu, Congmin Zheng, Jianghao Lin, Kounianhua Du, Ying Wen, Yong Yu, Jun Wang, and Weinan Zhang. Retrieval-augmented process reward model for generalizable mathematical reasoning. arXiv preprint arXiv:2502.14361, 2025. [93] Deyu Zou, Yongqiang Chen, Mufei Li, Siqi Miao, Chenxi Liu, Bo Han, James Cheng, and Pan Li. Weak-to-strong graphrag: Aligning weak retrievers with large language models for graph-based retrieval augmented generation. arXiv preprint arXiv:2506.22518, 2025. 16"
"graphflow_rag_p17_c001","graphflow_rag","graphflow_rag.pdf","17","1","NeurIPS Paper Checklist The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: • You should answer [Yes] , [No] , or [NA] . • [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. • Please provide a short (1–2 sentence) justification right after your answer (even for NA). The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we"
"graphflow_rag_p17_c002","graphflow_rag","graphflow_rag.pdf","17","2",""", it is perfectly acceptable to answer ""[No] "" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering ""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: • Delete this instruction block, but keep the section heading “NeurIPS Paper Checklist"", • Keep the checklist subsection headings, questions/answers and guidelines below. • Do not modify the questions and only use the provided macros for your answers. 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: The main claim in the abstract and introduction accurately reflect the paper’s contribution and scope. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No"
"graphflow_rag_p17_c003","graphflow_rag","graphflow_rag.pdf","17","3","means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] 17"
"graphflow_rag_p18_c001","graphflow_rag","graphflow_rag.pdf","18","1","Justification: We discuss the limitation in a separate PDF file due to the space limitation of the submission. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate ""Limitations"" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and"
"graphflow_rag_p18_c002","graphflow_rag","graphflow_rag.pdf","18","2","handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not contribute new theory or new proof. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be"
"graphflow_rag_p18_c003","graphflow_rag","graphflow_rag.pdf","18","3","paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe the details of the proposed GraphFlow framework in Method sec- tion. We also provide implementation including training details and dataset pre-processing. Guidelines: 18"
"graphflow_rag_p19_c001","graphflow_rag","graphflow_rag.pdf","19","1","• The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution"
"graphflow_rag_p19_c002","graphflow_rag","graphflow_rag.pdf","19","2","some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We use publicly available benchmark in our experiment. Moreover, we provide details on how we use this benchmark in Supplementary Material. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and"
"graphflow_rag_p19_c003","graphflow_rag","graphflow_rag.pdf","19","3","publicly available benchmark in our experiment. Moreover, we provide details on how we use this benchmark in Supplementary Material. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. 19"
"graphflow_rag_p20_c001","graphflow_rag","graphflow_rag.pdf","20","1","• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Due to space limit, we provide these details in Supplementary Material. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The error bar is not accessible in the standard metrics of the benchmark used in our paper. However, our main results have shown a significant performance gain over the baselines. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of"
"graphflow_rag_p20_c002","graphflow_rag","graphflow_rag.pdf","20","2","means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type"
"graphflow_rag_p20_c003","graphflow_rag","graphflow_rag.pdf","20","3","authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide these details in Supplementary Material. Guidelines: 20"
"graphflow_rag_p21_c001","graphflow_rag","graphflow_rag.pdf","21","1","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in this paper conform with the NeruIPS Code of Ethics in every respect. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The broader impacts are discussed in Supplementary Material. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA"
"graphflow_rag_p21_c002","graphflow_rag","graphflow_rag.pdf","21","2","negative societal impacts of the work performed? Answer: [Yes] Justification: The broader impacts are discussed in Supplementary Material. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss"
"graphflow_rag_p21_c003","graphflow_rag","graphflow_rag.pdf","21","3","correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? 21"
"graphflow_rag_p22_c001","graphflow_rag","graphflow_rag.pdf","22","1","Answer: [NA] Justification: This paper does not pose such risks. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly cite the open-sourced LLM model and dataset in our paper. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and"
"graphflow_rag_p22_c002","graphflow_rag","graphflow_rag.pdf","22","2","version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human"
"graphflow_rag_p22_c003","graphflow_rag","graphflow_rag.pdf","22","3","how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects 22"
"graphflow_rag_p23_c001","graphflow_rag","graphflow_rag.pdf","23","1","Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowd-sourcing nor research with human objects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowd-sourcing nor research with human objects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this"
"graphflow_rag_p23_c002","graphflow_rag","graphflow_rag.pdf","23","2","not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: We confirm that the core method development in this research does not involve LLMs. Guidelines: • The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. • Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM) for what should or should not be described. A Introduction of GFlowNet Generative Flow Networks (GFlowNets) [3] aim to learn a stochastic policy that generates objects x ∈ X through sequential decisions, such that the marginal probability of generatingx"
"graphflow_rag_p23_c003","graphflow_rag","graphflow_rag.pdf","23","3","https://neurips.cc/Conferences/2025/LLM) for what should or should not be described. A Introduction of GFlowNet Generative Flow Networks (GFlowNets) [3] aim to learn a stochastic policy that generates objects x ∈ X through sequential decisions, such that the marginal probability of generatingx is proportional 23"
"graphflow_rag_p24_c001","graphflow_rag","graphflow_rag.pdf","24","1","to a reward function R(x) > 0. Given a complete trajectory τ = (s0, a1, s1, . . . , aT , sT = x) that terminates in object x, the forward trajectory probability is: PF (τ) = T −1Y t=0 PF (at+1|st) and the backward probability (used to reverse the trajectory) is: PB(τ) = TY t=1 PB(at|st) Trajectory Balance (TB) Loss [52].. The Trajectory Balance objective ensures the ratio of forward to backward probability matches the reward: PF (τ) PB(τ) = R(x) Z ⇐ ⇒ log PF (τ) − log PB(τ) = log R(x) − log Z where Z is the global partition function. The loss function is then defined as: LTB = (log PF (τ) − log PB(τ) − log R(x) + logZ)2 In practice, log Z is treated as a learnable scalar parameter. Subtrajectory Balance (SubTB) Loss [ 51].. To enable learning from partial trajectories, the Subtrajectory Balance loss generalizes TB to arbitrary subpaths. For any subtrajectory τi:j = (si, ai+1, . . . , sj) from state si to sj, the balance condition becomes: PF (τi:j) PB(τi:j) = Z(sj) Z(si) ⇐ ⇒ log PF (τi:j) − log PB(τi:j) = log Z(sj) − log Z(si) This leads to the Subtrajectory Balance loss: LSubTB = (log PF (τi:j) − log PB(τi:j) − log Z(sj) + logZ(si))2 Here, Z(s) denotes the flow or partition function at state s, typically parameterized by a neural network as Fϕ(s) = log Z(s). SubTB enables more flexible and sample-efficient training, especially for long-horizon"
"graphflow_rag_p24_c002","graphflow_rag","graphflow_rag.pdf","24","2","− log PB(τi:j) − log Z(sj) + logZ(si))2 Here, Z(s) denotes the flow or partition function at state s, typically parameterized by a neural network as Fϕ(s) = log Z(s). SubTB enables more flexible and sample-efficient training, especially for long-horizon generation tasks. However, directly implementing GFlowNet on KG-based RAG faces several challenges. First, the objectives such as Trajectory balance and sub-trajectory balance are computed on the whole trajectories, leading to computational burden in KG-based RAG where entities are associated with long texts. Second, many states and transitions in KGs are less-valued and not visited, making the traditional GFlowNet objective inefficient. Second, the discrete and symbolic nature of KGs poses difficulty in defining state transitions and flow dynamics, especially when integrating pretrained language models to interpret semantic relevance. These factors collectively make it challenging to directly apply GFlowNet to KG-based retrieval without significant adaptations in trajectory design, reward shaping, and exploration strategy. B Implementation of GraphFlow with LLMs Model Architecture. We use LLaMA3-8B-Instruct as the backbone LLM to implement GraphFlow. Specifically, we first employ the following flow prompt template to wrap the retrieval trajectoryτ≤t at state st into a text sequence for flow estimation. ###Information trajectory you have visited: {history} ###Question: {question} Please predict the reward of the Information trajectory to the question: Here {history} is the concatenation of documents of previously visited entities. {question} is the input complex query. The backbone LLM encodes the above wrapped text sequence. The embedding of the last token is treated as the representation of"
"graphflow_rag_p24_c003","graphflow_rag","graphflow_rag.pdf","24","3","the question: Here {history} is the concatenation of documents of previously visited entities. {question} is the input complex query. The backbone LLM encodes the above wrapped text sequence. The embedding of the last token is treated as the representation of the wrapped sequence used for flow estimation. We employ a 1-layer MLP as the flow head, which receives the representation of the wrapped sequence and outputs the log value of the estimated flow log st. Then we employ the following policy prompt template to warp the retrieval trajectory τ≤t at state st into a text sequence for policy learning. 24"
"graphflow_rag_p25_c001","graphflow_rag","graphflow_rag.pdf","25","1","Figure 5: Training dynamics of GraphFlow. ###Information trajectory you have visited: {history} ###Question: {question} ###Candidate Information: {candidate} Please predict the score of the candidate to help you find the answer to the question: Here {history} is the concatenation of documents of previously visited entities. {question} is the input complex query. And {candidate} is one actionat that leads to the next state st+1. The backbone LLM encodes the above wrapped text sequence. The embedding of the last token is treated as the representation of the wrapped sequence used for learning P (st+1|st). Specifically, we employ a 1-layer MLP with a ReLU function to parameterize σθ(st, at). The forward policy P (st+1|st) is calculated as below: P (st+1|st) = σθ(st, at)P at σθ(st, at) . (8) Training Configuration. we apply LoRA [23] to inject learnable adapters into the frozen backbone of the LLM, and update the parameters of the flow head and the policy head. This design enables joint optimization of policy learning and flow estimation in a parameter-efficient manner, while also capturing rich contextual information through the LLM encoder. The parameters of these modules are trained by optimizing the detailed balance with local exploration (DBLE) objective: LDBLE(st) = kX i=0 [log F (st) − log F (s′ t+1,i) + logP (st+1 = s′ t+1,i|st)]2 = kX i=0 [log F (st) − log F (s′ t+1,i) + rθ(st, a′ t,i) − log kX i=0 erθ(st,a′ t,i)]2. (9) Experimental Settings. To facilitate training the LoRA module and the flow head and the policy"
"graphflow_rag_p25_c002","graphflow_rag","graphflow_rag.pdf","25","2","= s′ t+1,i|st)]2 = kX i=0 [log F (st) − log F (s′ t+1,i) + rθ(st, a′ t,i) − log kX i=0 erθ(st,a′ t,i)]2. (9) Experimental Settings. To facilitate training the LoRA module and the flow head and the policy head on the STaRK benchmark, we first collect training dataset consisting of transitions between states. For a given question Q with the set of ground truth retrieval entities VT in the training set, we first identify the initial entity V0 using vector similarity between the embedding of Q and V0. Then, we sample the trajectory τ≤T = V0 → · · · → VT staring from V0 and ending at VT . We collect the all the transitions between st to st+1 in the example the trajectory τ≤T = V0 → · · · → VT 25"
"graphflow_rag_p26_c001","graphflow_rag","graphflow_rag.pdf","26","1","Table 4: Parameters of GraphFlow training on STaRK benchmark. STaRK-AMAZON STaRK-MAG STaRK-PRIME Accumulation steps 2 alpha 16 batch_size 1 num_gpu 8 depth_cutoff 6 doc_cutoff 400 eval_ratio 0.8 eval_step 100 lora_dropout 0.05 lr 1.00E-05 max_length 1024 n_epochs 1 num_exploration 4 r 32 window_size 3 Table 5: We provide data statistics of STaRK. The statistics are from the STaRK benchmark [74]. entity type relation type avg. degree entities relations tokens STARK-AMAZON 4 5 18.2 1,035,542 9,443,802 592,067,882 STARK-MAG 4 4 43.5 1,872,968 39,802,116 212,602,571 STARK-PRIME 10 18 125.2 129,375 8,100,498 31,844,769 to implement local exploration as introduced in Section 3.2 in the main paper. For every training step, we construct mini-batch of traditions between states to calculate the loss in Eq. 9. The training dynamic is shown in Figure 5. Here, training transition loss is calculated using the transition between non-terminal states. And training starting loss and training end loss are calculated using boundary condition F (s0) = F (sT ) = 0. Training total loss and eval loss are calculated on all the transitions between states on the training and evaluation dataset. Eval policy accuracy is the accuracy of policy P (st+1|st) on the evaluation dataset. We training GraphFlow on these dataset for one epoch, other important parameters are shown in Table 4. C Implementations of Baselines To the best of our knowledge, few KG-based RAG methods are implemented on the text-rich STaRK benchmark. Instead, many KG-based RAG methods employ simple KGQA datasets such as CWQ, WEBQSP. Thus, we choose representative"
"graphflow_rag_p26_c002","graphflow_rag","graphflow_rag.pdf","26","2","Table 4. C Implementations of Baselines To the best of our knowledge, few KG-based RAG methods are implemented on the text-rich STaRK benchmark. Instead, many KG-based RAG methods employ simple KGQA datasets such as CWQ, WEBQSP. Thus, we choose representative retrieval-based and agent-based baselines with explicit retrieval results (i.e., the retrieved node index) on the STaRK benchmark [ 74]. We provide the implementation details of the used baseline methods as below. Dense-Retriever is implemented with SentenceBERT [62] to encode both questions and the documents of KG nodes into dense embeddings and retrieve the documents with top vector similarity. We choose SentenceBERT as the text document to be consistent with prior works [21], where SentenceBERT is used to encode the text information in KGs. Although STaRK benchmark provide the pre-processed text embedding of entities and relationships in KGs using text-embedding-ada-002 model, we find the inconsistency between the entities IDs and the entities embeddings. Some entities in KGs are not converted into embeddings. Thus, we rerun the encoding model using SentenceBERT to obtain the full entities embeddings. After encoding the text information into embeddings, we employ the vector similarity between the question embedding and text embeddings for retrieval. We evaluate the retrieval performance on top 20 retrieval results. G-Retriever [21] is a two-stage method for KG-based RAG. It first employs the Prize-Collecting Steiner Tree (PCST) [2] algorithm to retrieve a subgraph from KGs relevant to the query. Then, the 26"
"graphflow_rag_p27_c001","graphflow_rag","graphflow_rag.pdf","27","1","retrieved subgraph is encoded into the token space of LLM using a GNN for question answering (QA). To further improve the QA performance, G-Retriever also applies LoRA module to fine-tune LLM. Since we focus on the evaluating the retrieval performance of different KG-based RAG methods, we do not fine-tune the GNN and LLM for QA. To make PCST algorithm feasible on STaRK benchmark, we adopt a hybrid approach that first identify the 20 seed nodes and implement the PCST algorithm to extract the subgraphs around 2-hop ego graph around the seed nodes. We drop the seed nodes with dense neighborhoods to avoid computation overhead [33, 34]. SubgraphRAG [37] integrates a learnable subgraph retrieval module to retrieve from KGs. Since training the subgraph retrieval module on the STaRK benchmark is infeasible, we employ the ego- graph setting similar to G-Retriever. We identify the up-to 2 hop neighbor hood graph around the seed node to construct the training and testing set for SubgraphRAG. We also drop the the seed node that has dense neighborhood to avoid computation overhead. This ego-graph setting is also employed to construct the test set for the other KG-based RAG models. We follow the default setting of SubgraphRAG to reproduce it on STaRK benchmark. ToG [68] employs an LLM agent to search from the KG to support KG-based question anwsering. ToG is implemented with frozen LLMs by prompt engineering instead of fine-tuning. Specifically, ToG employs tree-based search [79] to transverse the KG and search the relevant information for"
"graphflow_rag_p27_c002","graphflow_rag","graphflow_rag.pdf","27","2","LLM agent to search from the KG to support KG-based question anwsering. ToG is implemented with frozen LLMs by prompt engineering instead of fine-tuning. Specifically, ToG employs tree-based search [79] to transverse the KG and search the relevant information for KG-based QA. Since we focus on evaluating the retrieval performance of KG-based RAG models, we modify ToG to retrieve the relevant document at each searching steps instead of incorporating the retrieved document to update the question answering results. Since running ToG on the whole KGs in STaRK is infeasible, we identify the seed node for ToG searching using vector similarity and constrain the searching area around the 2-hop neighborhood of the seed node. We instantiate ToG using both LLaMA3-8B and GPT-4o as backbone models, denoted as ToG+LLaMA3 and ToG+GPT4o, respectively. We also implement SFT and PRM as two fine-tuning baselines build upon ToG and LLaMA3-8B- Instruct. We use the sample training dataset to train ToG using SFT and PRM as GraphFlow for a fair comparison. We employ the TRL (Transformer Reinforcement Learning) package to fir SFT and PRM fine-tuning. We apply LoRA funetuning to improve the efficiency. Other potential baselines but hard to implement on STaRK. There are alternative KG-based RAG baseline methods for evaluation. However, we find it hard to implemented these baseline on STaRK, mostly due to the compatibility issues. We list some examples as below. QAGNN [80] is designed for improving the QA performance on KG-based QA task. Although its retrieval performance is reported on STaRK benchmark,"
"graphflow_rag_p27_c003","graphflow_rag","graphflow_rag.pdf","27","3","to implemented these baseline on STaRK, mostly due to the compatibility issues. We list some examples as below. QAGNN [80] is designed for improving the QA performance on KG-based QA task. Although its retrieval performance is reported on STaRK benchmark, detailed implementation code on STaRK is not publicly available. Although recent concurrent work [ 34, 33] tried to implement QAGNN on STaRK, the reported performances of QAGNN diverge from the reported results on STaRK benchmark. RoG [47] adopts similar approach as it finetunes the LLM to search from KGs. It first employs an LLM to generate retrieval trajectories for the input queries and use the generated retrieval trajectories to construct a training dataset to fine-tune the retrieval agent by SFT. However, we find that LLM usually generate invalid retrieval trajectory, leading to low quality training datasets for SFT fine- tuning. Thus, we finetune the retrieval agent using the valid retrieval trajectories by SFT in the main paper. ToG-2.0 [50] is a recently proposed method to retrieve from the structured database and unstructured database. The key to ToG-2.0 is to identify the topic entitiies for a given questions. However, the implementation of topic entity recognition is absent, making it difficult to reproduce ToG-2.0 on STaRK benchmark. HybridRAG [33], Mixture of RAG [34], and KAR [75] are recent pre-prints on Arxiv focusing on retrieving from text-rich KGs. However, their codes are not available yet, making it difficult for us to reproduce these methods. D More results of Cross-domain Generalization We show more"
"graphflow_rag_p27_c004","graphflow_rag","graphflow_rag.pdf","27","4","[34], and KAR [75] are recent pre-prints on Arxiv focusing on retrieving from text-rich KGs. However, their codes are not available yet, making it difficult for us to reproduce these methods. D More results of Cross-domain Generalization We show more generalization performance in terms of Hit@5 in Figure 6. 27"
"graphflow_rag_p28_c001","graphflow_rag","graphflow_rag.pdf","28","1","(a). Generalization Results without Rerank. (b). Generalization Results with Rerank. Figure 6: Generalization Performance (Hit@5) of KG-based RAG methods. GraphFlow shows superior cross-domain generalization performance, especially under the rerank setting (best viewed in color). Figure 7: GraphFlow shows improved retrieval diversity on different difficulty levels of retrieval queries on STaRK-PRIME. E More results of Hard Cases We categorize the retrieval queries with different numbers of retrieval targets into 4 difficulty levels. We provide the performance of different KG-based RAG on STaRK-PRIME, STaRK-MAG, and STaRK-AMAZON at different difficulty levels. The results are shown in Figure 7, Figure 8, and Figure 9. 28"
"graphflow_rag_p29_c001","graphflow_rag","graphflow_rag.pdf","29","1","Figure 8: GraphFlow shows improved retrieval diversity on different difficulty levels of retrieval queries on STaRK-AMAZON. Figure 9: GraphFlow shows improved retrieval diversity on different difficulty levels of retrieval queries on STaRK-MAG. F Benchmark Information We provide benchmark information in Table 5. G Computing Resources We run all experiments on 8/16 NVIDIA-A800-SXM4-80GB GPUs and 56 Intel(R) Xeon(R) Platinum 8336C CPUs. 29"
"graphflow_rag_p30_c001","graphflow_rag","graphflow_rag.pdf","30","1","H Broader Impact and Limitations GraphFlow introduces a novel framework for retrieval-augmented generation over text-rich knowl- edge graphs, enabling Large Language Models (LLMs) to reason more effectively through process supervision using GFlowNets. By modeling retrieval as a generative process that balances diverse and relevant paths, GraphFlow promotes both interpretability and coverage in knowledge-based reasoning. This has broad implications for applications such as scientific discovery, open-domain question answering, and medical decision support, where combining structured knowledge with free-text reasoning is crucial. Moreover, GraphFlow can serve as a foundation for future research in integrating generative decision-making with symbolic structures, thereby pushing forward the synergy between LLMs and knowledge graphs. One potential limitation is that we only evaluate the generalization ability of GraphFlow on two new domains. 30"
"rag_evaluation_benchmark_p01_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","1","1","Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries Gabrielle Kaili-May Liu 1, Bryan Li 2, Arman Cohan 1, William Gantt Walden3,4, and Eugene Yang3,4 1 Yale University 2 University of Pennsylvania 3 Human Language Technology Center of Excellence 4 Johns Hopkins University kaili.liu@yale.edu,{wwalden1,eugene.yang}@jhu.edu Abstract.Real-world use cases often present RAG systems with com- plex queries for which relevant information is missing from the corpus or is incomplete. In these settings, RAG systems must be able to re- ject unanswerable, out-of-scope queries and identify failures of retrieval and multi-hop reasoning. Despite this, existing RAG benchmarks rarely reflect realistic task complexity for multi-hop or out-of-scope questions, which often can be cheated via disconnected reasoning (i.e., solved with- out genuine multi-hop inference) or require only simple factual recall. This limits the ability for such benchmarks to uncover limitations of ex- isting RAG systems. To address this gap, we present the first pipeline for automatic, difficulty-controlled creation of uncheatable, realistic, unansw- erable, and multi-hop queries (CRUMQs), adaptable to any corpus and domain. We use our pipeline to create CRUMQs over two popular RAG datasets and demonstrate its effectiveness via benchmark experiments on leading retrieval-augmented LLMs. Results show that compared to prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve up to 81.0% reduction in cheatability scores. More broadly, our pipeline offers a simple way to enhance benchmark difficulty and realism and drive development of more capable RAG systems. Keywords:multi-hop QA·unanswerability evaluation·synthetic data 1 Introduction Retrieval Augmented Generation (RAG) [11, 23]"
"rag_evaluation_benchmark_p01_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","1","2","to 81.0% reduction in cheatability scores. More broadly, our pipeline offers a simple way to enhance benchmark difficulty and realism and drive development of more capable RAG systems. Keywords:multi-hop QA·unanswerability evaluation·synthetic data 1 Introduction Retrieval Augmented Generation (RAG) [11, 23] is a powerful approach for many NLP tasks, enabling LLMs to respond to diverse user requests by leveraging an external document collection. While RAG is highly effective at increasing model credibility [15], mitigating hallucinations, and improving response quality [12], there remains a need to better understand how such systems handle complex, multi-part queries when available information from the corpus is insufficient. In particular, RAG systems must be able to appropriately reject unanswerable arXiv:2510.11956v1 [cs.CL] 13 Oct 2025"
"rag_evaluation_benchmark_p02_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","2","1","2 Liu et al. queries and localize retrieval or reasoning failures when responding to multi-hop requests [29, 37]. These capabilities are crucial for reliable deployment of RAG systems in high-stakes domains where information is often missing or incomplete. Existing RAG benchmarks [14, 44] rarely evaluate systems’ ability to handle realisticmulti-hop or unanswerable queries. Multi-hop RAG benchmarks [21, 37] focus on synthetic task domains or suffer from disconnected reasoning [38, 39] wherein shortcuts can be exploited to achieve correct answers, while unanswer- able RAG benchmarks [29] remain limited to simplistic factual recall settings. To address these deficiencies, we present the first pipeline for automatic gen- eration of unc heatable, r ealistic, u nanswerable, m ultihop q ueries (CRUMQs), which are robust against reasoning shortcuts, target content beyond retrieval- augmented LLMs’ training data cutoff dates, and can be tailored to any docu- ment corpus. We leverage recent insights in synthetic data generation to ensure coverage of diverse task types and complexity levels, with benchmark difficulty easily controllable via the distribution of in- vs. out-of-scope hops per question. We use our pipeline to create CRUMQs over two popular RAG datasets and showcase its efficacy through experiments on leading retrieval-augmented LLMs. Analysis reveals that CRUMQs pose notable difficulty even for RAG systems em- ploying SOTA models such as GPT-5 [28]. We further show that CRUMQs are significantly less cheatable via disconnected reasoning than prior multi-hop RAG benchmarks, achieving up to an 81.0% decrease in cheatability. Overall, our work contributes to a better understanding of"
"rag_evaluation_benchmark_p02_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","2","2","SOTA models such as GPT-5 [28]. We further show that CRUMQs are significantly less cheatable via disconnected reasoning than prior multi-hop RAG benchmarks, achieving up to an 81.0% decrease in cheatability. Overall, our work contributes to a better understanding of RAG systems’ limitations in handling unanswerable queries. Beyond driving the development of stronger and more ca- pable RAG systems, our pipeline opens the door to automatically increasing the difficulty of existing datasets, addressing the challenge of benchmark longevity. 2 Related Work A few studies have examined RAG system performance on queries which either require multi-hop reasoning or are beyond the scope of the relevant document col- lection [34]. However, these RAG benchmarks are restricted to singular domains and reflect low task complexity: multi-hop queries are fully answerable given the associated corpus and involve≤4 hops, while the out-of-scope queries generally target short factual recall tasks. For instance, MultiHop-RAG [37] targets the news domain, with queries based on 2-4 document chunks, but nearly 90% of questions can be solved by GPT-4—reflecting low difficulty and reduced bench- mark value. MHTS [21] generates difficulty-controllable multi-hop RAG queries, but QA pairs are created over asinglenovel to evaluate asingleRAG system, limiting generalizability. On the other hand, UAEval4RAG [29] presents a frame- work to create out-of-database and inherently unanswerable RAG requests, yet resulting queries exhibit low complexity, lack difficulty modulation, and may overlap with models’ parametric knowledge due to limited data verification. Be- yond these, common multi-hop benchmarks used for RAG tend to suffer from disconnected reasoning,"
"rag_evaluation_benchmark_p02_c003","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","2","3","inherently unanswerable RAG requests, yet resulting queries exhibit low complexity, lack difficulty modulation, and may overlap with models’ parametric knowledge due to limited data verification. Be- yond these, common multi-hop benchmarks used for RAG tend to suffer from disconnected reasoning, not involve unanswerability, or reflect limited response formats (e.g., only short entities) [19, 32, 39–41]. Other QA datasets present"
"rag_evaluation_benchmark_p03_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","3","1","Unanswerable, Uncheatable, Realistic Multi-hop Queries for RAG Evaluation 3 Fig. 1.Overview of the CRUMQs generation pipeline. both answerable and unanswerable queries, but these adopt narrow task formu- lations, are not multi-hop, and/or do not involve retrieval [16, 30, 33, 35, 36, 43]. In contrast, we present the first pipeline for creating queries tailored to a given corpus that arebothunanswerableandmulti-hop—andof realistic complexity. 3 Method We follow the pipeline shown in Fig. 1 to generate CRUMQs. We begin by ex- tracting relevant topic keyphrases over the provided document collection. In this work, we assume the collection is a RAG corpus𝐷with associated information- seeking requests and gold retrieved documents from𝐷per request. However, our pipeline may be generalized by using synthetic requests or topic modeling. Step I.Topics are extracted via two simple steps. A frontier LLM is few-shot prompted to extract short keyphrases from each information-seeking request.5 To obtain finer-grained topics, we then pair each initial topic with a gold document for the request and employ a second few-shot prompt to extract document- grounded topics. To ensure topics are sufficiently distinct and have good coverage of the corpus, we perform a deduplication step via embedding similarity. We use theBAAI/bge-large-en-v1.5embedder with a similarity threshold of 0.95. Step II.To collect relevant information that is likely beyond the given corpus and training data cutoff date for retrieval-augmented LLMs, we next crawl the 𝑁 𝑒 most related recent articles for each topic from the external sources Google News [4], bioRxiv [2], chemRxiv [3], medRxiv [5], arXiv [1], and"
"rag_evaluation_benchmark_p03_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","3","2","the given corpus and training data cutoff date for retrieval-augmented LLMs, we next crawl the 𝑁 𝑒 most related recent articles for each topic from the external sources Google News [4], bioRxiv [2], chemRxiv [3], medRxiv [5], arXiv [1], and PubMed [7]. Step III.The externally sourced articles are then used to generate queries that are eitherfullyunanswerable (relevant information is not found in𝐷, only in the externally sourced documents) orpartiallyunanswerable (relevant informa- tion is in𝐷, but at least one key fact or detail required to provide a complete and correct answer is absent). To do so, we first split each gold article and each ex- ternal article into 1,024-token chunks via LangChain [8], tracking for each chunk its source, URL, associated topic keyphrase, and associated request. Chunks are filtered for relevance to the topic and request via a binary LLM judgment. We then construct the set of all possible groups of 2-6 chunks such that at least one externally sourced chunk is in each group. As this set may be overly large in prac- tice, we place a limit𝑁 𝑐 on the number of contexts that are created for each total number of chunks and each ratio of external:gold chunks. To obtain seed queries, we prompt a strong generator LLM to systematically create up to 10 multi-hop 5 Our code and prompts will be released soon."
"rag_evaluation_benchmark_p04_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","4","1","4 Liu et al. QA pairs using each multi-chunk context,6 making sure information is leveraged across chunk boundaries. For fully unanswerable queries we require all chunks in a context to be externally sourced; for partially unanswerable queries we re- quire at least one gold chunk. We ensure diverse task formulations by adapting prompts from prior work in multi-document QA generation [25, 42]. Step IV.The unanswerability of each seed question is verified as in UAE- val4RAG [29]: each question is used to retrieve the top 10 relevant chunks from the original corpus𝐷, and LLM judgment is used to verify these chunks cannot answer the question. QA pairs that pass this verification are retained. Step V.The data is finally filtered to ensure only truly multi-hop and high- quality queries remain. Following [24], we first annotate each QA pair with a synthetic chain-of-thought (CoT) explanation of the answer in the oracle setting (i.e., assuming both gold and externally sourced documents are available), and record the number of hops required to solve the question. Queries that do not adhere to the intended hop count are discarded.7 We then utilize a strong LLM to assess each unanswerable QA pair in the oracle setting according to the following criteria [13, 37]: context necessity, context sufficiency, answer correctness, answer uniqueness. Contextual criteria are additionally assessed assuming only the given corpus is available. We utilize the same scoring scale (Likert, 0-2) as [13]: only QA pairs which receive a score of at least 1 for all 6 criteria"
"rag_evaluation_benchmark_p04_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","4","2","correctness, answer uniqueness. Contextual criteria are additionally assessed assuming only the given corpus is available. We utilize the same scoring scale (Likert, 0-2) as [13]: only QA pairs which receive a score of at least 1 for all 6 criteria are retained. 8 4 Experimental Setup We demonstrate the efficacy of our pipeline in creating unanswerable, uncheat- able, realistic, and multi-hop RAG queries by comparing against established RAG benchmarks. We consider UAEval4RAG [29] and MultiHop-RAG [37] as leading baselines, as they are the most recent works to perform comprehensive unanswerable and multi-hop RAG evaluation, respectively. Datasets.Using our pipeline, we first generate CRUMQs over NeuCLIR [6] and TREC RAG 2025 [9], two popular RAG datasets which each consist of a large document collection with associated user requests. We focus on English texts, set 𝑁 𝑒 = 200and𝑁 𝑐 = 50, and use Llama3.3-70B-Instruct [18] as the generation and verification model to balance costs and quality. 9 This leads to a total of 3,048 CRUMQs. We next use UAEval4RAG to generate baseline out-of-database RAG queries over NeuCLIR and TREC RAG 2025, leading to 7,559 out-of-database queries. Finally, we utilize the MultiHop-RAG dataset as-is, which consists of 2,556 multi-hop RAG queries and their associated gold contexts. Unanswerability Evaluation.To verify the unanswerability value of our pipeline, we adopt the same experimental setup as in [29] to benchmark four leading RAG 6 We found that QA generation with multi-chunk contexts was more fruitful than with extracted claims [37] or entity-relation triplets [22] from document chunks."
"rag_evaluation_benchmark_p04_c003","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","4","3","our pipeline, we adopt the same experimental setup as in [29] to benchmark four leading RAG 6 We found that QA generation with multi-chunk contexts was more fruitful than with extracted claims [37] or entity-relation triplets [22] from document chunks. 7 We retain a selection of single-hop queries in§4 for comparability to prior works. 8 We manually review a subset of examples to validate the LLM verification results. 9 As we show in§5, creating effective CRUMQs does depend on proprietary models."
"rag_evaluation_benchmark_p05_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","5","1","Unanswerable, Uncheatable, Realistic Multi-hop Queries for RAG Evaluation 5 T able 1.Performance of RAG systems on CRUMQs vs. UAEval4RAG queries, using GPT-4o for generation. † denotes Holm-Bonferroni corrected significance (𝑝 <0.05). Dataset Embedding Retrieval Reranker Rewriting Accep.↑Unans.↑Clar.↑Acc.↑ UAEval4RAG Cohere Vector None None 0.43 0.37 0.03 0.34 Cohere Vector Cohere HyDE 0.50 0.940.05 0.01 BGE Vector Cohere None 0.50 0.940.05 0.01 OpenAI Vector Cohere HyDE 0.42 0.45 0.05 0.28 CRUMQs Cohere Vector None None 0.34† 0.48† 0.06† 0.18† Cohere Vector Cohere HyDE 0.29† 0.79† 0.21† 0.00† BGE Vector Cohere None 0.29† 0.80† 0.20† 0.00† OpenAI Vector Cohere HyDE 0.23† 0.66† 0.05† 0.06† systems on the CRUMQs and UAEval4RAG queries, downsampled to 3,048 ex- amples for fair comparison. We assume no access to external documents and use LlamaIndex [27] for RAG system implementation. We additionally evaluate the performance of RAG systems that use leading proprietary LLMs Gemini-2.5-Pro [17] and GPT-5 for generation, in addition to GPT-4o as in [29]. Cheatability Evaluation.To demonstrate the robustness of our queries against reasoning shortcuts, we compare our CRUMQs (downsampled to 2,556 exam- ples) against MultiHop-RAG [37]. Following [38], we first obtain LLM predic- tions for each dataset in the oracle setting on the tasks of answer prediction and paragraph-level support identification. We use Llama3.1-8B-Instruct, Llama3.3- 70B-Instruct, GPT-4o, and Gemini-2.5-Pro to ensure coverage of diverse model types and sizes. We then repeat the same experiment on the DiRe probe [38] of each dataset, which is a model-agnostic probe to gauge the extent of disconnected reasoning; additional details"
"rag_evaluation_benchmark_p05_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","5","2","GPT-4o, and Gemini-2.5-Pro to ensure coverage of diverse model types and sizes. We then repeat the same experiment on the DiRe probe [38] of each dataset, which is a model-agnostic probe to gauge the extent of disconnected reasoning; additional details are in [38]. Metrics.For unanswerability evaluation, we adopt the metrics of acceptable ratio, unanswered ratio, and ask-for-clarification ratio from [29]. We addition- ally score accuracy by running LLM judgments of semantic equivalence between target and predicted answers (Gemini-2.0-Flash prompted as in [25]). For cheata- bility evaluation, we compute the average F1 score for each model×dataset×task setting as in [38, 39]. The cheatability of each dataset is then measured as the ratio of F1 scores in the probe vs. non-probe settings, which represents the per- centage of model performance attributable to disconnected reasoning. 5 Results and Analysis We report unanswerability evaluation results in Tables 1 and 2. Consistent with the findings in [29], we observe in Table 1 that no single system achieves the best performance on both CRUMQs and UAEval4RAG queries, with different system configurations able to yield similar results. Importantly,CRUMQs are harder than UAEval4RAG queries.All systems respond less acceptably to CRUMQs than UAEval4RAG queries. Moreover, CRUMQs pose notable difficulty for lead- ing proprietary LLM-based RAG systems, which provide much fewer acceptable responses for CRUMQs versus UAEval4RAG queries (Table 2). While queries"
"rag_evaluation_benchmark_p06_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","6","1","6 Liu et al. T able 2.Performance of proprietary model RAG systems on CRUMQs vs. UAE- val4RAG queries. As in [29], no reranking or rewriting is used. † denotes Holm- Bonferroni corrected significance (𝑝 <0.05). Dataset LLM Embedding Retrieval Accep.↑Unans.↑Clar.↑Acc.↑ UAEval4RAG Gemini-2.5-Pro OpenAI Ensemble 0.48 0.51 0.00 0.39 GPT-5 OpenAI Ensemble 0.28 0.31 0.07 0.33 GPT-4o OpenAI Ensemble 0.670.01 0.43 0.26 CRUMQs Gemini-2.5-Pro OpenAI Ensemble 0.23† 0.23† 0.03† 0.19† GPT-5 OpenAI Ensemble 0.28† 0.03† 0.35† 0.18† GPT-4o OpenAI Ensemble 0.24† 0.28† 0.01† 0.12† Fig. 2.Acceptable ratios of RAG sys- tems on CRUMQs across hop counts. Performance drops with more hops. Fig. 3.DiRe F1 score ratios(↓)across bench- marks and tasks. Black points denote accu- racy per model per task (values on right axis). from both systems occasionally capture parametric knowledge (i.e., answerable queries are produced, indicated by nonzero accuracy), this proportion is signif- icantly lower for CRUMQs than for UAEval4RAG queries. Finally, CRUMQs with greater hop counts lead to consistently lower acceptable ratios across sys- tems (Fig. 2), indicating easily controllable difficulty level for queries. Overall, our pipeline enables creation of harder out-of-database queries with controllable difficulty, which can be used to more effectively differentiate RAG systems. Compared to prior multi-hop RAG benchmarks, CRUMQs are significantly more difficult and much less cheatable, with all comparisons significant after Holm–Bonferroni correction (𝑝 <0.05). As indicated by the black points in Fig. 3, we observe that CRUMQs pose a much greater challenge for leading LLMs than MultiHop-RAG across all tasks, with models achieving answer"
"rag_evaluation_benchmark_p06_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","6","2","cheatable, with all comparisons significant after Holm–Bonferroni correction (𝑝 <0.05). As indicated by the black points in Fig. 3, we observe that CRUMQs pose a much greater challenge for leading LLMs than MultiHop-RAG across all tasks, with models achieving answer prediction scores up to only 31.9 versus up to 84.4 for MultiHop-RAG. Moreover, up to 96.7% of queries in MultiHop-RAG can be answered via disconnected reasoning, compared to only up to 23.6% of our CRUMQs. A similar trend is observed for the support identification task. These results confirm the efficacy of our pipeline for creation of challenging multi-hop RAG queries."
"rag_evaluation_benchmark_p07_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","7","1","Unanswerable, Uncheatable, Realistic Multi-hop Queries for RAG Evaluation 7 6 Conclusion and Future Work In this work, we introduced an adaptable framework that is the first of its kind for creating highly challenging, difficulty-controllable, unanswerable and multi- hop RAG queries that cannot be easily cheated. Experiments demonstrate that our pipeline effectively addresses deficiencies of existing RAG benchmarks. In ad- dition to advancing the development of stronger RAG systems, our work paves the way toward automatically increasing benchmark difficulty through data aug- mentation. Future work may explore the performance of multi-hop-oriented RAG systems [10, 20, 26, 31] on CRUMQs, along with analysis of systems’ ability to localize sources of unanswerability and signal uncertainty in such settings. Acknowledgments.This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE- 2139841. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article. References 1. arxiv. https://www.arxiv.org/ 2. biorxiv. https://biorxiv.org 3. chemrxiv. https://www.chemrxiv.org/ 4. Google news. https://news.google.com/ 5. medrxiv. https://www.medrxiv.org/ 6. Neuclir corpus. https://ir-datasets.com/neuclir.html 7. Pubmed. https://pubmed.ncbi.nlm.nih.gov/ 8. Tokentextsplitter, https://python.langchain.com/api reference/ text splitters/base/langchain text splitters.base.TokenTextSplitter.html# tokentextsplitter 9. Trec 2025 rag corpus. https://trec-rag.github.io/annoucements/ 2025-rag25-corpus/ 10. Agrawal, R., Asrani, M., Youssef, H., Narayan, A.: Scmrag: Self-corrective multihop retrieval augmented generation system for llm agents. In: Pro- ceedings of the 24th International Conference"
"rag_evaluation_benchmark_p07_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","7","2","reference/ text splitters/base/langchain text splitters.base.TokenTextSplitter.html# tokentextsplitter 9. Trec 2025 rag corpus. https://trec-rag.github.io/annoucements/ 2025-rag25-corpus/ 10. Agrawal, R., Asrani, M., Youssef, H., Narayan, A.: Scmrag: Self-corrective multihop retrieval augmented generation system for llm agents. In: Pro- ceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems. p. 50–58. AAMAS ’25, International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC (2025) 11. Asai, A., Min, S., Zhong, Z., Chen, D.: Retrieval-based language mod- els and applications. In: Chen, Y.N.V., Margot, M., Reddy, S. (eds.) Proceedings of the 61st Annual Meeting of the Association for Com- putational Linguistics (Volume 6: Tutorial Abstracts). pp. 41–46. As- sociation for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-tutorials.6, https://aclanthology.org/ 2023.acl-tutorials.6/"
"rag_evaluation_benchmark_p08_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","8","1","8 Liu et al. 12. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Milli- can, K., Van Den Driessche, G.B., Lespiau, J.B., Damoc, B., Clark, A., De Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J., Elsen, E., Sifre, L.: Improv- ing language models by retrieving from trillions of tokens. In: Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., Sabato, S. (eds.) Proceed- ings of the 39th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 162, pp. 2206–2240. PMLR (17–23 Jul 2022), https://proceedings.mlr.press/v162/borgeaud22a.html 13. Chernogorskii, F., Averkiev, S., Kudraleeva, L., Martirosian, Z., Tikhonova, M., Malykh, V., Fenogenova, A.: Dragon: Dynamic rag benchmark on news (2025), https://arxiv.org/abs/2507.05713 14. Friel, R., Belyi, M., Sanyal, A.: Ragbench: Explainable benchmark for retrieval-augmented generation systems (2025), https://arxiv.org/abs/2407. 11005 15. Gao, T., Yen, H., Yu, J., Chen, D.: Enabling large language models to gen- erate text with citations. In: Bouamor, H., Pino, J., Bali, K. (eds.) Pro- ceedings of the 2023 Conference on Empirical Methods in Natural Lan- guage Processing. pp. 6465–6488. Association for Computational Linguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.emnlp-main.398, https://aclanthology.org/2023.emnlp-main.398/ 16. Gonz´ alez Torres, J.J., Bˆ ındil˘ a, M.B., Hofstee, S., Szondy, D., Nguyen, Q.H., Wang, S., Englebienne, G.: Automated question-answer generation for evaluating RAG-based chatbots. In: Demner-Fushman, D., Ananiadou, S., Thompson, P., Ondov, B. (eds.) Proceedings of the First Workshop on Patient-Oriented Language"
"rag_evaluation_benchmark_p08_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","8","2","J.J., Bˆ ındil˘ a, M.B., Hofstee, S., Szondy, D., Nguyen, Q.H., Wang, S., Englebienne, G.: Automated question-answer generation for evaluating RAG-based chatbots. In: Demner-Fushman, D., Ananiadou, S., Thompson, P., Ondov, B. (eds.) Proceedings of the First Workshop on Patient-Oriented Language Processing (CL4Health) @ LREC-COLING 2024. pp. 204–214. ELRA and ICCL, Torino, Italia (May 2024), https: //aclanthology.org/2024.cl4health-1.25/ 17. Google: Gemini 2.5 pro model card. https://modelcards.withgoogle.com/ assets/documents/gemini-2.5-pro.pdf (2025) 18. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C.C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E.M., Radenovic, F., Guzm´ an, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G.L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I.A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J.,"
"rag_evaluation_benchmark_p08_c003","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","8","3","Ibarra, I.A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J.,"
"rag_evaluation_benchmark_p09_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","9","1","Unanswerable, Uncheatable, Realistic Multi-hop Queries for RAG Evaluation 9 Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K.V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M.K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., C ¸ elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P.S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R.S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S.S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V.,"
"rag_evaluation_benchmark_p09_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","9","2","Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X.E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z.D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victo- ria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Al- varado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B.D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Mon- talvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil,"
"rag_evaluation_benchmark_p09_c003","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","9","3","C.H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G.M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J.,"
"rag_evaluation_benchmark_p10_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","10","1","10 Liu et al. Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K.H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veer- araghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M.L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M.J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N.P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bon- trager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mi- tra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S.J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S.C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen,"
"rag_evaluation_benchmark_p10_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","10","2","S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S.C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V.S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V.T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., Ma, Z.: The llama 3 herd of models (2024), https://arxiv.org/abs/2407.21783 19. Ho, X., Duong Nguyen, A.K., Sugawara, S., Aizawa, A.: Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In: Scott, D., Bel, N., Zong, C. (eds.) Proceedings of the 28th Interna- tional Conference on Computational Linguistics. pp. 6609–6625. Interna- tional Committee on Computational Linguistics, Barcelona, Spain (On- line) (Dec 2020). https://doi.org/10.18653/v1/2020.coling-main.580, https: //aclanthology.org/2020.coling-main.580/ 20. Hu, Y., Lei, Z., Dai, Z., Zhang, A., Angirekula, A., Zhang, Z., Zhao, L.: Cg-rag: Research question answering"
"rag_evaluation_benchmark_p10_c003","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","10","3","Conference on Computational Linguistics. pp. 6609–6625. Interna- tional Committee on Computational Linguistics, Barcelona, Spain (On- line) (Dec 2020). https://doi.org/10.18653/v1/2020.coling-main.580, https: //aclanthology.org/2020.coling-main.580/ 20. Hu, Y., Lei, Z., Dai, Z., Zhang, A., Angirekula, A., Zhang, Z., Zhao, L.: Cg-rag: Research question answering by citation graph retrieval-augmented llms (2025), https://arxiv.org/abs/2501.15067 21. Lee, J., Kwon, D., Jin, K., Jeong, J., Sim, M., Kim, M.: Mhts: Multi-hop tree structure framework for generating difficulty-controllable qa datasets for rag evaluation (2025), https://arxiv.org/abs/2504.08756 22. Lei, D., Li, Y., Li, S., Hu, M., Xu, R., Archer, K., Wang, M., Ching, E., Deng, A.: FactCG: Enhancing fact checkers with graph-based multi-hop data. In: Chiruzzo, L., Ritter, A., Wang, L. (eds.) Proceedings of the 2025 Conference"
"rag_evaluation_benchmark_p11_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","11","1","Unanswerable, Uncheatable, Realistic Multi-hop Queries for RAG Evaluation 11 of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 5002–5020. Association for Computational Linguistics, Albuquerque, New Mexico (Apr 2025). https://doi.org/10.18653/v1/2025.naacl-long.258, https: //aclanthology.org/2025.naacl-long.258/ 23. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨ uttler, H., Lewis, M., Yih, W.t., Rockt¨ aschel, T., et al.: Retrieval- augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems33, 9459–9474 (2020) 24. Li, Y., Liang, S., Lyu, M., Wang, L.: Making long-context language models better multi-hop reasoners. In: Ku, L.W., Martins, A., Sriku- mar, V. (eds.) Proceedings of the 62nd Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers). pp. 2462– 2475. Association for Computational Linguistics, Bangkok, Thailand (Aug 2024). https://doi.org/10.18653/v1/2024.acl-long.135, https://aclanthology. org/2024.acl-long.135/ 25. Liu, G.K.M., Shi, B., Caciularu, A., Szpektor, I., Cohan, A.: MDCure: A scalable pipeline for multi-document instruction-following. In: Che, W., Nabende, J., Shutova, E., Pilehvar, M.T. (eds.) Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers). pp. 29258–29296. Association for Computational Lin- guistics, Vienna, Austria (Jul 2025). https://doi.org/10.18653/v1/2025.acl- long.1418, https://aclanthology.org/2025.acl-long.1418/ 26. Liu, H., Wang, Z., Chen, X., Li, Z., Xiong, F., Yu, Q., Zhang, W.: Hoprag: Multi-hop reasoning for logic-aware retrieval-augmented generation (2025), https://arxiv.org/abs/2502.12442 27. Liu, J.: LlamaIndex (11 2022). https://doi.org/10.5281/zenodo.1234, https: //github.com/jerryjliu/llama index 28. OpenAI: Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf (2025) 29. Peng, X., Choubey, P.K., Xiong, C., Wu, C.S.: Unanswerability evaluation"
"rag_evaluation_benchmark_p11_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","11","2","Q., Zhang, W.: Hoprag: Multi-hop reasoning for logic-aware retrieval-augmented generation (2025), https://arxiv.org/abs/2502.12442 27. Liu, J.: LlamaIndex (11 2022). https://doi.org/10.5281/zenodo.1234, https: //github.com/jerryjliu/llama index 28. OpenAI: Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf (2025) 29. Peng, X., Choubey, P.K., Xiong, C., Wu, C.S.: Unanswerability evaluation for retreival augmented generation. arXiv preprint arXiv:2412.12300 (2024) 30. Peng, Z., Nian, J., Evfimievski, A., Fang, Y.: Eloq: Resources for enhancing llm detection of out-of-scope questions (2025), https://arxiv.org/abs/2410. 14567 31. Poliakov, M., Shvai, N.: Multi-Meta-RAG: Improving RAG for Multi- hop Queries Using Database Filtering with LLM-Extracted Metadata, p. 334–342. Springer Nature Switzerland (2025). https://doi.org/10.1007/978- 3-031-81372-6˙25, http://dx.doi.org/10.1007/978-3-031-81372-6 25 32. Rajpurkar, P., Jia, R., Liang, P.: Know what you don’t know: Unanswer- able questions for SQuAD. In: Gurevych, I., Miyao, Y. (eds.) Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). pp. 784–789. Association for Computational Lin- guistics, Melbourne, Australia (Jul 2018). https://doi.org/10.18653/v1/P18- 2124, https://aclanthology.org/P18-2124/"
"rag_evaluation_benchmark_p12_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","12","1","12 Liu et al. 33. Rosenthal, S., Sil, A., Florian, R., Roukos, S.: Clapnq: Cohesive long-form answers from passages in natural questions for rag systems (2024) 34. Shen, H., Yan, H., Xing, Z., Liu, M., Li, Y., Chen, Z., Wang, Y., Wang, J., Ma, Y.: Ragsynth: Synthetic data for robust and faithful rag component optimization (2025), https://arxiv.org/abs/2505.10989 35. Sun, Y., Yin, Z., Guo, Q., Wu, J., Qiu, X., Zhao, H.: Benchmarking hal- lucination in large language models based on unanswerable math word problem. In: Calzolari, N., Kan, M.Y., Hoste, V., Lenci, A., Sakti, S., Xue, N. (eds.) Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC- COLING 2024). pp. 2178–2188. ELRA and ICCL, Torino, Italia (May 2024), https://aclanthology.org/2024.lrec-main.196/ 36. Tan, C., Shao, W., Xiong, H., Zhu, T., Liu, Z., Shi, K., Chen, W.: Uaqfact: Evaluating factual knowledge utilization of llms on unanswerable questions (2025), https://arxiv.org/abs/2505.23461 37. Tang, Y., Yang, Y.: Multihop-rag: Benchmarking retrieval-augmented gen- eration for multi-hop queries (2024) 38. Trivedi, H., Balasubramanian, N., Khot, T., Sabharwal, A.: Is multihop QA in DiRe condition? measuring and reducing disconnected reasoning. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 8846–8863. Association for Computational Linguistics, On- line (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.712, https: //aclanthology.org/2020.emnlp-main.712/ 39. Trivedi, H., Balasubramanian, N., Khot, T., Sabharwal, A.: MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics10, 539–554 (2022). https://doi.org/10.1162/tacl˙a˙00475, https://aclanthology.org/2022.tacl-1. 31/ 40."
"rag_evaluation_benchmark_p12_c002","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","12","2","Association for Computational Linguistics, On- line (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.712, https: //aclanthology.org/2020.emnlp-main.712/ 39. Trivedi, H., Balasubramanian, N., Khot, T., Sabharwal, A.: MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics10, 539–554 (2022). https://doi.org/10.1162/tacl˙a˙00475, https://aclanthology.org/2022.tacl-1. 31/ 40. Yang, X., Sun, K., Xin, H., Sun, Y., Bhalla, N., Chen, X., Choudhary, S., Gui, R.D., Jiang, Z.W., Jiang, Z., Kong, L., Moran, B., Wang, J., Xu, Y.E., Yan, A., Yang, C., Yuan, E., Zha, H., Tang, N., Chen, L., Scheffer, N., Liu, Y., Shah, N., Wanga, R., Kumar, A., tau Yih, W., Dong, X.L.: Crag – comprehensive rag benchmark. arXiv preprint arXiv:2406.04744 (2024), https://arxiv.org/abs/2406.04744 41. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.W., Salakhutdinov, R., Manning, C.D.: Hotpotqa: A dataset for diverse, explainable multi-hop ques- tion answering (2018), https://arxiv.org/abs/1809.09600 42. Zhang, J., Bai, Y., Lv, X., Gu, W., Liu, D., Zou, M., Cao, S., Hou, L., Dong, Y., Feng, L., Li, J.: LongCite: Enabling LLMs to generate fine- grained citations in long-context QA. In: Che, W., Nabende, J., Shutova, E., Pilehvar, M.T. (eds.) Findings of the Association for Computational Lin- guistics: ACL 2025. pp. 5098–5122. Association for Computational Linguis- tics, Vienna, Austria (Jul 2025). https://doi.org/10.18653/v1/2025.findings- acl.264, https://aclanthology.org/2025.findings-acl.264/"
"rag_evaluation_benchmark_p13_c001","rag_evaluation_benchmark","rag_evaluation_benchmark.pdf","13","1","Unanswerable, Uncheatable, Realistic Multi-hop Queries for RAG Evaluation 13 43. Zhang, Q.W., Li, F., Wang, J., Qiao, L., Yu, Y., Yin, D., Sun, X.: Factguard: Leveraging multi-agent systems to generate answerable and unanswerable questions for enhanced long-context llm extraction (2025), https://arxiv. org/abs/2504.05607 44. Zhu, K., Luo, Y., Xu, D., Yan, Y., Liu, Z., Yu, S., Wang, R., Wang, S., Li, Y., Zhang, N., Han, X., Liu, Z., Sun, M.: RAGEval: Scenario specific RAG evaluation dataset generation framework. In: Che, W., Nabende, J., Shutova, E., Pilehvar, M.T. (eds.) Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 8520–8544. Association for Computational Linguistics, Vienna, Austria (Jul 2025). https://doi.org/10.18653/v1/2025.acl-long.418, https://aclanthology. org/2025.acl-long.418/"
"rag_evaluation_ragas_p01_c001","rag_evaluation_ragas","rag_evaluation_ragas.pdf","1","1","Ragas: Automated Evaluation of Retrieval Augmented Generation Shahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗ †Exploding Gradients ∗CardiffNLP, Cardiff University, United Kingdom ♢AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk Abstract We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the genera- tion itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs. 1 Introduction Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT (Devlin et al., 2019) and became more firmly established with the introduction of"
"rag_evaluation_ragas_p01_c002","rag_evaluation_ragas","rag_evaluation_ragas.pdf","1","2","which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT (Devlin et al., 2019) and became more firmly established with the introduction of ever larger LMs (Roberts et al., 2020). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks (Bubeck et al., 2023), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling (Khandel- wal et al., 2020; Borgeaud et al., 2022), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well (Khattab et al., 2022; Ram et al., 2023; Shi et al., 2023), thus making it possible to use retrieval- augmented strategies in combination with LLMs that"
"rag_evaluation_ragas_p01_c003","rag_evaluation_ragas","rag_evaluation_ragas.pdf","1","3","retrieved docu- ments to the input of a standard LM can also work well (Khattab et al., 2022; Ram et al., 2023; Shi et al., 2023), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs. While the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used. To address these issues, in this paper we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas. arXiv:2309.15217v2 [cs.CL] 28 Apr 2025"
"rag_evaluation_ragas_p02_c001","rag_evaluation_ragas","rag_evaluation_ragas.pdf","2","1","of retrieval augmented generation systems. We fo- cus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the useful- ness of the retrieved passages. The Ragas frame- work provides an integration with both llama-index and Langchain, the most widely used frameworks for building RAG solutions, thus enabling devel- opers to easily integrate Ragas into their standard workflow. 2 Related Work Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied (Ji et al., 2023). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy (Zhang et al., 2023). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies (Li et al., 2023; Azaria and Mitchell, 2023). Other approaches rely on linking the generated responses to facts from an external knowledge base (Min et al., 2023), but this is not always possible. Yet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. For instance, BARTScore (Yuan et al., 2021) estimates factuality by looking at the conditional probability of the gen- erated text given the input. Kadavath et al. (2022) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating"
"rag_evaluation_ragas_p02_c002","rag_evaluation_ragas","rag_evaluation_ragas.pdf","2","2","of the gen- erated text given the input. Kadavath et al. (2022) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. Rather than looking at the output probabil- ities, Azaria and Mitchell (2023) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API. For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT (Manakul et al., 2023) addresses this problem by instead sam- pling multiple answers. Their core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore (Fu et al., 2023) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to"
"rag_evaluation_ragas_p02_c003","rag_evaluation_ragas","rag_evaluation_ragas.pdf","2","3","aspects of generated text fragments, beyond factuality. For instance, GPTScore (Fu et al., 2023) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale (Wang et al., 2023a). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates (Wang et al., 2023b), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result (Wang et al., 2023b). In terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) use contex- tualised embeddings, produced by a"
"rag_evaluation_ragas_p02_c004","rag_evaluation_ragas","rag_evaluation_ragas.pdf","2","4","have been typically used in the literature, most approaches have relied on the availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system,"
"rag_evaluation_ragas_p03_c001","rag_evaluation_ragas","rag_evaluation_ragas.pdf","3","1","we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly important, e.g. in domains such as law, where information is constantly evolving. Sec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. Finally,Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is"
"rag_evaluation_ragas_p03_c002","rag_evaluation_ragas","rag_evaluation_ragas.pdf","3","2","can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statement si 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format."
"rag_evaluation_ragas_p03_c003","rag_evaluation_ragas","rag_evaluation_ragas.pdf","3","3","in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. statement: [statement 1] ... statement: [statement n] The final faithfulness score, F , is then computed as F = |V | |S| , where |V | is the number of statements that were supported according to the LLM and |S| is the total number of statements. Answer relevance We say that the answer as(q) is relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer as(q), we prompt the LLM to generate n potential questions qi based on as(q), as follows: Generate a question for the given answer. answer: [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each qi, we cal- culate the similarity sim(q, qi) with the original question q, as the cosine between the correspond- ing embeddings. The answer relevance score, AR, for question q is then computed as: AR = 1 n nX i=1 sim(q, qi) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion. Context relevance The context"
"rag_evaluation_ragas_p03_c004","rag_evaluation_ragas","rag_evaluation_ragas.pdf","3","4","answer relevance score, AR, for question q is then computed as: AR = 1 n nX i=1 sim(q, qi) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion. Context relevance The context c(q) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise the"
"rag_evaluation_ragas_p04_c001","rag_evaluation_ragas","rag_evaluation_ragas.pdf","4","1","inclusion of redundant information. To estimate context relevance, given a question q and its con- text c(q), the LLM extracts a subset of sentences, Sext, from c(q) that are crucial to answer q, using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase ""Insufficient Information"". While extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context. The context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c(q) (2) 4 The WikiEval Dataset To evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval4. To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 20225. In selecting these pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is"
"rag_evaluation_ragas_p04_c002","rag_evaluation_ragas","rag_evaluation_ragas.pdf","4","2","pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. The question should be fully answered from the given context. 2. The question should be framed from a part that contains non-trivial informa- tion. 3. The answer should not contain any 4https://huggingface.co/datasets/ explodinggradients/WikiEval 5That is, beyond the reported training cutoff of the model we used in our experiments. links. 4. The question should be of moderate difficulty. 5. The question must be reasonable and must be understood and responded to by humans. 6. Do not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context. question: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators. Faithfulness To obtain human judgements"
"rag_evaluation_ragas_p04_c003","rag_evaluation_ragas","rag_evaluation_ragas.pdf","4","3","For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators. Faithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context. We then asked the annotators to judge which of the two answers was the most faithful (i.e. the standard one or the one generated without context), given the question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner. question: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance. Context relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. In this way, we were able to add information to the context that was related but less relevant for"
"rag_evaluation_ragas_p05_c001","rag_evaluation_ragas","rag_evaluation_ragas.pdf","5","1","Faith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with- out any back-links, we instead used ChatGPT to complete the given context. 5 Experiments Table 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. Each WikiEval instance requires the model to compare two answers or two context fragments. We count how often the answer/context preferred by the model (i.e. with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators). To put the results in context, we compare our proposed metrics (shown as Ragas in Table 1) with two baseline methods. For the first method, shown as GPT Score, we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. Any claims that"
"rag_evaluation_ragas_p05_c002","rag_evaluation_ragas","rag_evaluation_ragas.pdf","5","2","quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized. Given an answer and context, assign a score for faithfulness in the range 0-10. context: [context] answer: [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in- stead asks ChatGPT to select the preferred answer/- context. In this case, the prompt again includes a definition of the considered quality metric. For instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question. It penalizes the present of redundant in- formation or incomplete answers given a question. Given an question and answer, rank each answer based on Answer Rele- vancy. question: [question] answer 1: [answer 1] answer 2: [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. For faithfulness, the Ragas prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We"
"rag_evaluation_ragas_p05_c003","rag_evaluation_ragas","rag_evaluation_ragas.pdf","5","3","base- lines. For faithfulness, the Ragas prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We found context relevance to be the hardest quality dimension to evaluate. In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts. 6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the answer grounded in the retrieved context), answer relevance (i.e. does the answer address the ques- tion) and context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval, a dataset which human judgements of these three different aspects. Finally, we have also described Ragas, our implementation of the three considered quality aspects. This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from Ragas are closely aligned with human predictions, especially for faithfulness and answer relevance."
"rag_evaluation_ragas_p06_c001","rag_evaluation_ragas","rag_evaluation_ragas.pdf","6","1","References Amos Azaria and Tom M. Mitchell. 2023. The inter- nal state of an LLM knows when its lying. CoRR, abs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research , pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. CoRR, abs/2302.04166. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training."
"rag_evaluation_ragas_p06_c002","rag_evaluation_ragas","rag_evaluation_ragas.pdf","6","2","Minnesota. Association for Computational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. CoRR, abs/2302.04166. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning, pages 3929–3938. PMLR. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation. ACM Comput- ing Surveys, 55(12):1–38. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. CoRR, abs/2211.08411. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and"
"rag_evaluation_ragas_p06_c003","rag_evaluation_ragas","rag_evaluation_ragas.pdf","6","3","neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP. CoRR, abs/2212.14024. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open do- main question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096. Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large- scale hallucination evaluation benchmark for large language models. CoRR, abs/2305.11747. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. Association for Computational Linguistics. Potsawee Manakul,"
"rag_evaluation_ragas_p06_c004","rag_evaluation_ragas","rag_evaluation_ragas.pdf","6","4","trust language models: Investigating effectiveness of parametric and non-parametric mem- ories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. Association for Computational Linguistics. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models. CoRR, abs/2303.08896. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation. CoRR, abs/2305.14251."
"rag_evaluation_ragas_p07_c001","rag_evaluation_ragas","rag_evaluation_ragas.pdf","7","1","Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented lan- guage models. CoRR, abs/2302.00083. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418–5426, Online. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-augmented black-box language models. CoRR, abs/2301.12652. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good NLG evaluator? A preliminary study. CoRR, abs/2303.04048. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. CoRR, abs/2305.17926. Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer. 2023c. KNN-LM does not improve open-ended text generation. CoRR, abs/2305.14625. Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text genera- tion. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual, pages 27263–27277. Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023. Interpretable unified language checking. CoRR, abs/2304.03728. Tianyi Zhang, Varsha Kishore, Felix Wu,"
"rag_evaluation_ragas_p07_c002","rag_evaluation_ragas","rag_evaluation_ragas.pdf","7","2","6-14, 2021, virtual, pages 27263–27277. Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023. Interpretable unified language checking. CoRR, abs/2304.03728. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu- ating text generation with BERT. In8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 563–578, Hong Kong, China. Association for Computational Lin- guistics. A Examples from WikiEval Tables 2, 3 and 4 show examples from the WikiEval dataset, focusing in particular on answers with high and low faithfulness (Table 2), high and low answer relevance (Table 3), and high and low context rele- vance (Table 4)."
"rag_evaluation_ragas_p08_c001","rag_evaluation_ragas","rag_evaluation_ragas.pdf","8","1","Question Context Answer Who directed the film Op- penheimer and who stars as J. Robert Oppenheimer in the film? Oppenheimer is a 2023 biographical thriller film written and directed by Christopher Nolan. Based on the 2005 biography American Prometheus by Kai Bird and Mar- tin J. Sherwin, the film chronicles the life of J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Man- hattan Project, and thereby ushering in the Atomic Age. Cillian Murphy stars as Oppenheimer, with Emily Blunt as Oppenheimer’s wife Katherine ""Kitty"" Oppenheimer. High Faithfulness : Christopher Nolan directed the film Oppen- heimer. Cillian Murphy stars as J. Robert Oppenheimer in the film. Low Faithfulness : James Cameron directed the film Op- penheimer. Tom Cruise stars as J. Robert Oppenheimer in the film. Table 2: Example from WikiEval, showing answers with high and low faithfulness. Question Answer When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from? High answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India. Low answer relevance: The scheduled launch date and time for the PSLV-C56 mission have not been provided.The PSLV-C56 mission is an important space mission for India. It aims to launch a satellite into orbit to study weather patterns. Table 3: Example from WikiEval, showing answers"
"rag_evaluation_ragas_p08_c002","rag_evaluation_ragas","rag_evaluation_ragas.pdf","8","2","launch date and time for the PSLV-C56 mission have not been provided.The PSLV-C56 mission is an important space mission for India. It aims to launch a satellite into orbit to study weather patterns. Table 3: Example from WikiEval, showing answers with high and low answer relevance. Question Context When was the Chimnabai Clock Tower completed, and who was it named af- ter? High context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. Low context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style. History. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023). Table 4: Example from"
"rag_evaluation_ragas_p08_c003","rag_evaluation_ragas","rag_evaluation_ragas.pdf","8","3","last Nawab of Baroda. During the rule of Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023). Table 4: Example from WikiEval, showing answers with high and low context relevance."
"img_bm25_prf_table3_1","bm25_prf_table3_1","bm25_prf_table3_1.png","0","0","bm25 prf table3 1"
"img_chain_of_retrieval_fig1_example","chain_of_retrieval_fig1_example","chain_of_retrieval_fig1_example.png","0","0","chain of retrieval fig1 example"
"img_chain_of_retrieval_fig2_overview","chain_of_retrieval_fig2_overview","chain_of_retrieval_fig2_overview.png","0","0","chain of retrieval fig2 overview"
"img_chain_of_retrieval_fig3_scaling","chain_of_retrieval_fig3_scaling","chain_of_retrieval_fig3_scaling.png","0","0","chain of retrieval fig3 scaling"
"img_gfm_rag_framework","gfm_rag_framework","gfm_rag_framework.png","0","0","gfm rag framework"
"img_gfm_rag_results_table","gfm_rag_results_table","gfm_rag_results_table.png","0","0","gfm rag results table"
"img_graphflow_rag_p04_architecture","graphflow_rag_p04_architecture","graphflow_rag_p04_architecture.png","0","0","graphflow rag p04 architecture"
"img_graphflow_rag_p25_training_dynamics","graphflow_rag_p25_training_dynamics","graphflow_rag_p25_training_dynamics.png","0","0","graphflow rag p25 training dynamics"
"img_ragas_table1_metrics","ragas_table1_metrics","ragas_table1_metrics.png","0","0","ragas table1 metrics"
